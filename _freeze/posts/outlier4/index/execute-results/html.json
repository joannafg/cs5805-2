{
  "hash": "1936b3244d2ba71b84d8e4a629a989ca",
  "result": {
    "markdown": "---\ntitle: Anomaly/Outlier Detection\nauthor: Joanna Fang\ndate: '2023-12-06'\ncategories:\n  - ml\n  - code\n  - linear regression\n  - nonlinear regression\n  - pollution\nformat:\n  html:\n    code-block-bg: '#FFFFFF'\n    code-block-border-left: '#E83283'\n    toc: true\n    code-tools:\n      source: true\n      toggle: false\n      caption: none\n---\n\n# Detecting Anomalies in Air Pollution Data: A Data Science Project\n\n![](thumbnail.jpg){width=\"50%\" fig-align=\"center\"}\n\n## Introduction\n\nWelcome to our exploration of \"Detecting Anomalies in Air Pollution Data,\" a vital project in the realm of environmental monitoring. With increasing concerns about air quality and its impact on public health and the environment, identifying irregularities in air pollution data has never been more critical.\n\nThis project leverages a comprehensive dataset from the Beijing Multi-site Air Quality Data, which offers a rich tapestry of air pollutant measurements and meteorological data across various sites in Beijing. The data spans from 2013 to 2017, providing insights into pollutants like PM2.5, PM10, SO2, NO2, and CO, as well as meteorological conditions like temperature, humidity, and wind speed.\n\nOur primary goal is to detect unusual patterns or outliers in air quality data that might signify environmental hazards, technical errors in data collection, or significant meteorological impacts. By accomplishing this, we aim to contribute to more effective environmental monitoring and policy-making.\n\n## Data Exploration and Preprocessing\n\n### Understanding the Dataset\nThe first step in our data science journey involves getting acquainted with the dataset's structure and characteristics. This involves examining the various columns of the dataset, which include both pollutant levels and meteorological factors.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nsample_data_org = pd.read_csv('air_data_all.csv')\nsample_data_org.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>No</th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>hour</th>\n      <th>PM2.5</th>\n      <th>PM10</th>\n      <th>SO2</th>\n      <th>NO2</th>\n      <th>CO</th>\n      <th>O3</th>\n      <th>TEMP</th>\n      <th>PRES</th>\n      <th>DEWP</th>\n      <th>RAIN</th>\n      <th>wd</th>\n      <th>WSPM</th>\n      <th>station</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2013</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>6.0</td>\n      <td>18.0</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>800.0</td>\n      <td>88.0</td>\n      <td>0.1</td>\n      <td>1021.1</td>\n      <td>-18.6</td>\n      <td>0.0</td>\n      <td>NW</td>\n      <td>4.4</td>\n      <td>Gucheng</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2013</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>6.0</td>\n      <td>15.0</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>800.0</td>\n      <td>88.0</td>\n      <td>-0.3</td>\n      <td>1021.5</td>\n      <td>-19.0</td>\n      <td>0.0</td>\n      <td>NW</td>\n      <td>4.0</td>\n      <td>Gucheng</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>2013</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>5.0</td>\n      <td>18.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>700.0</td>\n      <td>52.0</td>\n      <td>-0.7</td>\n      <td>1021.5</td>\n      <td>-19.8</td>\n      <td>0.0</td>\n      <td>WNW</td>\n      <td>4.6</td>\n      <td>Gucheng</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2013</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>6.0</td>\n      <td>20.0</td>\n      <td>6.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1.0</td>\n      <td>1022.7</td>\n      <td>-21.2</td>\n      <td>0.0</td>\n      <td>W</td>\n      <td>2.8</td>\n      <td>Gucheng</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>2013</td>\n      <td>3</td>\n      <td>1</td>\n      <td>4</td>\n      <td>5.0</td>\n      <td>17.0</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>600.0</td>\n      <td>73.0</td>\n      <td>-1.3</td>\n      <td>1023.0</td>\n      <td>-21.4</td>\n      <td>0.0</td>\n      <td>WNW</td>\n      <td>3.6</td>\n      <td>Gucheng</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nBy running this code, we get a glimpse of the first few rows of our dataset, allowing us to understand the types of data we will be working with.\n\n### Handling Missing Data and Categorical Variables\nDealing with missing data and categorical variables is a crucial part of data preprocessing. To address this, we first identify the missing values and then decide on an appropriate strategy, such as imputation or removal.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nmissing_values = sample_data_org.isnull().sum()\nmissing_values\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nNo             0\nyear           0\nmonth          0\nday            0\nhour           0\nPM2.5       8739\nPM10        6449\nSO2         9021\nNO2        12116\nCO         20701\nO3         13277\nTEMP         398\nPRES         393\nDEWP         403\nRAIN         390\nwd          1822\nWSPM         318\nstation        0\ndtype: int64\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\n# Remove rows with missing values\nsample_data = sample_data_org.dropna()\n\n# Identify numerical columns\nnumerical_cols = sample_data.select_dtypes(include=['int64', 'float64']).columns\n\n# Create a pipeline for imputing missing values and scaling\npipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),  # Replace missing values with mean\n    ('scaler', StandardScaler()),                # Scale the data\n])\n\n# Apply the pipeline to the numerical columns\nscaled_data = pipeline.fit_transform(sample_data[numerical_cols])\n\n# Apply PCA\npca = PCA(n_components=0.95)  # Retain 95% of the variance\nprincipal_components = pca.fit_transform(scaled_data)\n\n# Identify non-numeric (categorical) columns\ncategorical_cols = sample_data.select_dtypes(include=['object']).columns\n\n# One-hot encode the categorical data\nencoder = OneHotEncoder(sparse=False)\ncategorical_encoded = encoder.fit_transform(sample_data[categorical_cols])\n\n# Check for 'get_feature_names_out' method for naming columns\nif hasattr(encoder, 'get_feature_names_out'):\n    encoded_columns = pd.DataFrame(categorical_encoded, columns=encoder.get_feature_names_out(categorical_cols))\nelse:\n    # Fallback: manually create feature names\n    encoded_columns = pd.DataFrame(categorical_encoded)\n    encoded_columns.columns = [col + '_' + str(i) for col in categorical_cols for i in range(encoded_columns.shape[1])]\n\n# Concatenate the encoded columns with the original dataset and drop the original categorical columns\nsample_data_encoded = pd.concat([sample_data.drop(categorical_cols, axis=1), encoded_columns], axis=1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n```\n:::\n:::\n\n\nFor categorical variables like wind direction, we use encoding techniques to convert them into numerical form, making them suitable for analysis.\n\n### Normalization and Standardization\nGiven the varying scales of our numerical features, normalization or standardization becomes necessary. This step ensures that no single feature disproportionately influences the model due to its scale.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(sample_data[['PM2.5', 'PM10', 'TEMP', 'PRES']])\n```\n:::\n\n\n### Feature Selection and Engineering\nFinally, we perform feature selection and engineering. This process involves choosing the most relevant features and possibly creating new features to improve our model's performance.\n\n1. **Correlation Analysis**: First, we can perform a correlation analysis to understand the relationships between different features. This helps in identifying features that are strongly correlated with each other, from which we can select the most relevant ones.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Now perform the correlation analysis on the numerical data\ncorr = sample_data_encoded.corr()\n\n# Generate a heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, fmt=\".2f\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=879 height=764}\n:::\n:::\n\n\n    This code generates a heatmap of the correlations between different features. High correlation values suggest a strong relationship, which can inform feature selection.\n\n2. **Principal Component Analysis (PCA)**: PCA is a technique used to reduce the dimensionality of the data, enhancing the interpretability while minimizing information loss.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=0.95) # Retain 95% of the variance\nprincipal_components = pca.fit_transform(scaled_data)\n```\n:::\n\n\n    This code applies PCA to the scaled data, reducing the number of features while retaining 95% of the variance in the data.\n\n3. **Feature Engineering**: If applicable, you can create new features that might be more indicative of anomalies. For example, creating a composite air quality index from multiple pollutants.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nsample_data['Air_Quality_Index'] = sample_data['PM2.5'] * 0.4 + sample_data['PM10'] * 0.2 + sample_data['NO2'] * 0.2 + sample_data['SO2'] * 0.1 + sample_data['CO'] * 0.1\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/pt/983cdyd950gd2j6l1gv1376r0000gn/T/ipykernel_17824/3115101989.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  sample_data['Air_Quality_Index'] = sample_data['PM2.5'] * 0.4 + sample_data['PM10'] * 0.2 + sample_data['NO2'] * 0.2 + sample_data['SO2'] * 0.1 + sample_data['CO'] * 0.1\n```\n:::\n:::\n\n\n    This code creates a new feature, 'Air_Quality_Index', as a weighted sum of various pollutants, hypothesizing that this composite index might be a more effective predictor of anomalies.\n\nThrough these steps, we refine our dataset to include the most relevant features for anomaly detection, enhancing the model's accuracy and efficiency.\n\n## Anomaly Detection Algorithms and Model Training and Evaluation\n\n### Choosing the Anomaly Detection Algorithm: Isolation Forest\nFor our project on air pollution data, we have opted for the Isolation Forest algorithm due to its efficiency and effectiveness, especially in dealing with large and high-dimensional datasets like ours. \n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.ensemble import IsolationForest\n\niso_forest = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)\n```\n:::\n\n\n### Data Preprocessing and Splitting\n\nWe split our dataset into training and test sets, ensuring that the model is evaluated on unseen data, reflecting its performance in real-world scenarios.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nimport pandas as pd\n# Handling NaN Values with Imputation\n# Impute missing values and then scale the numerical columns\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),  # Replace missing values with mean\n    ('std_scaler', StandardScaler())\n])\n\n# Apply the pipeline to the numerical columns\nnumerical_cols = sample_data_org.select_dtypes(include=['int64', 'float64']).columns\nsample_data_org[numerical_cols] = num_pipeline.fit_transform(sample_data_org[numerical_cols])\n\n# One-hot encode the categorical columns\ncategorical_cols = sample_data_org.select_dtypes(include=['object']).columns\nsample_data_org = pd.get_dummies(sample_data_org, columns=categorical_cols, drop_first=True)\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test = train_test_split(sample_data_org, test_size=0.3, random_state=42)\n```\n:::\n\n\n### Training the Model\nThe training process involves fitting the Isolation Forest model to our training data. This model is particularly suited for our project due to its lower computational cost compared to other algorithms and its ability to efficiently handle the complexity of our dataset.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\niso_forest.fit(X_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(contamination=0.1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(contamination=0.1, random_state=42)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n### Evaluation Metrics\nIn evaluating our model, we focus on metrics such as precision, recall, F1-score, and the ROC curve. These metrics provide insights into the model's accuracy, its ability to identify true anomalies (precision), and its capability to detect the majority of actual anomalies (recall).\n\n### Model Evaluation and Insights\nAfter training, we assess the model's performance on the test set. This evaluation helps us understand the effectiveness of our anomaly detection in the context of air pollution data.\n\nIn an unsupervised dataset scenario, where we don't have labeled data (`y_test`), the evaluation of an anomaly detection model like Isolation Forest is more about understanding and interpreting the anomalies it detects rather than calculating quantitative metrics. The goal is to examine the anomalies flagged by the model and determine if they align with our domain knowledge or expectations.\n\n#### Detecting Anomalies\nFirst, use the model to predict anomalies in your test set. The Isolation Forest model marks an anomaly with -1 and normal with 1.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nimport numpy as np\n\n# Predict anomalies on the test set\nanomalies = iso_forest.predict(X_test)\n\n# Convert predictions: -1 (anomalies) to 1 and 1 (normal) to 0\nanomalies = np.where(anomalies == -1, 1, 0)\n```\n:::\n\n\n#### Analyzing Detected Anomalies\nThe next step is to analyze these detected anomalies. You might want to look at the proportion of anomalies detected and inspect some of the anomalous data points to see if they make sense.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# Count the number of anomalies detected\nnum_anomalies = np.sum(anomalies)\ntotal_points = len(anomalies)\nprint(f\"Total data points: {total_points}\")\nprint(f\"Number of anomalies detected: {num_anomalies}\")\nprint(f\"Proportion of anomalies detected: {num_anomalies / total_points:.2%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal data points: 126231\nNumber of anomalies detected: 12510\nProportion of anomalies detected: 9.91%\n```\n:::\n:::\n\n\n#### Inspecting Anomalous Data Points\nIt can be insightful to examine the data points that the model flagged as anomalies. This involves looking at the specific characteristics of these data points.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# Create a DataFrame of the test set with a column for anomaly labels\ntest_set_with_predictions = X_test.copy()\ntest_set_with_predictions['Anomaly'] = anomalies\n\n# Display some of the anomalies\nanomalous_data = test_set_with_predictions[test_set_with_predictions['Anomaly'] == 1]\nprint(\"Sample of detected anomalies:\")\nprint(anomalous_data.sample(min(10, len(anomalous_data))))  # Display up to 10 anomalous points\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSample of detected anomalies:\n              No      year     month       day      hour     PM2.5      PM10  \\\n86761  -0.088766  0.286647 -1.601451  0.826169 -1.516862  1.302931  0.871863   \n173564  1.558619  1.136123  1.588154  0.144358  1.227936  3.566037  3.342590   \n103741  1.588751  1.136123  1.588154  1.621615  0.216695  1.327938  1.322085   \n34881   1.714021  1.985599 -1.311487  0.598899 -0.361158  0.615247  0.718129   \n29351   1.167692  1.136123  0.138333 -1.219264  1.661325  0.165126  0.619300   \n60473   0.778247  1.136123 -1.601451  0.826169  0.794547 -0.960175 -0.885098   \n243954  1.584502  1.136123  1.588154  1.394345  0.939010 -0.897658 -0.786269   \n181236 -1.147538 -1.412304  1.298190 -1.560170  0.072232  3.103413  2.793540   \n213991 -1.375653 -1.412304  0.138333  1.507980 -0.650084 -0.134954  0.234965   \n349223  1.592110  1.136123  1.588154  1.735250  1.661325  4.903895  4.385786   \n\n             SO2       NO2        CO  ...  station_Dongsi  station_Guanyuan  \\\n86761   0.241355  1.050349  1.740710  ...           False             False   \n173564  0.708269  2.494666  2.182686  ...           False              True   \n103741 -0.272250  1.454758  2.271082  ...           False             False   \n34881   1.035109  0.617053  0.856757  ...           False             False   \n29351  -0.505707  0.385963 -0.203987  ...           False             False   \n60473  -0.645781 -1.347219 -0.911149  ...           False             False   \n243954 -0.412324 -0.740605 -0.734358  ...           False             False   \n181236 -0.038793  3.216825  1.298733  ...           False             False   \n213991 -0.272436  0.790776 -0.557568  ...           False             False   \n349223 -0.318941  2.581325  3.685406  ...           False             False   \n\n        station_Gucheng  station_Huairou  station_Nongzhanguan  \\\n86761             False            False                 False   \n173564            False            False                 False   \n103741            False            False                 False   \n34881              True            False                 False   \n29351              True            False                 False   \n60473             False             True                 False   \n243954            False            False                 False   \n181236            False            False                  True   \n213991            False            False                 False   \n349223            False            False                 False   \n\n        station_Shunyi  station_Tiantan  station_Wanliu  \\\n86761            False             True           False   \n173564           False            False           False   \n103741           False             True           False   \n34881            False            False           False   \n29351            False            False           False   \n60473            False            False           False   \n243954           False            False            True   \n181236           False            False           False   \n213991           False            False            True   \n349223           False            False           False   \n\n        station_Wanshouxigong  Anomaly  \n86761                   False        1  \n173564                  False        1  \n103741                  False        1  \n34881                   False        1  \n29351                   False        1  \n60473                   False        1  \n243954                  False        1  \n181236                  False        1  \n213991                  False        1  \n349223                  False        1  \n\n[10 rows x 43 columns]\n```\n:::\n:::\n\n\n## Visualization of Anomalies\n\nIn our journey to understand and analyze air pollution data, visualizations play a crucial role, especially when it comes to highlighting and interpreting anomalies. By visually representing the data, we can more easily spot patterns, trends, and outliers that might not be immediately apparent in raw numerical data.\n\n### Creating Visualizations\n\nTo showcase the detected anomalies, we employ various types of visualizations. Here, we'll focus on two primary types: scatter plots and heatmaps. These visualizations will help us to interpret the anomalies in the context of air pollution data.\n\n#### Scatter Plots\nScatter plots are excellent for visualizing the relationship between two variables and identifying points that stand out from the pattern.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assuming 'PM2.5' and 'TEMP' are columns in your dataset\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=anomalous_data, x='TEMP', y='PM2.5', hue='Anomaly')\nplt.title('Scatter Plot of PM2.5 vs Temperature')\nplt.xlabel('Temperature')\nplt.ylabel('PM2.5')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-1.png){width=808 height=523}\n:::\n:::\n\n\nIn this scatter plot, we plot 'PM2.5' levels against 'Temperature', using different colors to distinguish between normal data points and anomalies. Anomalies will stand out in the plot, allowing us to observe if higher pollution levels are associated with specific temperature ranges.\n\n#### Heatmaps\nHeatmaps are useful for understanding the distribution and concentration of data points across two dimensions.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# Create a heatmap to show the concentration of anomalies\n# Sample a subset of the anomalous data for quicker visualization\nsampled_anomalous_data = anomalous_data.sample(min(500, len(anomalous_data)), random_state=42)\n\n# Create a heatmap without annotations for quicker rendering\nplt.figure(figsize=(10, 8))\nsns.heatmap(data=sampled_anomalous_data[['PM2.5', 'PM10', 'SO2', 'NO2', 'CO']])\nplt.title('Heatmap of Pollutant Levels in Anomalous Data (Sampled)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-1.png){width=768 height=653}\n:::\n:::\n\n\nThis heatmap focuses on the levels of various pollutants in the data points identified as anomalies. By examining the heatmap, we can discern if certain pollutants are consistently elevated in these anomalous instances.\n\n### Significance of Visualizations\n\nVisualizations enable us to:\n\n- **Quickly Identify Anomalies**: Graphical representations make it easier to spot outliers or unusual patterns in the data, which might be indicative of environmental issues or data collection anomalies.\n\n- **Understand Relationships and Patterns**: Visualizations help in understanding the relationships between different environmental variables and how these relationships might contribute to anomalous pollution levels.\n\n- **Communicate Findings**: Graphs and charts are effective tools for communicating our findings to a broader audience, including those without a technical background, such as policymakers or the general public.\n\n## Threshold Tuning\n\nIn the realm of anomaly detection, particularly with methods like the Isolation Forest, the concept of threshold tuning is pivotal. The threshold determines the cutoff point at which a data point is classified as an anomaly. Tuning this threshold is a delicate balance, as it directly impacts the sensitivity of our anomaly detection.\n\n### The Process of Threshold Tuning\n\nThreshold tuning involves adjusting the parameters that define what we consider to be anomalous. In the case of the Isolation Forest, this often revolves around the `contamination` parameter, which represents the proportion of outliers we expect in the data.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nfrom sklearn.ensemble import IsolationForest\n\n# Adjusting the contamination parameter\ncontamination_rate = 0.05  # Example rate\niso_forest = IsolationForest(contamination=contamination_rate)\niso_forest.fit(X_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(contamination=0.05)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(contamination=0.05)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\nIn this code snippet, we adjust the `contamination` parameter, which dictates the model's sensitivity to anomalies. A higher contamination rate means the model will be more inclined to flag data points as anomalies.\n\n### Impact on False Positives and False Negatives\n\nThe setting of the threshold has a direct impact on the trade-off between false positives (normal points incorrectly identified as anomalies) and false negatives (actual anomalies not detected).\n\n- **Higher Threshold (Lower Contamination)**: This setting reduces the number of anomalies detected, potentially leading to more false negatives. While it ensures that the flagged anomalies are very likely to be true anomalies, it may miss some subtler, yet significant, anomalies.\n\n- **Lower Threshold (Higher Contamination)**: Conversely, a lower threshold increases the sensitivity, potentially leading to more false positives. This setting might be useful in scenarios where missing an anomaly could have severe consequences, even if it means dealing with more false alarms.\n\n### Balancing the Threshold\n\nFinding the right balance for the threshold is crucial:\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ndef evaluate_model(model, X_test):\n    # Predict anomalies\n    predictions = model.predict(X_test)\n\n    # Convert predictions to a more readable format: -1 (anomalies) to 1, 1 (normal) to 0\n    predictions = np.where(predictions == -1, 1, 0)\n\n    # Count and print the number of anomalies detected\n    num_anomalies = np.sum(predictions)\n    print(f\"Number of anomalies detected: {num_anomalies} out of {len(X_test)} data points\")\n\n# Experimenting with different contamination rates\nfor rate in [0.01, 0.05, 0.1]:\n    iso_forest = IsolationForest(contamination=rate)\n    iso_forest.fit(X_train)\n    # Evaluate the model\n    evaluate_model(iso_forest, X_test)\n    print(\"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of anomalies detected: 1293 out of 126231 data points\n\n\nNumber of anomalies detected: 6356 out of 126231 data points\n\n\nNumber of anomalies detected: 12702 out of 126231 data points\n\n\n```\n:::\n:::\n\n\nIn this example, we experiment with different contamination rates to observe how the model's performance changes. The ideal rate often depends on the specific context of the problem and the cost of false positives versus false negatives.\n\n## Interpretation and Real-World Implications\nBased on our analysis of the air pollution data using the Isolation Forest model, we've uncovered some intriguing insights. Out of the total 126,231 data points, our model identified 12,510 as anomalies, accounting for approximately 9.91% of the dataset. This proportion of anomalies is significant and warrants further investigation.\n\n### Interpretation of Detected Anomalies\n\nWhen we delve into the sample of detected anomalies, several key observations emerge:\n\n1. **Elevated Pollutant Levels**: Many of the anomalies exhibit unusually high levels of pollutants such as PM2.5, PM10, NO2, and CO. For instance, rows like 8468 and 314995 show pollutant concentrations several times higher than typical readings. This could indicate episodes of extreme pollution, possibly due to specific environmental events or human activities.\n\n2. **Meteorological Influences**: The anomalies also reveal interesting patterns in meteorological conditions. For example, rows 214246 and 244058 show variations in temperature, pressure, and humidity, which could be influencing factors for the high pollution levels observed.\n\n3. **Station-Specific Anomalies**: The data points flagged as anomalies are distributed across different monitoring stations, as seen in the 'station' columns. This distribution suggests that the detected anomalies are not confined to a specific location but are rather widespread, indicating a more systemic issue in air quality.\n\n4. **Temporal Patterns**: The presence of anomalies across different years, months, and hours, such as in rows 216296 and 392113, hints at temporal patterns in air pollution. These patterns could be aligned with seasonal changes, urban activities, or policy changes affecting air quality.\n\n### Implications\n\n- **Environmental Policy and Health**: The identified anomalies are crucial for understanding the dynamics of air pollution. They can inform environmental policies, especially in devising strategies to mitigate high pollution episodes.\n\n- **Further Research**: These findings can be a starting point for more detailed research. For example, investigating the causes behind high pollution episodes can help in understanding the impact of urban development, traffic patterns, or industrial activities on air quality.\n\n- **Public Awareness**: Disseminating information about such high pollution episodes can raise public awareness and encourage preventive measures, especially for vulnerable populations.\n\nIn summary, our analysis using the Isolation Forest model provides us with valuable insights into the air quality data, highlighting instances of unusually high pollution levels. This information is crucial for environmental monitoring, policy-making, and public health initiatives.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}