{
  "hash": "10cd3b522f8237e3f29acd0a7b006d76",
  "result": {
    "markdown": "---\ntitle: 5. Anomaly/Outlier Detection\nauthor: Joanna Fang\ndate: '2023-12-06'\ncategories:\n  - ml\n  - code\n  - anomaly detection\n  - outlier detection\n  - pollution\nformat:\n  html:\n    code-block-bg: '#FFFFFF'\n    code-block-border-left: '#E83283'\n    toc: true\n    code-tools:\n      source: true\n      toggle: false\n      caption: none\n---\n\n# Detecting Anomalies in Air Pollution Data: A Data Science Project\n\n![](thumbnail.jpg){width=\"50%\" fig-align=\"center\"}\n\n## Introduction\n\nWelcome to our exploration of \"Detecting Anomalies in Air Pollution Data,\" a vital project in the realm of environmental monitoring. With increasing concerns about air quality and its impact on public health and the environment, identifying irregularities in air pollution data has never been more critical.\n\nThis project leverages a comprehensive dataset from the Beijing Multi-site Air Quality Data, which offers a rich tapestry of air pollutant measurements and meteorological data across various sites in Beijing. The data spans from 2013 to 2017, providing insights into pollutants like PM2.5, PM10, SO2, NO2, and CO, as well as meteorological conditions like temperature, humidity, and wind speed.\n\nOur primary goal is to detect unusual patterns or outliers in air quality data that might signify environmental hazards, technical errors in data collection, or significant meteorological impacts. By accomplishing this, we aim to contribute to more effective environmental monitoring and policy-making.\n\n## Data Exploration and Preprocessing\n\n### Understanding the Dataset\nThe first step in our data science journey involves getting acquainted with the dataset's structure and characteristics. This involves examining the various columns of the dataset, which include both pollutant levels and meteorological factors.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Install and import all libraries\nimport sys\n!{sys.executable} -m pip install seaborn\n!{sys.executable} -m pip install matplotlib\n!{sys.executable} -m pip install scikit-learn\n!{sys.executable} -m pip install pandas\n!{sys.executable} -m pip install numpy\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import IsolationForest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: seaborn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.13.0)\r\nRequirement already satisfied: matplotlib!=3.6.1,>=3.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (3.8.2)\r\nRequirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (1.26.2)\r\nRequirement already satisfied: pandas>=1.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (2.1.3)\r\nRequirement already satisfied: cycler>=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (0.12.1)\r\nRequirement already satisfied: packaging>=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (23.2)\r\nRequirement already satisfied: importlib-resources>=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (6.1.1)\r\nRequirement already satisfied: pillow>=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (10.1.0)\r\nRequirement already satisfied: fonttools>=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (4.45.1)\r\nRequirement already satisfied: kiwisolver>=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.4.5)\r\nRequirement already satisfied: python-dateutil>=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\r\nRequirement already satisfied: pyparsing>=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.1.1)\r\nRequirement already satisfied: contourpy>=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.2.0)\r\nRequirement already satisfied: zipp>=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.3->seaborn) (3.17.0)\r\nRequirement already satisfied: pytz>=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\r\nRequirement already satisfied: tzdata>=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas>=1.2->seaborn) (2023.3)\r\nRequirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.15.0)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: matplotlib in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (3.8.2)\r\nRequirement already satisfied: importlib-resources>=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.1.1)\r\nRequirement already satisfied: pillow>=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.1.0)\r\nRequirement already satisfied: fonttools>=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.45.1)\r\nRequirement already satisfied: cycler>=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\r\nRequirement already satisfied: python-dateutil>=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)\r\nRequirement already satisfied: packaging>=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (23.2)\r\nRequirement already satisfied: pyparsing>=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.1)\r\nRequirement already satisfied: kiwisolver>=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)\r\nRequirement already satisfied: numpy<2,>=1.21 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.2)\r\nRequirement already satisfied: contourpy>=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.0)\r\nRequirement already satisfied: zipp>=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\r\nRequirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: scikit-learn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.3.2)\r\nRequirement already satisfied: numpy<2.0,>=1.17.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.2)\r\nRequirement already satisfied: scipy>=1.5.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.11.4)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.2.0)\r\nRequirement already satisfied: joblib>=1.1.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.3.2)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: pandas in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (2.1.3)\r\nRequirement already satisfied: numpy<2,>=1.22.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)\r\nRequirement already satisfied: tzdata>=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\r\nRequirement already satisfied: pytz>=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)\r\nRequirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: numpy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.26.2)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nsample_data_org = pd.read_csv('air_data_all.csv')\nsample_data_org.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>No</th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>hour</th>\n      <th>PM2.5</th>\n      <th>PM10</th>\n      <th>SO2</th>\n      <th>NO2</th>\n      <th>CO</th>\n      <th>O3</th>\n      <th>TEMP</th>\n      <th>PRES</th>\n      <th>DEWP</th>\n      <th>RAIN</th>\n      <th>wd</th>\n      <th>WSPM</th>\n      <th>station</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2013</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>6.0</td>\n      <td>18.0</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>800.0</td>\n      <td>88.0</td>\n      <td>0.1</td>\n      <td>1021.1</td>\n      <td>-18.6</td>\n      <td>0.0</td>\n      <td>NW</td>\n      <td>4.4</td>\n      <td>Gucheng</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2013</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>6.0</td>\n      <td>15.0</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>800.0</td>\n      <td>88.0</td>\n      <td>-0.3</td>\n      <td>1021.5</td>\n      <td>-19.0</td>\n      <td>0.0</td>\n      <td>NW</td>\n      <td>4.0</td>\n      <td>Gucheng</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>2013</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>5.0</td>\n      <td>18.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>700.0</td>\n      <td>52.0</td>\n      <td>-0.7</td>\n      <td>1021.5</td>\n      <td>-19.8</td>\n      <td>0.0</td>\n      <td>WNW</td>\n      <td>4.6</td>\n      <td>Gucheng</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2013</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>6.0</td>\n      <td>20.0</td>\n      <td>6.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1.0</td>\n      <td>1022.7</td>\n      <td>-21.2</td>\n      <td>0.0</td>\n      <td>W</td>\n      <td>2.8</td>\n      <td>Gucheng</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>2013</td>\n      <td>3</td>\n      <td>1</td>\n      <td>4</td>\n      <td>5.0</td>\n      <td>17.0</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>600.0</td>\n      <td>73.0</td>\n      <td>-1.3</td>\n      <td>1023.0</td>\n      <td>-21.4</td>\n      <td>0.0</td>\n      <td>WNW</td>\n      <td>3.6</td>\n      <td>Gucheng</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nBy running this code, we get a glimpse of the first few rows of our dataset, allowing us to understand the types of data we will be working with.\n\n### Handling Missing Data and Categorical Variables\nDealing with missing data and categorical variables is a crucial part of data preprocessing. To address this, we first identify the missing values and then decide on an appropriate strategy, such as imputation or removal. However, in later processing, we realized we hadn't done enough here. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nmissing_values = sample_data_org.isnull().sum()\nmissing_values\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nNo             0\nyear           0\nmonth          0\nday            0\nhour           0\nPM2.5       8739\nPM10        6449\nSO2         9021\nNO2        12116\nCO         20701\nO3         13277\nTEMP         398\nPRES         393\nDEWP         403\nRAIN         390\nwd          1822\nWSPM         318\nstation        0\ndtype: int64\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Remove rows with missing values\nsample_data = sample_data_org.dropna()\n\n# Identify numerical columns\nnumerical_cols = sample_data.select_dtypes(include=['int64', 'float64']).columns\n\n# Create a pipeline for imputing missing values and scaling\npipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),  # Replace missing values with mean\n    ('scaler', StandardScaler()),                # Scale the data\n])\n\n# Apply the pipeline to the numerical columns\nscaled_data = pipeline.fit_transform(sample_data[numerical_cols])\n\n# Apply PCA\npca = PCA(n_components=0.95)  # Retain 95% of the variance\nprincipal_components = pca.fit_transform(scaled_data)\n\n# Identify non-numeric (categorical) columns\ncategorical_cols = sample_data.select_dtypes(include=['object']).columns\n\n# One-hot encode the categorical data\nencoder = OneHotEncoder(sparse=False)\ncategorical_encoded = encoder.fit_transform(sample_data[categorical_cols])\n\n# Check for 'get_feature_names_out' method for naming columns\nif hasattr(encoder, 'get_feature_names_out'):\n    encoded_columns = pd.DataFrame(categorical_encoded, columns=encoder.get_feature_names_out(categorical_cols))\nelse:\n    # Fallback: manually create feature names\n    encoded_columns = pd.DataFrame(categorical_encoded)\n    encoded_columns.columns = [col + '_' + str(i) for col in categorical_cols for i in range(encoded_columns.shape[1])]\n\n# Concatenate the encoded columns with the original dataset and drop the original categorical columns\nsample_data_encoded = pd.concat([sample_data.drop(categorical_cols, axis=1), encoded_columns], axis=1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n```\n:::\n:::\n\n\nFor categorical variables like wind direction, we use encoding techniques to convert them into numerical form, making them suitable for analysis.\n\n### Normalization and Standardization\nGiven the varying scales of our numerical features, normalization or standardization becomes necessary. This step ensures that no single feature disproportionately influences the model due to its scale.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(sample_data[['PM2.5', 'PM10', 'TEMP', 'PRES']])\n```\n:::\n\n\n### Feature Selection and Engineering\nFinally, we perform feature selection and engineering. This process involves attempting to choose the most relevant features and possibly creating new features to improve our model's performance.\n\n1. **Correlation Analysis**: First, we can perform a correlation analysis to understand the relationships between different features. This can help in identifying features that are strongly correlated with each other, from which we can select the most relevant ones.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Now perform the correlation analysis on the numerical data\ncorr = sample_data_encoded.corr()\n\n# Generate a heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, fmt=\".2f\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=879 height=764}\n:::\n:::\n\n\nThe heatmap visualizes the correlation coefficients between different variables related to air quality and weather conditions. The scale on the right indicates the strength of the correlation, ranging from -1 (a perfect negative correlation) to 1 (a perfect positive correlation). Dark red shades indicate strong positive correlations, whereas dark blue shades represent strong negative correlations. Most variables do not exhibit a strong correlation with each other, as indicated by the shades of white (near-zero correlation). This kind of visualization helps to identify relationships between variables, which can be further investigated for causal connections or dependencies.\n\n2. **Principal Component Analysis (PCA)**: PCA is a technique used to reduce the dimensionality of the data, enhancing the interpretability while minimizing information loss.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\npca = PCA(n_components=0.95) # Retain 95% of the variance\nprincipal_components = pca.fit_transform(scaled_data)\n```\n:::\n\n\nThis code applies PCA to the scaled data, reducing the number of features while retaining 95% of the variance in the data.\n\n3. **Feature Engineering**: Here, we are attempting to create a new feature that might be more indicative of anomalies by creating a composite air quality index from multiple pollutants.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nsample_data['Air_Quality_Index'] = sample_data['PM2.5'] * 0.4 + sample_data['PM10'] * 0.2 + sample_data['NO2'] * 0.2 + sample_data['SO2'] * 0.1 + sample_data['CO'] * 0.1\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/pt/983cdyd950gd2j6l1gv1376r0000gn/T/ipykernel_18793/3115101989.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  sample_data['Air_Quality_Index'] = sample_data['PM2.5'] * 0.4 + sample_data['PM10'] * 0.2 + sample_data['NO2'] * 0.2 + sample_data['SO2'] * 0.1 + sample_data['CO'] * 0.1\n```\n:::\n:::\n\n\nThis code creates a new feature, 'Air_Quality_Index', as a weighted sum of various pollutants, hypothesizing that this composite index might be a more effective predictor of anomalies.\n\nThrough these steps, we attempted to refine our dataset to include the most relevant features for anomaly detection, enhancing the model's accuracy and efficiency.\n\n## Anomaly Detection Algorithms and Model Training and Evaluation\n\n### Choosing the Anomaly Detection Algorithm: Isolation Forest\nAfter trying out other models, which took more than 5 minutes to run, for our project on air pollution data, we have opted for the Isolation Forest algorithm due to its efficiency and effectiveness, especially in dealing with large and high-dimensional datasets like ours.  \n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\niso_forest = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)\n```\n:::\n\n\n### Data Preprocessing and Splitting\n\nWe split our dataset into training and test sets, ensuring that the model is evaluated on unseen data, reflecting its performance in real-world scenarios.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Handling NaN Values with Imputation\n# Impute missing values and then scale the numerical columns\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),  # Replace missing values with mean\n    ('std_scaler', StandardScaler())\n])\n# Apply the pipeline to the numerical columns\nnumerical_cols = sample_data_org.select_dtypes(include=['int64', 'float64']).columns\nsample_data_org[numerical_cols] = num_pipeline.fit_transform(sample_data_org[numerical_cols])\n# One-hot encode the categorical columns\ncategorical_cols = sample_data_org.select_dtypes(include=['object']).columns\nsample_data_org = pd.get_dummies(sample_data_org, columns=categorical_cols, drop_first=True)\n\n# Spliting the Dataset \nX_train, X_test = train_test_split(sample_data_org, test_size=0.3, random_state=42)\n```\n:::\n\n\n### Training the Model\nThe training process involves fitting the Isolation Forest model to our training data.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\niso_forest.fit(X_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(contamination=0.1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(contamination=0.1, random_state=42)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n### Evaluation Metrics\n\n### Model Evaluation and Insights\nAfter training, we assess the model's performance on the test set. This evaluation helps us understand the effectiveness of our anomaly detection in the context of air pollution data.\n\nIn an unsupervised dataset scenario, where we don't have labeled data (`y_test`), the evaluation of an anomaly detection model like Isolation Forest is more about understanding and interpreting the anomalies it detects rather than calculating quantitative metrics. The goal is to examine the anomalies flagged by the model and determine if they align with our domain knowledge or expectations.\n\n#### Detecting Anomalies\nFirst, use the model to predict anomalies in your test set. The Isolation Forest model marks an anomaly with -1 and normal with 1.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# Predict anomalies on the test set\nanomalies = iso_forest.predict(X_test)\n\n# Convert predictions: -1 (anomalies) to 1 and 1 (normal) to 0\nanomalies = np.where(anomalies == -1, 1, 0)\n```\n:::\n\n\n#### Analyzing Detected Anomalies\nThe next step is to analyze these detected anomalies. \n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# Count the number of anomalies detected\nnum_anomalies = np.sum(anomalies)\ntotal_points = len(anomalies)\nprint(f\"Total data points: {total_points}\")\nprint(f\"Number of anomalies detected: {num_anomalies}\")\nprint(f\"Proportion of anomalies detected: {num_anomalies / total_points:.2%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal data points: 126231\nNumber of anomalies detected: 12510\nProportion of anomalies detected: 9.91%\n```\n:::\n:::\n\n\n#### Inspecting Anomalous Data Points\nIt can be insightful to examine the data points that the model flagged as anomalies. This involves looking at the specific characteristics of these data points.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n# Create a DataFrame of the test set with a column for anomaly labels\ntest_set_with_predictions = X_test.copy()\ntest_set_with_predictions['Anomaly'] = anomalies\n\n# Display some of the anomalies\nanomalous_data = test_set_with_predictions[test_set_with_predictions['Anomaly'] == 1]\nprint(\"Sample of detected anomalies:\")\nprint(anomalous_data.sample(min(10, len(anomalous_data))))  # Display up to 10 anomalous points\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSample of detected anomalies:\n              No      year     month       day      hour     PM2.5      PM10  \\\n227923  0.000741  0.286647 -1.021523 -1.673805  1.083473 -0.597578 -0.325067   \n7715   -0.969808 -0.562829 -1.601451  0.030723 -0.072232  3.903627  3.166894   \n262102 -0.086691  0.286647 -1.601451  0.826169  1.516862  2.590776  2.002907   \n104857  1.699004  1.985599 -1.311487 -0.082912 -1.516862  1.928098  1.596610   \n2914   -1.444117 -1.412304 -0.151631  1.621615 -0.216695  2.478246  1.827211   \n68714   1.592406  1.985599 -1.601451 -1.673805 -1.372399  1.828072  1.926040   \n243867  1.575907  1.136123  1.588154  1.053439 -1.227936  1.015354  0.740091   \n219210 -0.860048 -0.562829 -1.021523 -1.446535  0.939010  2.190669  2.266451   \n213542 -1.420011 -1.412304  0.138333 -0.651088  0.361158 -0.635088 -0.797250   \n34431   1.669564  1.985599 -1.311487 -1.560170  0.505621 -0.109947 -0.270162   \n\n             SO2       NO2        CO  ...  station_Dongsi  station_Guanyuan  \\\n227923 -0.178867  0.472622 -0.469173  ...           False             False   \n7715    6.591382  3.332371  4.038987  ...           False             False   \n262102  2.482541  2.783530  3.420220  ...            True             False   \n104857 -0.365632  2.032485  2.005896  ...           False             False   \n2914   -0.505707  0.877031  2.094291  ...           False             False   \n68714  -0.552398  0.530394  1.652314  ...           False             False   \n243867  0.474812  1.743621  2.005896  ...           False             False   \n219210  4.817110  3.476802  3.243430  ...           False             False   \n213542 -0.592478 -0.454601 -0.734358  ...           False             False   \n34431   0.428121  0.270417  0.061199  ...           False             False   \n\n        station_Gucheng  station_Huairou  station_Nongzhanguan  \\\n227923            False            False                 False   \n7715               True            False                 False   \n262102            False            False                 False   \n104857            False            False                 False   \n2914               True            False                 False   \n68714             False             True                 False   \n243867            False            False                 False   \n219210            False            False                 False   \n213542            False            False                 False   \n34431              True            False                 False   \n\n        station_Shunyi  station_Tiantan  station_Wanliu  \\\n227923           False            False            True   \n7715             False            False           False   \n262102           False            False           False   \n104857           False             True           False   \n2914             False            False           False   \n68714            False            False           False   \n243867           False            False            True   \n219210           False            False            True   \n213542           False            False            True   \n34431            False            False           False   \n\n        station_Wanshouxigong  Anomaly  \n227923                  False        1  \n7715                    False        1  \n262102                  False        1  \n104857                  False        1  \n2914                    False        1  \n68714                   False        1  \n243867                  False        1  \n219210                  False        1  \n213542                  False        1  \n34431                   False        1  \n\n[10 rows x 43 columns]\n```\n:::\n:::\n\n\n## Visualization of Anomalies\n\n### Creating Visualizations\n\nTo showcase the detected anomalies, we employ various types of visualizations. Here, we'll focus on two primary types: scatter plots and heatmaps. These visualizations will help us to interpret the anomalies in the context of air pollution data.\n\n#### Scatter Plots\nScatter plots are excellent for visualizing the relationship between two variables and identifying points that stand out from the pattern.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=anomalous_data, x='TEMP', y='PM2.5', hue='Anomaly')\nplt.title('Scatter Plot of PM2.5 vs Temperature')\nplt.xlabel('Temperature')\nplt.ylabel('PM2.5')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-1.png){width=808 height=523}\n:::\n:::\n\n\nThe scatter plot visualizes PM2.5 levels against temperature, with each point representing an observation. The data points are standardized, as indicated by the temperature axis ranging from approximately -2 to 2. A dense clustering of points suggests that lower PM2.5 levels are common across the temperature range, with a noticeable spread in higher PM2.5 levels at mid-range temperatures. Points that stand out from the dense cloud indicate potential anomalies with higher PM2.5 levels, which could be of interest for further investigation into air quality issues at different temperatures.\n\n#### Heatmaps\nHeatmaps are useful for understanding the distribution and concentration of data points across two dimensions.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# Create a heatmap to show the concentration of anomalies\n# Sample a subset of the anomalous data for quicker visualization\nsampled_anomalous_data = anomalous_data.sample(min(500, len(anomalous_data)), random_state=42)\n\n# Create a heatmap without annotations for quicker rendering\nplt.figure(figsize=(10, 8))\nsns.heatmap(data=sampled_anomalous_data[['PM2.5', 'PM10', 'SO2', 'NO2', 'CO']])\nplt.title('Heatmap of Pollutant Levels in Anomalous Data (Sampled)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-1.png){width=768 height=653}\n:::\n:::\n\n\nThe heatmap depicts pollutant levels in a subset of data identified as anomalous, with rows representing individual instances and columns for various pollutants. The color scale, ranging from dark purple to bright orange, illustrates the intensity of pollutant levels, with orange indicating higher concentrations. The stark contrast across the rows, primarily in the deep purple range, suggests that most selected data points do not have extremely high pollutant levels. However, occasional streaks of orange and red reveal instances where one or more pollutants reach notably higher levels, warranting closer scrutiny for potential environmental health concerns.\n\n## Threshold Tuning\n\nIn the realm of anomaly detection, particularly with methods like the Isolation Forest, the concept of threshold tuning is pivotal. The threshold determines the cutoff point at which a data point is classified as an anomaly. Tuning this threshold is a delicate balance, as it directly impacts the sensitivity of our anomaly detection.\n\n### The Process of Threshold Tuning\n\nThreshold tuning involves adjusting the parameters that define what we consider to be anomalous. In the case of the Isolation Forest, this often revolves around the `contamination` parameter, which represents the proportion of outliers we expect in the data.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# Adjusting the contamination parameter\ncontamination_rate = 0.05  # Example rate\niso_forest = IsolationForest(contamination=contamination_rate)\niso_forest.fit(X_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(contamination=0.05)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(contamination=0.05)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\nIn this code snippet, we adjust the `contamination` parameter, which dictates the model's sensitivity to anomalies. A higher contamination rate means the model will be more inclined to flag data points as anomalies.\n\n### Impact on False Positives and False Negatives\n\nThe setting of the threshold has a direct impact on the trade-off between false positives (normal points incorrectly identified as anomalies) and false negatives (actual anomalies not detected).\n\n- **Higher Threshold (Lower Contamination)**: This setting reduces the number of anomalies detected, potentially leading to more false negatives. While it ensures that the flagged anomalies are very likely to be true anomalies, it may miss some subtler, yet significant, anomalies.\n\n- **Lower Threshold (Higher Contamination)**: Conversely, a lower threshold increases the sensitivity, potentially leading to more false positives. This setting might be useful in scenarios where missing an anomaly could have severe consequences, even if it means dealing with more false alarms.\n\n### Balancing the Threshold\n\nFinding the right balance for the threshold is crucial:\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\ndef evaluate_model(model, X_test):\n    # Predict anomalies\n    predictions = model.predict(X_test)\n\n    # Convert predictions to a more readable format: -1 (anomalies) to 1, 1 (normal) to 0\n    predictions = np.where(predictions == -1, 1, 0)\n\n    # Count and print the number of anomalies detected\n    num_anomalies = np.sum(predictions)\n    print(f\"Number of anomalies detected: {num_anomalies} out of {len(X_test)} data points\")\n\n# Experimenting with different contamination rates\nfor rate in [0.01, 0.05, 0.1]:\n    iso_forest = IsolationForest(contamination=rate)\n    iso_forest.fit(X_train)\n    # Evaluate the model\n    evaluate_model(iso_forest, X_test)\n    print(\"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of anomalies detected: 1284 out of 126231 data points\n\n\nNumber of anomalies detected: 6391 out of 126231 data points\n\n\nNumber of anomalies detected: 12796 out of 126231 data points\n\n\n```\n:::\n:::\n\n\nIn our exploration of the optimal contamination rate for the Isolation Forest model, we experimented with various rates and observed their impact on anomaly detection in our dataset of 126,231 data points. When we set the contamination rate at 0.01, our model identified 1,293 anomalies, suggesting a more conservative approach to anomaly detection. Increasing the rate to 0.05 led to a significant rise in detected anomalies, totaling 6,356, indicating a moderate level of sensitivity. Further amplifying the rate to 0.1 resulted in the detection of 12,702 anomalies, reflecting a highly sensitive setting that captures a broader spectrum of potential anomalies. These varying results illustrate the crucial influence of the contamination rate on the model's behavior, underscoring the importance of fine-tuning this parameter to strike a balance between identifying true anomalies and avoiding excessive false positives. Our analysis highlights the need for a thoughtful approach to setting this threshold, considering both the nature of our data and the specific requirements of our air quality monitoring objectives.\n\n## Interpretation and Real-World Implications\nBased on our analysis of the air pollution data using the Isolation Forest model, we've uncovered some intriguing insights. Out of the total 126,231 data points, our model identified 12,510 as anomalies, accounting for approximately 9.91% of the dataset. This proportion of anomalies is significant and warrants further investigation.\n\n### Interpretation of Detected Anomalies\n\nWhen we delve into the sample of detected anomalies, several key observations emerge:\n\n1. **Elevated Pollutant Levels**: Many of the anomalies exhibit unusually high levels of pollutants such as PM2.5, PM10, NO2, and CO. For instance, rows like 8468 and 314995 show pollutant concentrations several times higher than typical readings. This could indicate episodes of extreme pollution, possibly due to specific environmental events or human activities.\n\n2. **Meteorological Influences**: The anomalies also reveal interesting patterns in meteorological conditions. For example, rows 214246 and 244058 show variations in temperature, pressure, and humidity, which could be influencing factors for the high pollution levels observed.\n\n3. **Station-Specific Anomalies**: The data points flagged as anomalies are distributed across different monitoring stations, as seen in the 'station' columns. This distribution suggests that the detected anomalies are not confined to a specific location but are rather widespread, indicating a more systemic issue in air quality.\n\n4. **Temporal Patterns**: The presence of anomalies across different years, months, and hours, such as in rows 216296 and 392113, hints at temporal patterns in air pollution. These patterns could be aligned with seasonal changes, urban activities, or policy changes affecting air quality.\n\n### Implications\n\n- **Environmental Policy and Health**: The identified anomalies are crucial for understanding the dynamics of air pollution. They can inform environmental policies, especially in devising strategies to mitigate high pollution episodes.\n\n- **Further Research**: These findings can be a starting point for more detailed research. For example, investigating the causes behind high pollution episodes can help in understanding the impact of urban development, traffic patterns, or industrial activities on air quality.\n\n- **Public Awareness**: Disseminating information about such high pollution episodes can raise public awareness and encourage preventive measures, especially for vulnerable populations.\n\nIn summary, our analysis using the Isolation Forest model provides us with valuable insights into the air quality data, highlighting instances of unusually high pollution levels. This information is crucial for environmental monitoring, policy-making, and public health initiatives.\n\n## Reference \nhttps://www.knowledgehut.com/blog/data-science/machine-learning-for-anomaly-detection \nhttps://www.techtarget.com/searchenterpriseai/definition/anomaly-detection\nhttps://medium.com/@corymaklin/isolation-forest-799fceacdda4\nOpenAI. (2023). ChatGPT [Large language model]. https://chat.openai.com\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}