{
  "hash": "c44c70682e6ed36eee7fa238b9f67473",
  "result": {
    "markdown": "---\ntitle: Linear and Nonlinear Regression\nauthor: Joanna Fang\ndate: '2023-11-29'\ncategories:\n  - ml\n  - code\n  - linear regression\n  - nonlinear regression\n  - driving\n  - kaggle\nformat:\n  html:\n    toc: true\n    code-block-bg: '#FFFFFF'\n    code-block-border-left: '#E83283'\n    code-tools:\n      source: true\n      toggle: false\n      caption: none\n---\n\n# Linear and Nonlinear Regression: Predictive Analysis of Driving Behavior \n\n![](thumbnail.jpg){width=\"50%\" fig-align=\"center\"}\n\n## Introduction\n\nPredictive analysis has become a cornerstone in various fields, including transportation and automotive safety. At the heart of this analysis lies the ability to predict future outcomes based on historical data. In the realm of driving behavior, predictive analysis opens the door to understanding and anticipating a driver's actions, which can be crucial for enhancing road safety, improving vehicle design, and tailoring driver assistance systems.\n\nIn this blog, we delve into the fascinating world of predictive analysis of driving behavior. We utilize a rich dataset available on Kaggle, which provides a detailed collection of driving data. This dataset includes several key features:\n\n- **Acceleration**: Measured across the X, Y, and Z axes in meters per second squared (\\( \\text{m/s}^2 \\)), these values provide insight into the forward, lateral, and vertical movements of the vehicle.\n- **Rotation**: Also across the X, Y, and Z axes but measured in degrees per second (\\( \\text{°/s} \\)), offering a perspective on how the vehicle is turning or tilting.\n- **Classification Labels**: Each data point is tagged with labels such as SLOW, NORMAL, or AGGRESSIVE, giving us a qualitative assessment of the driving behavior.\n- **Timestamps**: These indicate the time at which each reading was taken, allowing us to analyze the data in a time-series context.\n\nThe overarching goal of our exploration is to employ and compare different regression models to predict driving behavior effectively. We aim to understand how well linear regression models perform against more complex, nonlinear models such as polynomial regression and Support Vector Machines (SVMs) in categorizing driving patterns into slow, normal, or aggressive. This comparison will not only highlight the strengths and limitations of each model but also provide valuable insights into the dynamics of driving behavior.\n\nJoin us as we embark on this analytical journey, where data meets the road, and predictions pave the way for understanding the nuances of driving behaviors.\n\n## Overview of the Kaggle Dataset\n\nThe dataset we're utilizing, sourced from Kaggle, provides an extensive and detailed collection of driving data. It's a rich dataset that captures various aspects of driving behavior through several key features:\n\n- **Acceleration (X, Y, Z)**: These features are measured in meters per second squared ($\\text{m/s}^2$). The X-axis typically represents forward or backward acceleration, the Y-axis indicates left or right movement, and the Z-axis captures upward or downward motion. In the context of driving, acceleration data can reveal how smoothly or abruptly a vehicle is speeding up or slowing down, which can be indicative of driving style – gentle or aggressive.\n\n- **Rotation (X, Y, Z)**: Rotation data is measured in degrees per second ($\\text{°/s}$). These measurements provide insights into the angular velocity around each of the three axes: roll (X-axis), pitch (Y-axis), and yaw (Z-axis). Understanding these rotational movements is crucial in analyzing maneuvers such as sharp turns, sudden lane changes, or even the stability of the vehicle on different terrains.\n\n- **Classification Labels (SLOW, NORMAL, AGGRESSIVE)**: Each instance in the dataset is classified into one of these three categories. This classification helps in contextualizing the numerical data (acceleration and rotation) into distinct driving behaviors, making it easier to correlate specific patterns with particular driving styles.\n\n- **Timestamps**: The time at which each data point was recorded, typically in seconds. Timestamps are vital for time-series analysis, allowing us to track changes in driving behavior over time and identify any patterns or anomalies.\n\nTo begin our analysis, we first need to load and display a portion of the dataset. This gives us a preliminary view of the data structure and the type of information we're dealing with. Let’s look at the code snippet that accomplishes this in a Jupyter Notebook:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport sys\n# !{sys.executable} -m pip install numpy\n!{sys.executable} -m pip install matplotlib\n# !{sys.executable} -m pip install scipy\n!{sys.executable} -m pip install scikit-learn\n!{sys.executable} -m pip install pandas\n!{sys.executable} -m pip install seaborn\n\nimport pandas as pd\n\n# Load the datasets\ndata_1 = pd.read_csv('motion_data_1.csv')\ndata_2 = pd.read_csv('motion_data_2.csv')\n\n# Combine datasets\ndata = pd.concat([data_1, data_2])\n\n# Display the first few rows of the dataset\ndata.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: matplotlib in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (3.8.2)\r\nRequirement already satisfied: fonttools>=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.45.1)\r\nRequirement already satisfied: packaging>=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (23.2)\r\nRequirement already satisfied: python-dateutil>=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)\r\nRequirement already satisfied: pillow>=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.1.0)\r\nRequirement already satisfied: pyparsing>=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.1)\r\nRequirement already satisfied: cycler>=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\r\nRequirement already satisfied: contourpy>=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.0)\r\nRequirement already satisfied: importlib-resources>=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.1.1)\r\nRequirement already satisfied: numpy<2,>=1.21 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.2)\r\nRequirement already satisfied: kiwisolver>=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)\r\nRequirement already satisfied: zipp>=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\r\nRequirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: scikit-learn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.3.2)\r\nRequirement already satisfied: numpy<2.0,>=1.17.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.2)\r\nRequirement already satisfied: joblib>=1.1.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.3.2)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.2.0)\r\nRequirement already satisfied: scipy>=1.5.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.11.4)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: pandas in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (2.1.3)\r\nRequirement already satisfied: tzdata>=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\r\nRequirement already satisfied: numpy<2,>=1.22.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)\r\nRequirement already satisfied: pytz>=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)\r\nRequirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: seaborn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.13.0)\r\nRequirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (1.26.2)\r\nRequirement already satisfied: matplotlib!=3.6.1,>=3.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (3.8.2)\r\nRequirement already satisfied: pandas>=1.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (2.1.3)\r\nRequirement already satisfied: packaging>=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (23.2)\r\nRequirement already satisfied: python-dateutil>=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\r\nRequirement already satisfied: pillow>=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (10.1.0)\r\nRequirement already satisfied: fonttools>=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (4.45.1)\r\nRequirement already satisfied: importlib-resources>=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (6.1.1)\r\nRequirement already satisfied: pyparsing>=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.1.1)\r\nRequirement already satisfied: kiwisolver>=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.4.5)\r\nRequirement already satisfied: contourpy>=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.2.0)\r\nRequirement already satisfied: cycler>=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (0.12.1)\r\nRequirement already satisfied: zipp>=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.3->seaborn) (3.17.0)\r\nRequirement already satisfied: pytz>=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\r\nRequirement already satisfied: tzdata>=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas>=1.2->seaborn) (2023.3)\r\nRequirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.15.0)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AccX</th>\n      <th>AccY</th>\n      <th>AccZ</th>\n      <th>GyroX</th>\n      <th>GyroY</th>\n      <th>GyroZ</th>\n      <th>Class</th>\n      <th>Timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.758194</td>\n      <td>-0.217791</td>\n      <td>0.457263</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>AGGRESSIVE</td>\n      <td>818922</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.667560</td>\n      <td>-0.038610</td>\n      <td>0.231416</td>\n      <td>-0.054367</td>\n      <td>-0.007712</td>\n      <td>0.225257</td>\n      <td>AGGRESSIVE</td>\n      <td>818923</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.724449</td>\n      <td>-7.584121</td>\n      <td>2.390926</td>\n      <td>0.023824</td>\n      <td>0.013668</td>\n      <td>-0.038026</td>\n      <td>AGGRESSIVE</td>\n      <td>818923</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.330950</td>\n      <td>-7.621754</td>\n      <td>2.529024</td>\n      <td>0.056810</td>\n      <td>-0.180587</td>\n      <td>-0.052076</td>\n      <td>AGGRESSIVE</td>\n      <td>818924</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.847215</td>\n      <td>-6.755621</td>\n      <td>2.224640</td>\n      <td>-0.031765</td>\n      <td>-0.035201</td>\n      <td>0.035277</td>\n      <td>AGGRESSIVE</td>\n      <td>818924</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThis code block imports the pandas library, which is instrumental in data manipulation and analysis. We then load the dataset and store it in a DataFrame `data`. Finally, using `data.head()`, we display the first few rows of the dataset to get an initial understanding of its structure and the type of data it contains. This step is crucial as it sets the stage for the subsequent data exploration and preprocessing tasks.\n\n## Data Exploration and Preprocessing\n\nWith the dataset successfully loaded and combined from two separate CSV files, we move into the critical stages of data exploration and preprocessing. This process is essential for preparing our dataset for effective model training and analysis.\n\n### Data Visualization\nThe first step in our data exploration is visualization. This helps us understand the distribution of our data and the relationships between different variables.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Visualizing the distributions of acceleration and rotation\nsns.pairplot(data[['AccX', 'AccY', 'AccZ', 'GyroX', 'GyroY', 'GyroZ']], diag_kind='kde')\nplt.show()\n\n# Visualizing the distribution of classification labels\nsns.countplot(x='Class', data=data)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=1417 height=1417}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=602 height=429}\n:::\n:::\n\n\nIn these snippets, we utilize `seaborn` and `matplotlib` for creating insightful visualizations. The `pairplot` function is particularly useful for visualizing pairwise relationships in the dataset and for seeing the distribution of single variables. The `countplot` is used to observe the frequency distribution of the classification labels (SLOW, NORMAL, AGGRESSIVE).\n\n### Data Cleaning\nThe next crucial step is data cleaning, which includes handling missing values and outliers, ensuring the quality of our dataset.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Checking for and handling missing values\nmissing_values = data.isnull().sum()\ndata = data.dropna()\n```\n:::\n\n\nThis code checks for missing values in the dataset and removes any rows containing them. Dropping missing values is one approach, but depending on the context, other strategies like imputation might be more appropriate.\n\n### Feature Engineering\nFeature engineering involves creating new features or modifying existing ones to improve model performance.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Example: Creating a total acceleration feature\ndata['Total_Acceleration'] = (data['AccX']**2 + data['AccY']**2 + data['AccZ']**2)**0.5\n```\n:::\n\n\nHere, we calculate the total acceleration as a new feature. This is done by computing the Euclidean norm of the acceleration components, potentially providing a more comprehensive view of the vehicle's acceleration at any given point.\n\n### Data Normalization or Scaling\nNormalization or scaling is a key step, ensuring that all features contribute equally to the model's performance.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardizing the features\nscaler = StandardScaler()\nfeatures = ['AccX', 'AccY', 'AccZ', 'GyroX', 'GyroY', 'GyroZ', 'Total_Acceleration']\ndata[features] = scaler.fit_transform(data[features])\n```\n:::\n\n\nIn this snippet, `StandardScaler` from Scikit-learn is used to standardize the features, an important step for many machine learning models.\n\n### Splitting the Data into Training and Test Sets\nFinally, we split our dataset into training and testing sets to evaluate the performance of our models.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\n\n# Splitting the dataset\nX = data[features]\ny = data['Class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n:::\n\n\nThis code uses `train_test_split` to divide the dataset, ensuring that we have a separate set for model testing, which is crucial for unbiased evaluation of model performance.\n\nThrough each of these preprocessing steps, from initial visualization to the final splitting of the data, we are laying the groundwork for effective and accurate predictive modeling. This thorough preparation is key to the success of our subsequent analysis.\n\n## Regression Models: An Overview\n\nIn the realm of machine learning, regression models are pivotal tools for understanding and predicting continuous outcomes. In the context of our driving behavior analysis, we focus on three types of regression models: linear regression, polynomial regression, and Support Vector Machines (SVMs). Each of these models offers unique perspectives and strengths in modeling complex relationships in data.\n\n### Linear Regression\nLinear regression is perhaps the most fundamental and widely used form of regression. It assumes a linear relationship between the input variables (features) and a single output variable (target). When there is a single input variable, it is known as simple linear regression, and when multiple input variables are involved, it is termed multiple linear regression.\n\nThe model is expressed in the form of $$ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n + \\epsilon $$ where \\( Y \\) is the dependent variable, \\( X_1, X_2, ..., X_n \\) are the independent variables, \\( \\beta_0 \\) is the intercept, \\( \\beta_1, \\beta_2, ..., \\beta_n \\) are the coefficients, and \\( \\epsilon \\) is the error term.\n\nLinear regression is chosen for its simplicity and interpretability. It is particularly useful for understanding the strength and type of relationship between the dependent and independent variables.\n\n### Polynomial Regression\nWhile linear regression assumes a linear relationship between the dependent and independent variables, polynomial regression extends this idea by allowing for polynomial (non-linear) relationships. This model is expressed as $$ Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + ... + \\beta_n X^n + \\epsilon $$\n\nPolynomial regression is particularly useful in cases where the relationship between variables is not linear but still follows a specific curve. It allows for a better fit to the data compared to linear regression when the data shows non-linear trends.\n\n### Support Vector Machines (SVMs)\nSVMs are typically associated with classification tasks but can be extended to regression, known as Support Vector Regression (SVR). SVR attempts to fit the best line within a threshold value, where the best line is the line that has the maximum number of points.\n\nThe rationale behind choosing SVMs for this analysis is their effectiveness in handling high-dimensional space and their capability to model complex, non-linear relationships. SVMs can be particularly powerful when the data has a clear margin of separation and is not linearly separable.\n\nBy leveraging these different models, we can gain a comprehensive view of driving behavior patterns. Linear regression serves as a starting point for understanding basic relationships, polynomial regression helps us capture non-linear trends, and SVMs provide robustness in modeling complex, high-dimensional patterns. Together, these models offer a rich toolkit for dissecting and predicting driving behaviors.\n\n## Implementing Linear Regression\n\nWith the dataset now appropriately prepared and categorical variables converted into a numerical format, we can proceed with the implementation of linear regression. Linear regression is a straightforward yet powerful tool in predictive modeling, especially useful for understanding the relationships between variables.\n\n### Preparing the Data for Linear Regression\n\nBefore we can apply linear regression, it’s crucial to ensure that our data is in the correct format. This involves encoding the categorical 'Class' labels and ensuring our dataframes are aligned correctly.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Creating an instance of OneHotEncoder\nencoder = OneHotEncoder(sparse=False)\n\n# Reshaping the 'Class' column as it needs to be 2D\nclass_labels = data['Class'].values.reshape(-1, 1)\n\n# Applying OneHotEncoder\nencoded_labels = encoder.fit_transform(class_labels)\n\n# Creating column names for the encoded labels\nencoded_labels_columns = [f'Class_{cat}' for cat in encoder.categories_[0]]\n\n# Converting to DataFrame and resetting index\nencoded_labels_df = pd.DataFrame(encoded_labels, columns=encoded_labels_columns).reset_index(drop=True)\n\n# Dropping the original 'Class' column from data and resetting index\ndata = data.drop('Class', axis=1).reset_index(drop=True)\n\n# Concatenating the original DataFrame with the new one-hot encoded DataFrame\ndata = pd.concat([data, encoded_labels_df], axis=1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n```\n:::\n:::\n\n\n### Implementing Linear Regression\n\nWith our data ready, we can now implement the linear regression model.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Separating features and target variable\nfeatures = data.drop(encoded_labels_columns, axis=1)\ntarget = data[encoded_labels_columns]\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n\n# Creating and training the linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\n# Making predictions on the test set\ny_pred = lin_reg.predict(X_test)\n```\n:::\n\n\nIn this code, we first isolate our features and target variables. We then split the dataset into a training set and a test set, which is essential for evaluating our model's performance. Following this, we create an instance of `LinearRegression`, fit it to our training data, and then make predictions on our test data.\n\n### Evaluating the Model\n\nAfter training our model, it's important to evaluate its performance.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Calculating performance metrics\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)\nprint(\"R-squared:\", r2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 0.21244929071927454\nR-squared: 0.04040366849924115\n```\n:::\n:::\n\n\nThese metrics, Mean Squared Error (MSE) and R-squared, give us an understanding of the model's accuracy and goodness of fit. MSE is the average of the squares of the errors, and R-squared represents the proportion of the variance for the dependent variable that's explained by the independent variables in the model.\n\n### Visualization of the Regression Results\n\nVisualizing the results can provide additional insights into our model's performance.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Plotting actual vs. predicted values\nplt.scatter(y_test, y_pred)\nplt.xlabel('Actual Labels')\nplt.ylabel('Predicted Labels')\nplt.title('Actual vs Predicted Labels')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=589 height=449}\n:::\n:::\n\n\nThis plot helps us visually assess how well our predicted values align with the actual values. In an ideal scenario, we would expect the data points to fall along a diagonal line, indicating perfect predictions.\n\nBy implementing and analyzing the results of linear regression, we gain valuable insights into the patterns within our data. This sets the stage for exploring more complex models, such as polynomial regression or SVMs, which might capture nuances in the data that linear regression cannot.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}