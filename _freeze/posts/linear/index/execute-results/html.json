{
  "hash": "cd063209e379bda89305607724fc6c5f",
  "result": {
    "markdown": "---\ntitle: 3\\. Linear and Nonlinear Regression\nauthor: Joanna Fang\ndate: '2023-11-28'\ncategories:\n  - ml\n  - code\n  - linear regression\n  - nonlinear regression\n  - pollution\nformat:\n  html:\n    code-block-bg: '#FFFFFF'\n    code-block-border-left: '#E83283'\n    toc: true\n    code-tools:\n      source: true\n      toggle: false\n      caption: none\n---\n\n# Predicting Air Pollutant Concentrations Using Linear and Random Forest Regression: A Jupyter Notebook Guide\n\n![](thumbnail.jpg){width=\"20%\" fig-align=\"center\"}\n\n## Introduction\n\nAir quality is a critical environmental factor impacting public health, ecosystem sustainability, and the global climate. Pollutants such as particulate matter (PM2.5 and PM10), sulfur dioxide (SO2), nitrogen dioxide (NO2), carbon monoxide (CO), and ozone (O3) can have severe health impacts, including respiratory and cardiovascular diseases. Understanding and predicting the concentrations of these pollutants is essential for creating effective environmental policies and public health interventions.\n\nIn this blog, we'll delve into two powerful statistical methods used in predicting air pollutant concentrations: linear regression and Random Forest regression.\n\n### Linear Regression\nLinear regression is a fundamental statistical approach used to model the relationship between a dependent variable and one or more independent variables. In the context of air quality, it helps us understand how various environmental factors like temperature, humidity, and wind speed influence pollutant levels. The model assumes a linear relationship between the variables, which can be represented as:\n\n$$\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n + \\epsilon\n$$\n\nHere, \\( Y \\) is the pollutant concentration we want to predict, \\( X_1, X_2, ..., X_n \\) are the environmental factors, \\( \\beta_0, \\beta_1, ..., \\beta_n \\) are the coefficients to be estimated, and \\( \\epsilon \\) is the error term.\n\n### Random Forest Regression\nIn this blog, we'll harness the power of Random Forest—an ensemble learning method ideal for complex datasets with non-linear relationships—to predict air quality. Starting with an exploration of the `air_data_all.csv` dataset, we'll guide you through using Random Forest and other regression techniques in a Jupyter Notebook to analyze environmental conditions and temporal factors. By the end, you'll be well-equipped to apply these methods in Python for comprehensive environmental data analysis.\n\n## Understanding the Dataset\n\nBefore delving into regression models, it's essential to familiarize ourselves with the dataset at hand—`air_data_all.csv`. This dataset contains hourly air quality measurements and meteorological data from Beijing, spanning from March 1st, 2013, to February 28th, 2017. The dataset is sourced from the Beijing Municipal Environmental Monitoring Center and is matched with meteorological data from the China Meteorological Administration. However, it's important to note that missing data points are marked as NA. Link to dataset is https://archive.ics.uci.edu/dataset/501/beijing+multi+site+air+quality+data. \n\n### Dataset Overview\nThe dataset is a valuable resource, encompassing a wide range of environmental conditions and pollutant concentrations. It records temporal information, including the year, month, day, and hour, alongside readings of key air pollutants such as PM2.5, PM10, SO2, NO2, CO, and O3. Additionally, meteorological factors like temperature (TEMP), pressure (PRES), dew point temperature (DEWP), precipitation (RAIN), wind direction (wd), and wind speed (WSPM) are included. This comprehensive data is instrumental for studying air pollution dynamics and its correlation with various environmental and temporal factors.\n\n### Column Descriptions\nEach column in the dataset serves a specific purpose:\n\n1. **Temporal Data (year, month, day, hour)**: These columns provide insights into pollutant variations across different timescales.\n2. **Pollutant Concentrations (PM2.5, PM10, SO2, NO2, CO, O3)**: These are primary pollutants, crucial for urban air quality analysis.\n3. **Meteorological Data (TEMP, PRES, DEWP, RAIN, wd, WSPM)**: Weather conditions significantly impact pollutant dispersion and concentration.\n4. **Station**: This column identifies the monitoring site, facilitating the study of geographical variations in air quality.\n\n## Data Cleaning and Transformation\n\nBefore delving into sophisticated regression models, it's imperative to prepare our dataset, \"air_data_all.csv,\" for analysis. This stage, known as data cleaning and transformation, involves several key steps to ensure the data's integrity and usability.\n\n### Identifying and Handling Missing or Inconsistent Data\nThe initial step in data preprocessing is to identify and address any missing (NaN) or inconsistent data. This is crucial as such data can significantly skew our analysis.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport sys\n!{sys.executable} -m pip install seaborn\n!{sys.executable} -m pip install matplotlib\n!{sys.executable} -m pip install statsmodels\n!{sys.executable} -m pip install scikit-learn\n!{sys.executable} -m pip install pandas\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: seaborn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.13.0)\r\nRequirement already satisfied: matplotlib!=3.6.1,>=3.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (3.8.2)\r\nRequirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (1.26.2)\r\nRequirement already satisfied: pandas>=1.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (2.1.3)\r\nRequirement already satisfied: packaging>=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (23.2)\r\nRequirement already satisfied: importlib-resources>=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (6.1.1)\r\nRequirement already satisfied: fonttools>=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (4.45.1)\r\nRequirement already satisfied: pillow>=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (10.1.0)\r\nRequirement already satisfied: contourpy>=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.2.0)\r\nRequirement already satisfied: python-dateutil>=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\r\nRequirement already satisfied: pyparsing>=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.1.1)\r\nRequirement already satisfied: cycler>=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (0.12.1)\r\nRequirement already satisfied: kiwisolver>=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.4.5)\r\nRequirement already satisfied: zipp>=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.3->seaborn) (3.17.0)\r\nRequirement already satisfied: pytz>=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\r\nRequirement already satisfied: tzdata>=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas>=1.2->seaborn) (2023.3)\r\nRequirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.15.0)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: matplotlib in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (3.8.2)\r\nRequirement already satisfied: cycler>=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\r\nRequirement already satisfied: numpy<2,>=1.21 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.2)\r\nRequirement already satisfied: fonttools>=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.45.1)\r\nRequirement already satisfied: packaging>=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (23.2)\r\nRequirement already satisfied: python-dateutil>=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)\r\nRequirement already satisfied: pillow>=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.1.0)\r\nRequirement already satisfied: importlib-resources>=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.1.1)\r\nRequirement already satisfied: pyparsing>=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.1)\r\nRequirement already satisfied: kiwisolver>=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)\r\nRequirement already satisfied: contourpy>=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.0)\r\nRequirement already satisfied: zipp>=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\r\nRequirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: statsmodels in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.14.0)\r\nRequirement already satisfied: patsy>=0.5.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (0.5.3)\r\nRequirement already satisfied: packaging>=21.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (23.2)\r\nRequirement already satisfied: scipy!=1.9.2,>=1.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (1.11.4)\r\nRequirement already satisfied: pandas>=1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (2.1.3)\r\nRequirement already satisfied: numpy>=1.18 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (1.26.2)\r\nRequirement already satisfied: pytz>=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas>=1.0->statsmodels) (2023.3.post1)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas>=1.0->statsmodels) (2.8.2)\r\nRequirement already satisfied: tzdata>=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas>=1.0->statsmodels) (2023.3)\r\nRequirement already satisfied: six in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from patsy>=0.5.2->statsmodels) (1.15.0)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: scikit-learn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.3.2)\r\nRequirement already satisfied: joblib>=1.1.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.3.2)\r\nRequirement already satisfied: numpy<2.0,>=1.17.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.2)\r\nRequirement already satisfied: scipy>=1.5.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.11.4)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.2.0)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: pandas in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (2.1.3)\r\nRequirement already satisfied: numpy<2,>=1.22.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)\r\nRequirement already satisfied: pytz>=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)\r\nRequirement already satisfied: tzdata>=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\r\nRequirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Load the dataset\nsample_data = pd.read_csv('air_data_all.csv')\n\n# Identifying missing or infinite values\nsample_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# Checking for missing values\nmissing_values = sample_data.isnull().sum()\n```\n:::\n\n\nIn this code block, we first attempt to replace any infinite values with NaNs. Then, we calculate the number of missing values in each column. Depending on the nature and volume of missing data, we can either fill these gaps using statistical methods (like mean, median) or consider removing the rows/columns entirely.\n\n### Normalization or Standardization of Data\nNormalization (rescaling data to a range, like 0–1) and standardization (shifting the distribution to have a mean of zero and a standard deviation of one) are crucial for models sensitive to the scale of data, such as linear regression.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Standardizing the dataset\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(sample_data[['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']])\n\n# Converting scaled data back to a DataFrame for further use\nscaled_df = pd.DataFrame(scaled_data, columns=['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM'])\n```\n:::\n\n\nHere, we use `StandardScaler` from Scikit-learn to standardize the continuous variables such as temperature and pressure. This process aligns the data onto one scale, removing bias due to different units or scales.\n\n### Transforming Categorical Data into a Usable Format\nMany regression models require numerical input, so transforming categorical data into a numerical format is essential.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Creating dummy variables for categorical data\nwd_dummies = pd.get_dummies(sample_data['wd'])\nsample_data = pd.concat([sample_data, wd_dummies], axis=1)\n```\n:::\n\n\nIn the above snippet, we create dummy variables for the `wd` column (wind direction), converting it into a format that can be efficiently processed by regression algorithms.\n\n### Visuals Showing Before and After Data Transformation\nVisualizations are effective for demonstrating the impact of data transformation. For instance, before and after standardization, we can plot histograms of a variable to observe changes in its distribution.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Plotting before and after standardization\nplt.hist(sample_data['TEMP'], bins=30, alpha=0.5, label='Original TEMP')\nplt.hist(scaled_df['TEMP'], bins=30, alpha=0.5, label='Standardized TEMP')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=592 height=411}\n:::\n:::\n\n\nThis histogram allows us to compare the distribution of the temperature data before and after standardization, showcasing the effects of our data transformation steps.\n\nBy completing these data cleaning and transformation processes, we ensure that our dataset is primed for accurate and effective regression analysis, laying a solid foundation for our subsequent modeling steps.\n\n## Correlation Analysis and Multicollinearity Check\n\nAfter preparing our dataset, the next step in our analysis involves understanding the relationships between variables using correlation analysis and checking for multicollinearity. These steps are critical for ensuring the reliability and interpretability of our regression models.\n\n### Correlation Analysis and Its Importance\nCorrelation analysis helps us understand the strength and direction of the relationship between two variables. In regression analysis, it's important to identify how independent variables are related to the dependent variable and to each other.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Removing missing or infinite values from the scaled dataset\nscaled_df.replace([np.inf, -np.inf], np.nan, inplace=True)\nscaled_df.dropna(inplace=True)\n\n# Calculating the correlation matrix for key variables\ncorr_matrix = sample_data[['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']].corr()\n\n# Visualizing the correlation matrix using a heatmap\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title(\"Correlation Matrix of Environmental Factors and Pollutants\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=888 height=801}\n:::\n:::\n\n\nIn this code, we calculate and visualize the correlation matrix of key pollutants and environmental factors. This heatmap provides a clear visual representation of the relationships, where the color intensity and the value in each cell indicate the strength and direction of the correlation.\n\n### Multicollinearity Check and Its Implications\nMulticollinearity occurs when two or more independent variables in a regression model are highly correlated. This can lead to unreliable coefficient estimates, making it difficult to determine the effect of each independent variable.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Preparing data for multicollinearity check\nfeatures = scaled_df[['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']]\n\n# Calculating VIF for each feature\nvif_data = pd.DataFrame()\nvif_data['Feature'] = features.columns\nvif_data['VIF'] = [variance_inflation_factor(features.values, i) for i in range(features.shape[1])]\n\nvif_data\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>VIF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TEMP</td>\n      <td>5.355958</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PRES</td>\n      <td>3.155330</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DEWP</td>\n      <td>4.747345</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>RAIN</td>\n      <td>1.020343</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>WSPM</td>\n      <td>1.486136</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nHere, we calculate the Variance Inflation Factor (VIF) for each feature. A VIF value greater than 5 or 10 indicates high multicollinearity, suggesting that the variable could be linearly predicted from the others with a substantial degree of accuracy.\n\n### Visual Representation of Correlation and Multicollinearity Findings\nVisualizing these statistics can help in better understanding and communicating the findings.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Visualizing VIF values\nplt.bar(vif_data['Feature'], vif_data['VIF'])\nplt.xlabel('Features')\nplt.ylabel('Variance Inflation Factor (VIF)')\nplt.title('Multicollinearity Check - VIF Values')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=576 height=449}\n:::\n:::\n\n\nThis bar chart provides a clear representation of the VIF values for each feature, helping us identify which variables might be contributing to multicollinearity in the model.\n\nBy conducting both correlation analysis and a multicollinearity check, we ensure the integrity and effectiveness of our regression models, setting a strong foundation for accurate and insightful analysis of the factors influencing air quality.\n\n### Feature Selection\nBased on the results of Correlation Analysis and Multicollinearity Check. I decided to predict SO2 with 'TEMP', 'PRES', 'DEWP'. \n\n## Linear Regression Analysis\n\nIn this section, we will apply linear regression analysis to predict the concentration of sulfur dioxide (SO2) based on three key environmental factors: 'TEMP', 'PRES', and 'DEWP'. Linear regression is a fundamental statistical method used to understand the relationship between a dependent variable and one or more independent variables.\n\n### Introduction to Linear Regression and Its Applicability\nLinear regression is a widely used statistical technique for modeling and analyzing the relationship between a scalar response (dependent variable) and one or more explanatory variables (independent variables). The method assumes a linear relationship between the variables. In our context, we will use linear regression to understand how temperature ('TEMP'), pressure ('PRES'), and dew point ('DEWP') affect the concentration of SO2 in the air.\n\n### Step-by-Step Linear Regression Analysis Using Jupyter Notebook\nNow, let's conduct a linear regression analysis using Python in a Jupyter Notebook environment.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Filter out rows where any of the feature columns or 'SO2' is NaN\nfiltered_data = sample_data.dropna(subset=['TEMP', 'PRES', 'DEWP', 'SO2'])\n\n# Standardizing the relevant columns of the filtered data\nscaler = StandardScaler()\nscaled_columns = scaler.fit_transform(filtered_data[['TEMP', 'PRES', 'DEWP']])\n\n# Converting scaled data back to a DataFrame\nscaled_df = pd.DataFrame(scaled_columns, columns=['TEMP', 'PRES', 'DEWP'])\n\n# Defining features (X) and target variable (y)\nX = scaled_df\ny = filtered_data['SO2']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Creating and fitting the linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\nIn this code, we first select our features and target variable, split the data into training and test sets, create a Linear Regression model, and then fit it to our training data.\n\n### Visual Representation of Linear Regression Results and Plotting the Best Fit Line\nVisualizing the model's predictions in comparison with the actual values is crucial for assessing its performance. We'll also plot the best-fit line to better understand the linear relationship.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Predicting SO2 values for the test set\nlr_y_pred = model.predict(X_test)\n\n# Visualizing the actual vs predicted values and the best-fit line\nplt.scatter(y_test, lr_y_pred, alpha=0.6, color='blue')  # Actual vs Predicted scatter plot\nplt.xlabel('Actual SO2')\nplt.ylabel('Predicted SO2')\nplt.title('Actual vs Predicted SO2 Concentrations')\n\n# Plotting the best-fit line\nplt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, lr_y_pred, 1))(np.unique(y_test)), color='red')\n\nplt.show()\n\n# Zoom in \nplt.xlim(0, 200)\nplt.ylim(0, 40)\n\n# Visualizing the actual vs predicted values and the best-fit line\nplt.scatter(y_test, lr_y_pred, alpha=0.6, color='blue')  # Actual vs Predicted scatter plot\nplt.xlabel('Actual SO2')\nplt.ylabel('Predicted SO2')\nplt.title('Actual vs Predicted SO2 Concentrations')\n\n# Plotting the best-fit line\nplt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, lr_y_pred, 1))(np.unique(y_test)), color='red')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=585 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-2.png){width=597 height=449}\n:::\n:::\n\n\nThe scatter plot shows the actual vs. predicted SO2 values, and the red line represents the linear fit, providing a visual indication of how well the model predicts SO2 concentration.\n\n### Evaluating the Performance of the Linear Regression Model\nFinally, we evaluate the performance of our model using common statistical metrics.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Computing performance metrics\nlr_mse = mean_squared_error(y_test, lr_y_pred)\nlr_r2 = r2_score(y_test, lr_y_pred)\n\nprint(f\"Mean Squared Error: {lr_mse}\")\nprint(f\"R² Score: {lr_r2}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 411.5799313674985\nR² Score: 0.10938551133078755\n```\n:::\n:::\n\n\nThe Mean Squared Error (MSE) provides an average of the squares of the errors, essentially quantifying the difference between predicted and actual values. The R² Score measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n\nBy following these steps, we can use linear regression to effectively predict environmental factors' impact on air quality, specifically sulfur dioxide concentrations, and evaluate the accuracy of our predictions.\n\nSure, I'll help you generate text for the \"Random Forest Regression Analysis\" section of your blog. Here's the content for that section:\n\n## Random Forest Regression Analysis\n\n### Introduction to Random Forest Regression\n\nRandom Forest is an ensemble learning method predominantly used for classification and regression tasks. It operates by constructing a multitude of decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Compared to linear regression, Random Forest offers several advantages:\n\n- **Handling Non-linear Data**: It can model complex relationships between features and the target variable, which linear regression may fail to capture.\n- **Reducing Overfitting**: By averaging multiple decision trees, it reduces the risk of overfitting to the training data.\n- **Importance of Features**: Random Forest can provide insights into the relative importance of each feature in prediction.\n\n### Implementing Random Forest Regression\n\nLet's implement Random Forest regression to predict the concentration of sulfur dioxide (SO2) using 'TEMP' (temperature), 'PRES' (pressure), and 'DEWP' (dew point). We have already preprocessed and scaled our dataset. Now, we'll apply Random Forest regression:\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Create a Random Forest model\nrf_model = RandomForestRegressor(random_state=0)\n\n# Fit the model to the training data\nrf_model.fit(X_train, y_train)\n\n# Predicting the SO2 values using the test set\nrf_y_pred = rf_model.predict(X_test)\n```\n:::\n\n\n### Visualization: Feature Importance and Prediction vs Actual\n\n1. **Feature Importance Plot**: This graph illustrates the relative importance of each feature in predicting the SO2 levels.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nfeature_importances = rf_model.feature_importances_\nplt.barh(['TEMP', 'PRES', 'DEWP'], feature_importances)\nplt.xlabel('Feature Importance')\nplt.ylabel('Feature')\nplt.title('Feature Importance in Random Forest Model')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){width=608 height=449}\n:::\n:::\n\n\nThe feature importance plot shows 'TEMP' with the highest score, indicating it has the most significant impact on predicting SO2 levels, followed by 'PRES' and 'DEWP'. This suggests that temperature changes are potentially a more dominant factor in influencing SO2 concentrations in the atmosphere.\n\n2. **Prediction vs Actual Plot**: This plot compares the actual vs. predicted SO2 levels using the Random Forest model.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nplt.scatter(y_test, rf_y_pred)\nplt.xlabel('Actual SO2 Levels')\nplt.ylabel('Predicted SO2 Levels')\nplt.title('Random Forest: Actual vs Predicted SO2 Levels')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-1.png){width=593 height=449}\n:::\n:::\n\n\nThe Prediction vs Actual Plot for the Random Forest model reveals a tighter clustering of data points along the line of perfect prediction compared to the Linear Regression model. This clustering indicates a higher accuracy in predictions made by the Random Forest model.\n\n## Comparative Analysis and Conclusion\n\nWe compare the performance metrics of Random Forest and Linear Regression:\n\n- **Random Forest**\n  - MSE: 204.29218141691157\n  - R²: 0.5579337989410323\n\n- **Linear Regression**\n  - MSE: 411.5799313674985\n  - R²: 0.10938551133078755\n\n#### Interpretation\n\nThe Random Forest model shows a significantly lower Mean Squared Error (MSE) and higher R² value compared to Linear Regression. This indicates that the Random Forest model fits the data better and has a greater predictive accuracy. The reduced MSE suggests that the Random Forest model's predictions are closer to the actual data. The higher R² value indicates that a larger proportion of the variance in the SO2 concentration is being explained by the model.\n\n### Visual Comparison: Prediction vs Actual Plot for Both Models\n\nThis plot will compare the predictions of both models against the actual SO2 levels. Here, 'lr_y_pred' represents the predicted values from the Linear Regression model.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nplt.scatter(y_test, lr_y_pred, label='Linear Regression', alpha=0.5, color='b', marker='o')\nplt.scatter(y_test, rf_y_pred, label='Random Forest', alpha=0.5, color='r', marker='+')\nplt.xlabel('Actual SO2 Levels')\nplt.ylabel('Predicted SO2 Levels')\nplt.title('Comparison of Predictions: Linear Regression vs Random Forest')\nplt.legend()\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-1.png){width=593 height=449}\n:::\n:::\n\n\nThe combined Prediction vs Actual Plot demonstrates a stark contrast between the two models. The Random Forest predictions are more concentrated around the line of perfect fit, while the Linear Regression predictions are more dispersed, indicating more errors in prediction. This visual reaffirms the quantitative metrics, illustrating that Random Forest provides a more accurate model for predicting SO2 levels based on 'TEMP', 'PRES', and 'DEWP'.\n\n### Limitation\n\nAs depicted in the visualizations, there appear to be a few outliers in the graph. Conducting an outlier analysis before proceeding with modeling could potentially enhance the accuracy of our predictions.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}