{
  "hash": "01f260f673fd39045ab96a2dc71e3c65",
  "result": {
    "markdown": "---\ntitle: Anomaly/Outlier Detection\nauthor: Joanna Fang\ndate: '2023-12-06'\ncategories:\n  - ml\n  - code\n  - linear regression\n  - nonlinear regression\n  - pollution\nformat:\n  html:\n    code-block-bg: '#FFFFFF'\n    code-block-border-left: '#E83283'\n    toc: true\n    code-tools:\n      source: true\n      toggle: false\n      caption: none\n---\n\n# Detecting Anomalies in Air Pollution Data: A Data Science Project\n\n![](thumbnail.jpg){width=\"50%\" fig-align=\"center\"}\n\n## Introduction\n\nWelcome to our exploration of \"Detecting Anomalies in Air Pollution Data,\" a vital project in the realm of environmental monitoring. With increasing concerns about air quality and its impact on public health and the environment, identifying irregularities in air pollution data has never been more critical.\n\nThis project leverages a comprehensive dataset from the Beijing Multi-site Air Quality Data, which offers a rich tapestry of air pollutant measurements and meteorological data across various sites in Beijing. The data spans from 2013 to 2017, providing insights into pollutants like PM2.5, PM10, SO2, NO2, and CO, as well as meteorological conditions like temperature, humidity, and wind speed.\n\nOur primary goal is to detect unusual patterns or outliers in air quality data that might signify environmental hazards, technical errors in data collection, or significant meteorological impacts. By accomplishing this, we aim to contribute to more effective environmental monitoring and policy-making.\n\n### Data Exploration and Preprocessing\n\n#### Understanding the Dataset\nThe first step in our data science journey involves getting acquainted with the dataset's structure and characteristics. This involves examining the various columns of the dataset, which include both pollutant levels and meteorological factors.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nsample_data = pd.read_csv('air_data_all.csv')\nsample_data.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>No</th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>hour</th>\n      <th>PM2.5</th>\n      <th>PM10</th>\n      <th>SO2</th>\n      <th>NO2</th>\n      <th>CO</th>\n      <th>O3</th>\n      <th>TEMP</th>\n      <th>PRES</th>\n      <th>DEWP</th>\n      <th>RAIN</th>\n      <th>wd</th>\n      <th>WSPM</th>\n      <th>station</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2013</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>6.0</td>\n      <td>18.0</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>800.0</td>\n      <td>88.0</td>\n      <td>0.1</td>\n      <td>1021.1</td>\n      <td>-18.6</td>\n      <td>0.0</td>\n      <td>NW</td>\n      <td>4.4</td>\n      <td>Gucheng</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2013</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>6.0</td>\n      <td>15.0</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>800.0</td>\n      <td>88.0</td>\n      <td>-0.3</td>\n      <td>1021.5</td>\n      <td>-19.0</td>\n      <td>0.0</td>\n      <td>NW</td>\n      <td>4.0</td>\n      <td>Gucheng</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>2013</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>5.0</td>\n      <td>18.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>700.0</td>\n      <td>52.0</td>\n      <td>-0.7</td>\n      <td>1021.5</td>\n      <td>-19.8</td>\n      <td>0.0</td>\n      <td>WNW</td>\n      <td>4.6</td>\n      <td>Gucheng</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2013</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>6.0</td>\n      <td>20.0</td>\n      <td>6.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1.0</td>\n      <td>1022.7</td>\n      <td>-21.2</td>\n      <td>0.0</td>\n      <td>W</td>\n      <td>2.8</td>\n      <td>Gucheng</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>2013</td>\n      <td>3</td>\n      <td>1</td>\n      <td>4</td>\n      <td>5.0</td>\n      <td>17.0</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>600.0</td>\n      <td>73.0</td>\n      <td>-1.3</td>\n      <td>1023.0</td>\n      <td>-21.4</td>\n      <td>0.0</td>\n      <td>WNW</td>\n      <td>3.6</td>\n      <td>Gucheng</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nBy running this code, we get a glimpse of the first few rows of our dataset, allowing us to understand the types of data we will be working with.\n\n#### Handling Missing Data and Categorical Variables\nDealing with missing data and categorical variables is a crucial part of data preprocessing. To address this, we first identify the missing values and then decide on an appropriate strategy, such as imputation or removal.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nmissing_values = sample_data.isnull().sum()\nmissing_values\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nNo             0\nyear           0\nmonth          0\nday            0\nhour           0\nPM2.5       8739\nPM10        6449\nSO2         9021\nNO2        12116\nCO         20701\nO3         13277\nTEMP         398\nPRES         393\nDEWP         403\nRAIN         390\nwd          1822\nWSPM         318\nstation        0\ndtype: int64\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\n# Remove rows with missing values\nsample_data = sample_data.dropna()\n\n# Identify numerical columns\nnumerical_cols = sample_data.select_dtypes(include=['int64', 'float64']).columns\n\n# Create a pipeline for imputing missing values and scaling\npipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),  # Replace missing values with mean\n    ('scaler', StandardScaler()),                # Scale the data\n])\n\n# Apply the pipeline to the numerical columns\nscaled_data = pipeline.fit_transform(sample_data[numerical_cols])\n\n# Apply PCA\npca = PCA(n_components=0.95)  # Retain 95% of the variance\nprincipal_components = pca.fit_transform(scaled_data)\n\n# Identify non-numeric (categorical) columns\ncategorical_cols = sample_data.select_dtypes(include=['object']).columns\n\n# One-hot encode the categorical data\nencoder = OneHotEncoder(sparse=False)\ncategorical_encoded = encoder.fit_transform(sample_data[categorical_cols])\n\n# Check for 'get_feature_names_out' method for naming columns\nif hasattr(encoder, 'get_feature_names_out'):\n    encoded_columns = pd.DataFrame(categorical_encoded, columns=encoder.get_feature_names_out(categorical_cols))\nelse:\n    # Fallback: manually create feature names\n    encoded_columns = pd.DataFrame(categorical_encoded)\n    encoded_columns.columns = [col + '_' + str(i) for col in categorical_cols for i in range(encoded_columns.shape[1])]\n\n# Concatenate the encoded columns with the original dataset and drop the original categorical columns\nsample_data_encoded = pd.concat([sample_data.drop(categorical_cols, axis=1), encoded_columns], axis=1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n```\n:::\n:::\n\n\nFor categorical variables like wind direction, we use encoding techniques to convert them into numerical form, making them suitable for analysis.\n\n#### Normalization and Standardization\nGiven the varying scales of our numerical features, normalization or standardization becomes necessary. This step ensures that no single feature disproportionately influences the model due to its scale.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(sample_data[['PM2.5', 'PM10', 'TEMP', 'PRES']])\n```\n:::\n\n\n#### Feature Selection and Engineering\nFinally, we perform feature selection and engineering. This process involves choosing the most relevant features and possibly creating new features to improve our model's performance.\n\n1. **Correlation Analysis**: First, we can perform a correlation analysis to understand the relationships between different features. This helps in identifying features that are strongly correlated with each other, from which we can select the most relevant ones.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Now perform the correlation analysis on the numerical data\ncorr = sample_data_encoded.corr()\n\n# Generate a heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, fmt=\".2f\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=879 height=764}\n:::\n:::\n\n\n    This code generates a heatmap of the correlations between different features. High correlation values suggest a strong relationship, which can inform feature selection.\n\n2. **Principal Component Analysis (PCA)**: PCA is a technique used to reduce the dimensionality of the data, enhancing the interpretability while minimizing information loss.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=0.95) # Retain 95% of the variance\nprincipal_components = pca.fit_transform(scaled_data)\n```\n:::\n\n\n    This code applies PCA to the scaled data, reducing the number of features while retaining 95% of the variance in the data.\n\n3. **Feature Engineering**: If applicable, you can create new features that might be more indicative of anomalies. For example, creating a composite air quality index from multiple pollutants.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nsample_data['Air_Quality_Index'] = sample_data['PM2.5'] * 0.4 + sample_data['PM10'] * 0.2 + sample_data['NO2'] * 0.2 + sample_data['SO2'] * 0.1 + sample_data['CO'] * 0.1\n```\n:::\n\n\n    This code creates a new feature, 'Air_Quality_Index', as a weighted sum of various pollutants, hypothesizing that this composite index might be a more effective predictor of anomalies.\n\nThrough these steps, we refine our dataset to include the most relevant features for anomaly detection, enhancing the model's accuracy and efficiency.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}