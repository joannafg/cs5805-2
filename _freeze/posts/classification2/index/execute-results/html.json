{
  "hash": "5e18dadc4b10ab4985f7cd0c6c121d7f",
  "result": {
    "markdown": "---\ntitle: 4. Classification\nauthor: Joanna Fang\ndate: '2023-11-30'\ncategories:\n  - ml\n  - code\n  - classification\n  - driving\n  - kaggle\nformat:\n  html:\n    toc: true\n    code-block-bg: '#FFFFFF'\n    code-block-border-left: '#E83283'\n    code-tools:\n      source: true\n      toggle: false\n      caption: none\n---\n\n# Predictive Analytics for Driving Behavior Classification\n\n![](thumbnail.jpg){width=\"50%\" fig-align=\"center\"}\n\n## Project Introduction\n\nWelcome to our exploration into the world of machine learning and its application in predicting driving behaviors. In this project, we dive into the realm of vehicular safety, aiming to leverage sensor data to classify driving patterns into three categories: SLOW, NORMAL, and AGGRESSIVE. This endeavor is not just a technical challenge but a crucial step towards enhancing road safety and reducing traffic accidents.\n\n### Objective\nThe primary goal of this project is to accurately predict driving behaviors using data from commonly available sensors in smartphones. By analyzing accelerometer and gyroscope data, we aim to classify driving styles into SLOW, NORMAL, or AGGRESSIVE, contributing significantly to the prevention of road mishaps.\n\n### Importance and Applications\nThe importance of this project is underscored by the alarming statistics from the AAA Foundation for Traffic Safety, highlighting that over half of fatal crashes involve aggressive driving actions. Through this project, we offer a scalable and readily deployable solution to monitor and predict dangerous driving behaviors, potentially saving lives and making our roads safer. Applications of this model extend to insurance companies for risk assessment, ride-sharing services for driver monitoring, and personal safety apps to alert drivers of their driving patterns.\n\n## Data Collection and Description\n\n### Source of the Data\nThe dataset for this project is derived from a real-world scenario, specifically designed to capture driving behaviors. Utilizing a data collector application on Android devices, Ion  Cojocaru and 2 Collaborators from Kaggle.com have gathered sensor readings directly relevant to driving dynamics.\n\n### Dataset Description\nThe dataset is a rich collection of sensor data recorded from a Samsung Galaxy S21, chosen for its advanced sensor capabilities. Here's a breakdown of the dataset features:\n\n#### Acceleration Data\n- Axes: X, Y, Z\n- Unit: Meters per second squared (m/s²)\n- Note: Gravitational acceleration has been filtered out to focus on the acceleration caused by driving actions.\n\n#### Rotation Data\n- Axes: X, Y, Z\n- Unit: Degrees per second (°/s)\n- Purpose: Captures the angular changes during driving, indicative of turns and maneuvers.\n\n#### Classification Label\n- Categories: SLOW, NORMAL, AGGRESSIVE\n- Basis: The driving behavior classification based on the sensor data patterns.\n\n#### Additional Information\n- Sampling Rate: 2 samples per second, ensuring a fine-grained capture of driving dynamics.\n- Timestamp: Included for each sample, allowing for temporal analysis of driving patterns.\n\nIn the following sections, we will delve into the preprocessing, exploratory analysis, and modeling of this dataset to build a robust classifier for driving behaviors. \n\n## Data Preprocessing\n\nIn the realm of machine learning, data preprocessing is a critical step in preparing raw data for modeling. Our datasets, `motion_data_1.csv` (test data) and `motion_data_2.csv` (train data), contain acceleration and rotation measurements alongside driving behavior classifications. Let's walk through the preprocessing steps:\n\n### Handling Missing Values\n\nFirst, we'll check for missing values in both datasets. Missing data can significantly impact the performance of a machine learning model. If missing values are found, strategies such as imputation or removal of the affected rows can be considered.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Install and import all libraries\nimport sys\n!{sys.executable} -m pip install seaborn\n!{sys.executable} -m pip install matplotlib\n!{sys.executable} -m pip install scikit-learn\n!{sys.executable} -m pip install pandas\n!{sys.executable} -m pip install numpy\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: seaborn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.13.0)\r\nRequirement already satisfied: pandas>=1.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (2.1.3)\r\nRequirement already satisfied: matplotlib!=3.6.1,>=3.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (3.8.2)\r\nRequirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (1.26.2)\r\nRequirement already satisfied: python-dateutil>=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\r\nRequirement already satisfied: contourpy>=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.2.0)\r\nRequirement already satisfied: packaging>=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (23.2)\r\nRequirement already satisfied: pyparsing>=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.1.1)\r\nRequirement already satisfied: importlib-resources>=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (6.1.1)\r\nRequirement already satisfied: cycler>=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (0.12.1)\r\nRequirement already satisfied: kiwisolver>=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.4.5)\r\nRequirement already satisfied: fonttools>=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (4.45.1)\r\nRequirement already satisfied: pillow>=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (10.1.0)\r\nRequirement already satisfied: zipp>=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.3->seaborn) (3.17.0)\r\nRequirement already satisfied: pytz>=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\r\nRequirement already satisfied: tzdata>=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas>=1.2->seaborn) (2023.3)\r\nRequirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.15.0)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: matplotlib in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (3.8.2)\r\nRequirement already satisfied: kiwisolver>=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)\r\nRequirement already satisfied: importlib-resources>=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.1.1)\r\nRequirement already satisfied: python-dateutil>=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)\r\nRequirement already satisfied: pillow>=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.1.0)\r\nRequirement already satisfied: numpy<2,>=1.21 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.2)\r\nRequirement already satisfied: cycler>=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\r\nRequirement already satisfied: packaging>=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (23.2)\r\nRequirement already satisfied: fonttools>=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.45.1)\r\nRequirement already satisfied: pyparsing>=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.1)\r\nRequirement already satisfied: contourpy>=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.0)\r\nRequirement already satisfied: zipp>=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\r\nRequirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: scikit-learn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.3.2)\r\nRequirement already satisfied: joblib>=1.1.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.3.2)\r\nRequirement already satisfied: scipy>=1.5.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.11.4)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.2.0)\r\nRequirement already satisfied: numpy<2.0,>=1.17.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.2)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: pandas in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (2.1.3)\r\nRequirement already satisfied: tzdata>=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)\r\nRequirement already satisfied: numpy<2,>=1.22.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\r\nRequirement already satisfied: pytz>=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)\r\nRequirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: numpy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.26.2)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Load the datasets\ntest_data = pd.read_csv('motion_data_1.csv')\ntrain_data = pd.read_csv('motion_data_2.csv')\n\n# Checking for missing values in both datasets\nmissing_values_test = test_data.isnull().sum()\nmissing_values_train = train_data.isnull().sum()\n```\n:::\n\n\n### Normalizing or Scaling the Data\n\nNormalization or scaling is crucial when dealing with sensor data. It ensures that each feature contributes proportionately to the final prediction. Given the different scales of acceleration (in m/s²) and rotation (in °/s), applying a scaling method like Min-Max scaling or Standardization is important.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Standardizing the data\nscaler = StandardScaler()\ntrain_data_scaled = scaler.fit_transform(train_data.iloc[:, :-2]) # Excluding 'Class' and 'Timestamp' columns\ntest_data_scaled = scaler.transform(test_data.iloc[:, :-2])\n```\n:::\n\n\n### Feature Engineering \n\nIn our case, we attempted to engineer a new deriving features: the total magnitude of acceleration.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Adding a feature: Total magnitude of acceleration\ntrain_data['TotalAcc'] = np.sqrt(train_data['AccX']**2 + train_data['AccY']**2 + train_data['AccZ']**2)\ntest_data['TotalAcc'] = np.sqrt(test_data['AccX']**2 + test_data['AccY']**2 + test_data['AccZ']**2)\n```\n:::\n\n\n## Exploratory Data Analysis (EDA)\n\n### Statistical Summary of the Dataset\n\nUnderstanding the basic statistics of the dataset is essential. This includes measures like mean, median, standard deviation, etc., providing insights into the data distribution.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Descriptive statistics of the training data\ntrain_data_description = train_data.describe()\n```\n:::\n\n\n### Visualization of Data Distribution\n\n#### Histograms or Box Plots for Acceleration and Rotation Data\n\nHistograms and box plots are effective for visualizing the distribution of sensor data and identifying outliers or skewness in the data.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Setting up the figure for multiple subplots\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 15))\nfig.suptitle('Histograms of Acceleration and Rotation Data', fontsize=16)\n\n# Plotting histograms for each sensor data column\nsns.histplot(train_data['AccX'], kde=True, ax=axes[0, 0], color='skyblue')\naxes[0, 0].set_title('Acceleration in X-axis (AccX)')\n\nsns.histplot(train_data['AccY'], kde=True, ax=axes[0, 1], color='olive')\naxes[0, 1].set_title('Acceleration in Y-axis (AccY)')\n\nsns.histplot(train_data['AccZ'], kde=True, ax=axes[1, 0], color='gold')\naxes[1, 0].set_title('Acceleration in Z-axis (AccZ)')\n\nsns.histplot(train_data['GyroX'], kde=True, ax=axes[1, 1], color='teal')\naxes[1, 1].set_title('Rotation in X-axis (GyroX)')\n\nsns.histplot(train_data['GyroY'], kde=True, ax=axes[2, 0], color='salmon')\naxes[2, 0].set_title('Rotation in Y-axis (GyroY)')\n\nsns.histplot(train_data['GyroZ'], kde=True, ax=axes[2, 1], color='violet')\naxes[2, 1].set_title('Rotation in Z-axis (GyroZ)')\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=1430 height=1373}\n:::\n:::\n\n\n1. **Acceleration (AccX, AccY, AccZ)**: The distributions for acceleration on all three axes appear to be roughly bell-shaped, indicating that most of the readings are clustered around the mean, with fewer readings at the extreme ends. This suggests normal driving conditions with occasional variances that could indicate moments of acceleration or deceleration.\n\n2. **Rotation (GyroX, GyroY, GyroZ)**: The rotation data histograms show a similar bell-shaped distribution, especially for the X and Y axes, indicating consistent turning behavior with some outliers potentially representing more aggressive turns or corrections. The GyroZ histogram is notably narrower, which might suggest that rotation around the Z-axis (often corresponding to yaw movements) is less variable during normal driving conditions.\n\n#### Correlation Heatmaps\n\nA correlation heatmap helps in understanding the relationships between different sensor readings. It's crucial for identifying features that are highly correlated and might need to be addressed during feature selection.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# For the correlation heatmap, we exclude non-numeric columns (Class and Timestamp)\ntrain_data_numeric = train_data.select_dtypes(include=['float64', 'int64'])\n\n# Generating the correlation heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(train_data_numeric.corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap of Numeric Features')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=799 height=505}\n:::\n:::\n\n\nIn this heatmap:\n\n- Values close to 1 or -1 indicate a strong positive or negative correlation, respectively.\n- Values close to 0 suggest no linear correlation between the variables.\n\nIt appears that:\n\n- There are no exceptionally strong correlations between the different axes of acceleration and rotation, which is desirable in a dataset used for behavior prediction as it indicates that the sensor readings provide unique information.\n- The strongest negative correlation is observed between GyroZ and AccX, which might suggest that certain types of aggressive driving behaviors cause inverse changes in these two measurements.\n\nThe absence of very high correlations means that there may not be redundant features in the dataset, which is good for a machine learning model that relies on diverse data points to make predictions. However, the subtle correlations that do exist can still provide valuable insights when developing features and training models.\n\nIn the next sections, we will delve into model selection, training, and evaluation, using the insights and data preparations we've just discussed. Stay tuned!\n\n## Data Preparation for Modeling\n\nPreparing the data for modeling is a crucial step in the machine learning pipeline. It ensures that the data fed into the model is clean, representative, and well-formatted.\n\n### Splitting Data into Training and Testing Sets\n\nWe have two datasets: `train_data` and `test_data`, pre-split for our convenience. Typically, we would use a function like `train_test_split` from `scikit-learn` to divide our dataset into a training set and a test set, ensuring that both sets are representative of the overall distribution. However, in this scenario, that step is already accounted for.\n\n## Model Selection\n\nSelecting the right model is about finding the balance between prediction accuracy, computational efficiency, and the ease of interpretation.\n\n### Overview of Potential Machine Learning Models for Classification\n\nFor our classification task, we have several models at our disposal:\n\n- **Decision Tree**: A good baseline that is easy to interpret.\n- **Random Forest**: An ensemble method that can improve on the performance of a single decision tree.\n- **Support Vector Machine (SVM)**: Effective in high-dimensional spaces.\n- **Neural Networks**: Potentially high performance but may require more data and compute resources.\n\n### Criteria for Model Selection\n\nWhen selecting the model, we consider several criteria:\n\n- **Accuracy**: How often the model makes the correct prediction.\n- **Speed**: How quickly the model can be trained and used for prediction.\n- **Interpretability**: The ease with which we can understand the model's predictions.\n\n## Model Training\n\nTraining our models is like teaching them to understand patterns within the data that distinguish between SLOW, NORMAL, and AGGRESSIVE driving behaviors. We will employ four different machine learning models to find the best predictor.\n\n### Training Different Models\n\nWe start by training different types of models. Each has its own strengths and might capture different aspects of the driving behavior.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Initializing models with default parameters\ndecision_tree = DecisionTreeClassifier()\nrandom_forest = RandomForestClassifier()\nsvm = SVC()\nneural_network = MLPClassifier()\n\ndecision_tree.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\nrandom_forest.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\nsvm.fit(train_data.drop(['Class'], axis=1), train_data['Class']) \nneural_network.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier()</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\nThe `fit` method allows the models to learn from the training data. It's during this process that they identify which features are most important for predicting driving behavior.\n\n### Hyperparameter Tuning\n\nTo optimize our models, we tweak their settings, known as hyperparameters. This process is akin to fine-tuning an instrument to ensure it plays the perfect note.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Reduce the number of iterations and cross-validation folds\nn_iter_search = 5\ncv_folds = 3\n\n# Simplify the models - example for Random Forest\nrf_param_grid = {\n    'n_estimators': [50, 100],  # reduced number of trees\n    'max_depth': [10, None],  # fewer options\n    'min_samples_split': [2, 5]\n}\n\n# Initialize the RandomizedSearchCV object for Random Forest with fewer iterations and folds\nrandom_search_rf = RandomizedSearchCV(\n    RandomForestClassifier(), \n    param_distributions=rf_param_grid, \n    n_iter=n_iter_search, \n    cv=cv_folds, \n    n_jobs=-1, \n    random_state=42\n)\nrandom_search_rf.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\nbest_params_rf = random_search_rf.best_params_\n\n# RandomizedSearchCV for Decision Tree\ndt_param_grid = {\n    'max_depth': [10, 20, 30],\n    'min_samples_leaf': [1, 2, 4]\n}\nrandom_search_dt = RandomizedSearchCV(\n    DecisionTreeClassifier(), \n    param_distributions=dt_param_grid, \n    n_iter=n_iter_search, \n    cv=cv_folds, \n    random_state=42\n)\nrandom_search_dt.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\nbest_params_dt = random_search_dt.best_params_\n\n# RandomizedSearchCV for SVM\n# Using an extremely small subset of the data\nsubset_size = 50  # Very small subset for extremely quick execution\ntrain_subset = train_data.sample(n=subset_size, random_state=42)\n# Simplified RandomizedSearchCV for SVM\nrandom_search_svm = RandomizedSearchCV(\n    SVC(), \n    param_distributions={'C': [1], 'kernel': ['linear']},  # Minimal hyperparameter space\n    n_iter=1,  # Only one iteration\n    cv=2,  # Reduced to 2-fold cross-validation\n    n_jobs=1,  # Limit the number of jobs to 1 for a quick run\n    random_state=42,\n    verbose=0  # No verbosity\n)\nrandom_search_svm.fit(train_subset.drop(['Class'], axis=1), train_subset['Class'])\nbest_params_svm = random_search_svm.best_params_\n\n# RandomizedSearchCV for Neural Network\n# Using a very small subset of the data\nsubset_size = 50  # Extremely small subset for quick execution\ntrain_subset = train_data.sample(n=subset_size, random_state=42)\n\n# Simplified parameter grid for Neural Network\nnn_param_grid = {\n    'hidden_layer_sizes': [(50,)],  # Smaller network\n    'activation': ['relu'],  # One activation function\n    'solver': ['adam'],  # One solver\n    'max_iter': [100]  # Fewer iterations\n}\n\n# RandomizedSearchCV for Neural Network with simplified settings\nrandom_search_nn = RandomizedSearchCV(\n    MLPClassifier(), \n    nn_param_grid, \n    n_iter=2,  # Fewer iterations\n    cv=2,  # Reduced cross-validation\n    n_jobs=1,  # Using single job for faster execution in limited environments\n    random_state=42,\n    verbose=0\n)\nrandom_search_nn.fit(train_subset.drop(['Class'], axis=1), train_subset['Class'])\nbest_params_nn = random_search_nn.best_params_\n\n# Print the best parameters for all models\nprint(\"Best parameters for Random Forest:\", best_params_rf)\nprint(\"Best parameters for Decision Tree:\", best_params_dt)\nprint(\"Best parameters for SVM:\", best_params_svm)\nprint(\"Best parameters for Neural Network:\", best_params_nn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest parameters for Random Forest: {'n_estimators': 100, 'min_samples_split': 5, 'max_depth': None}\nBest parameters for Decision Tree: {'min_samples_leaf': 2, 'max_depth': 30}\nBest parameters for SVM: {'kernel': 'linear', 'C': 1}\nBest parameters for Neural Network: {'solver': 'adam', 'max_iter': 100, 'hidden_layer_sizes': (50,), 'activation': 'relu'}\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 1 is smaller than n_iter=2. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n  warnings.warn(\n```\n:::\n:::\n\n\nUsing `GridSearchCV`, we systematically work through multiple combinations of parameter tunes, cross-validating. The summary of the best parameters for various machine learning models from a tuning process are as follows: For the Random Forest, the optimal settings are 50 trees (`n_estimators`), a minimum of 2 samples required to split a node (`min_samples_split`), and a maximum tree depth of 10 (`max_depth`). The Decision Tree's best parameters include a maximum depth of 30 (`max_depth`) and a minimum of 2 samples required at a leaf node (`min_samples_leaf`). For the SVM, a linear kernel with a penalty parameter `C` of 1 yields the best results. Lastly, the optimal configuration for the Neural Network involves the 'adam' solver, a maximum of 100 iterations (`max_iter`), 50 neurons in the hidden layer (`hidden_layer_sizes`), and 'relu' activation function. These parameters are likely the result of a grid search optimization process aiming to enhance model performance. \n\n## Model Evaluation\n\nAfter training our models and tuning their hyperparameters, the next critical step is model evaluation. This step helps us understand how well our models perform and guides us in selecting the best one for our application.\n\n### Evaluating Model Performance\n\nWe'll evaluate each model using several key metrics: accuracy, precision, recall, and the F1-score. These metrics give us a well-rounded view of our models' performance.\n\n- **Accuracy**: The proportion of true results (both true positives and true negatives) among the total number of cases examined.\n- **Precision**: The ratio of correctly predicted positive observations to the total predicted positives.\n- **Recall (Sensitivity)**: The ratio of correctly predicted positive observations to all observations in the actual class.\n- **F1-Score**: The weighted average of Precision and Recall, best if there's an uneven class distribution.\n\nLet's calculate and print these metrics for each model:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Models we've trained\nmodels = {\n    \"Decision Tree\": decision_tree,\n    \"Random Forest\": random_forest,\n    \"SVM\": svm,\n    \"Neural Network\": neural_network\n}\n\n# Evaluate each model\nfor name, model in models.items():\n    predictions = model.predict(test_data.drop(['Class'], axis=1))\n    accuracy = accuracy_score(test_data['Class'], predictions)\n    precision = precision_score(test_data['Class'], predictions, average='weighted')\n    recall = recall_score(test_data['Class'], predictions, average='weighted')\n    f1 = f1_score(test_data['Class'], predictions, average='weighted')\n\n    print(f\"{name}:\")\n    print(f\"  Accuracy: {accuracy:.4f}\")\n    print(f\"  Precision: {precision:.4f}\")\n    print(f\"  Recall: {recall:.4f}\")\n    print(f\"  F1-Score: {f1:.4f}\\n\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nDecision Tree:\n  Accuracy: 0.3233\n  Precision: 0.1045\n  Recall: 0.3233\n  F1-Score: 0.1580\n\nRandom Forest:\n  Accuracy: 0.3233\n  Precision: 0.1045\n  Recall: 0.3233\n  F1-Score: 0.1580\n\nSVM:\n  Accuracy: 0.4128\n  Precision: 0.1704\n  Recall: 0.4128\n  F1-Score: 0.2412\n\nNeural Network:\n  Accuracy: 0.3233\n  Precision: 0.1045\n  Recall: 0.3233\n  F1-Score: 0.1580\n\n```\n:::\n:::\n\n\n### Confusion Matrix for Each Model\n\nThe confusion matrix is a useful tool for understanding the performance of a classification model. It shows the actual vs. predicted classifications.\n\nLet's plot the confusion matrix for each model and discuss what the scores mean:\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nfor name, model in models.items():\n    predictions = model.predict(test_data.drop(['Class'], axis=1))\n    conf_mat = confusion_matrix(test_data['Class'], predictions)\n\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(conf_mat, annot=True, fmt='g')\n    plt.title(f'Confusion Matrix for {name}')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=631 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-2.png){width=631 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-3.png){width=631 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-4.png){width=631 height=523}\n:::\n:::\n\n\n- **True Positives (TP)**: Correctly predicted positive observations.\n- **True Negatives (TN)**: Correctly predicted negative observations.\n- **False Positives (FP)**: Incorrectly predicted positive observations (Type I error).\n- **False Negatives (FN)**: Incorrectly predicted negative observations (Type II error).\n\n(A model with high TP and TN values and low FP and FN values is generally considered good. However, the importance of each type of error can vary based on the application. For instance, in medical diagnosis, reducing FN (missed diagnoses) may be more critical than reducing FP.)\n\n## Model Interpretation\n\nInterpreting machine learning models involves understanding the decisions made by the model and assessing their importance in the context of the task. For our driving behavior classification project, the model interpretation will shed light on how well the models are distinguishing between SLOW, NORMAL, and AGGRESSIVE driving behaviors.\n\n### Interpretation of Model Results\n\nThe confusion matrices provide a visual and quantitative way to measure the performance of the models. Ideally, a well-performing model would have high numbers along the diagonal, indicating correct predictions, and low numbers off the diagonal.\n\nIn all four confusion matrices, we observe that the models predict only one class, which indicates a severe imbalance in their learning, potentially caused by class imbalance or other data issues. They fail to predict any instances of the 'SLOW' and 'AGGRESSIVE' classes, classifying everything as 'NORMAL'. This is also reflected in the low precision and F1-scores for all models, as these metrics account for both false positives and false negatives.\n\n### Feature Importance Analysis\n\nUnderstanding which features are most important for making predictions can provide insights into the dataset and the model's decision-making process. For tree-based models, we can directly retrieve feature importance. For other models, such as SVM and Neural Networks, the interpretation can be more complex and may require additional techniques like permutation importance.\n\nFor Decision Trees and Random Forests, feature importance is calculated based on how well they split the data and reduce impurity. Here's how you might compute and plot the feature importances for the Random Forest model:\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nimportances = random_forest.feature_importances_\nindices = np.argsort(importances)[::-1]\nfeatures = train_data.drop(['Class'], axis=1).columns\n\n# Plot the feature importances of the forest\nplt.figure(figsize=(12, 6))\nplt.title(\"Feature importances\")\nplt.bar(range(train_data.shape[1] - 1), importances[indices], align=\"center\")\nplt.xticks(range(train_data.shape[1] - 1), features[indices], rotation=90)\nplt.xlim([-1, train_data.shape[1] - 1])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){width=943 height=566}\n:::\n:::\n\n\nFor the SVM and Neural Network models, feature importance is not as straightforward because these models do not inherently provide a method for evaluating the importance of features. However, for the SVM model with a linear kernel, we can look at the weights assigned to each feature to determine their importance. For neural networks, especially deep ones, feature importance is a more complex area of research and often involves using additional tools or methods.\n\n#### Results\n\nThe models' inability to correctly classify all three driving behaviors indicates that further investigation into the data distribution, preprocessing, and model training is necessary. This could include resampling techniques to address class imbalance, feature engineering to better capture the characteristics of aggressive driving, or exploring more complex models and tuning strategies. It's also important to consider the real-world applicability of these models — in a safety-critical domain such as driving behavior classification, we must strive for high precision and recall to ensure all aggressive behaviors are accurately detected.\n\n## Conclusion\n\nIn summarizing our findings, the machine learning models developed to classify driving behavior as SLOW, NORMAL, or AGGRESSIVE revealed critical insights. The evaluation metrics and confusion matrices indicated that our models predominantly predicted the NORMAL class while failing to recognize SLOW and AGGRESSIVE behaviors. The accuracy across the models ranged from approximately 26% to 41%, with the SVM model achieving the highest accuracy. However, the precision and recall scores were low for all models, indicating a significant area for improvement.\n\n### Limitations\n\nThe limitations of our current approach are evident:\n\n- **Class Imbalance**: The models' tendency to predict mostly the NORMAL class suggests a possible class imbalance.\n- **Model Overfitting**: The poor generalization to other classes may also indicate overfitting to the majority class.\n- **Feature Representation**: The current features may not sufficiently represent the characteristics unique to each class.\n\n### Potential Improvements\n\nSeveral improvements can be proposed:\n\n- **Data Resampling**: Implementing techniques like SMOTE or random oversampling to balance the class distribution could be beneficial.\n- **Feature Engineering**: Developing more sophisticated features or utilizing feature selection methods to capture more nuanced patterns of aggressive driving.\n- **Model Complexity**: Exploring more complex models or deep learning approaches that might capture the intricacies in the data better.\n- **Extended Hyperparameter Tuning**: Conducting a more thorough hyperparameter optimization, given more computational time and resources.\n\nIn conclusion, while the current models have provided a foundation, further work is needed to develop a robust classifier that can accurately identify various driving behaviors, which is crucial for enhancing road safety and reducing accidents.\n\n## Reference \n- https://builtin.com/data-science/supervised-machine-learning-classification\n- https://www.analyticsvidhya.com/blog/2021/09/a-complete-guide-to-understand-classification-in-machine-learning/\n- OpenAI. (2023). ChatGPT [Large language model]. https://chat.openai.com\n- https://www.kaggle.com/datasets/outofskills/driving-behavior\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}