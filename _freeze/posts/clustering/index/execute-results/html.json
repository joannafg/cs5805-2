{
  "hash": "53f612acb020c1a32d15f893e2b05628",
  "result": {
    "markdown": "---\ntitle: Clustering\nauthor: Joanna Fang\ndate: '2023-11-29'\ncategories:\n  - ml\n  - code\n  - clustering\n  - driving\n  - kaggle\nformat:\n  html:\n    toc: true\n    code-block-bg: '#FFFFFF'\n    code-block-border-left: '#E83283'\n    code-tools:\n      source: true\n      toggle: false\n      caption: none\n---\n\n# Clustering: Optimizing Driver Behavior Analysis with Machine Learning Clustering \n\n![](thumbnail.jpg){width=\"50%\" fig-align=\"center\"}\n\n## Introduction\n\nThe analysis of driving behaviors, a task of both complexity and nuance, is increasingly significant in the realms of traffic safety and autonomous vehicle development. Machine learning, particularly clustering algorithms, stands at the forefront of this analysis, offering the ability to dissect and understand the myriad patterns hidden within driving data. Clustering, a form of unsupervised learning, excels in finding structure within unlabeled data, grouping similar data points together based on shared characteristics.\n\nIn this blog, we dive into a practical application of clustering using a dataset from Kaggle titled 'Driving Behavior'. This dataset is intriguing because it comes with labeled data, categorizing driving behaviors into three types: SLOW, NORMAL, and AGGRESSIVE. While clustering typically operates on unlabeled data, we use this labeled dataset to our advantage. It will allow us to compare the clusters formed by our machine learning algorithm against the pre-labeled categories, providing an insightful backdrop to understand and validate our clustering results.\n\nOur aim is to illustrate the entire process of applying clustering to analyze driving behavior. We start with a predefined notion of having three clusters based on the labels present in the dataset. However, as we delve deeper into the analysis, we realize the necessity to step back and question our initial assumptions. We employ the Elbow Method, supplemented by the `kneed` Python package, to determine the optimal number of clusters in a more data-driven manner. This journey from assumption to optimization is not just about applying a machine learning technique but also about understanding the nuances and intricacies that come with real-world data analysis.\n\nJoin us as we unravel the complexities of driving behaviors through clustering, starting with predefined labels and moving towards an optimized, data-driven approach.\n\n## Setting the Stage with Data and Tools\n\nIn the world of machine learning, the choice of dataset is as crucial as the algorithm itself. For our endeavor into clustering driving behaviors, we've chosen a dataset from Kaggle named 'Driving Behavior'. This dataset is particularly interesting for a few reasons. Firstly, it's labeled with specific driving behaviors, which, while not necessary for clustering, provides us with a unique opportunity to validate and understand our clustering results in a more tangible way.\n\n### The Kaggle 'Driving Behavior' Dataset\n\nThis dataset offers a detailed glimpse into various aspects of driving, captured through different features:\n\n- **Acceleration (AccX, AccY, AccZ)**: These features measure the vehicle's acceleration in meters per second squared ($$m/s^2$$) along the X, Y, and Z axes. Acceleration data is crucial for understanding sudden movements and changes in driving speed.\n- **Rotation (GyroX, GyroY, GyroZ)**: Here, we have the vehicle's angular velocity around the X, Y, and Z axes, measured in degrees per second ($$Â°/s$$). This data helps in identifying steering patterns and gauging the stability of the vehicle during various maneuvers.\n- **Classification Labels (SLOW, NORMAL, AGGRESSIVE)**: Each data point is tagged with one of these labels. It's important to note that in clustering, such labels are not typically required. However, in our case, these labels will serve as a benchmark, enabling us to compare our unsupervised learning results with pre-defined categories.\n\nThis dataset not only provides a foundation for applying clustering techniques but also allows us to explore how well unsupervised learning can mirror human-labeled classifications.\n\n### Tools and Libraries\n\nOur analysis will be powered by several key tools and libraries, each bringing its own strengths to the table:\n\n- **Python**: Our language of choice, renowned for its ease of use and strong community support, especially in data science.\n- **Scikit-learn**: A powerful Python library for machine learning. We will use its clustering algorithms, specifically KMeans, to group our data.\n- **Matplotlib**: This library will help us in visualizing our data and the results of our clustering, making the insights more accessible.\n- **kneed**: An invaluable tool when it comes to determining the optimal number of clusters. It programmatically identifies the 'elbow point' in our dataset, a crucial step in ensuring our clustering approach is as effective as possible.\n\nIn the next sections, we'll dive into the process of applying these tools to our dataset, starting with our initial approach and gradually moving towards a more refined, data-driven methodology.\n\n## Initial Clustering Approach\n\nEmbarking on our journey to unravel the driving patterns hidden in the dataset, we initially gravitated towards a straightforward approach: clustering the data into three groups. This decision was influenced by the dataset's labels - SLOW, NORMAL, and AGGRESSIVE. It seemed logical to align our clusters with these pre-defined categories, under the assumption that they would naturally encapsulate the essence of the driving behaviors.\n\n### Why Start with Three Clusters?\n\nThe rationale was straightforward: the dataset labels suggested three distinct types of driving behaviors. Clustering aims to group similar data points, and with these labels as a guide, it seemed reasonable to start our analysis by segmenting the data into three clusters, hoping they would align with the labeled behaviors.\n\n### Applying KMeans Clustering\n\nThe KMeans algorithm was chosen for its simplicity and effectiveness in many clustering scenarios. Here's a breakdown of how we applied it:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport sys\n!{sys.executable} -m pip install numpy\n!{sys.executable} -m pip install matplotlib\n!{sys.executable} -m pip install scipy\n!{sys.executable} -m pip install scikit-learn\n!{sys.executable} -m pip install pandas\n!{sys.executable} -m pip install kneed\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\n# Load the datasets\ndata_1 = pd.read_csv('motion_data_1.csv')\ndata_2 = pd.read_csv('motion_data_2.csv')\n\n# Combine datasets\ndata = pd.concat([data_1, data_2])\n\n# Data preprocessing\nscaler = StandardScaler()\nfeatures = data[['AccX', 'AccY', 'AccZ', 'GyroX', 'GyroY', 'GyroZ']]\nscaled_features = scaler.fit_transform(features)\n\n# Applying KMeans with 3 clusters\nkmeans = KMeans(n_clusters=3, random_state=42)\nclusters = kmeans.fit_predict(scaled_features)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: numpy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.26.2)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: matplotlib in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (3.8.2)\r\nRequirement already satisfied: kiwisolver>=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)\r\nRequirement already satisfied: pyparsing>=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.1)\r\nRequirement already satisfied: importlib-resources>=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.1.1)\r\nRequirement already satisfied: python-dateutil>=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)\r\nRequirement already satisfied: cycler>=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\r\nRequirement already satisfied: contourpy>=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.0)\r\nRequirement already satisfied: packaging>=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (23.2)\r\nRequirement already satisfied: pillow>=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.1.0)\r\nRequirement already satisfied: fonttools>=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.45.1)\r\nRequirement already satisfied: numpy<2,>=1.21 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.2)\r\nRequirement already satisfied: zipp>=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\r\nRequirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: scipy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.11.4)\r\nRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scipy) (1.26.2)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: scikit-learn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.3.2)\r\nRequirement already satisfied: joblib>=1.1.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.3.2)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.2.0)\r\nRequirement already satisfied: scipy>=1.5.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.11.4)\r\nRequirement already satisfied: numpy<2.0,>=1.17.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.2)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: pandas in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (2.1.3)\r\nRequirement already satisfied: tzdata>=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)\r\nRequirement already satisfied: pytz>=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\r\nRequirement already satisfied: numpy<2,>=1.22.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)\r\nRequirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: kneed in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.8.5)\r\nRequirement already satisfied: numpy>=1.14.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from kneed) (1.26.2)\r\nRequirement already satisfied: scipy>=1.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from kneed) (1.11.4)\r\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\r\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\r\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n:::\n\n\nIn this code snippet, we begin by loading the dataset and selecting our features of interest, namely the acceleration and rotation measurements. These features are then scaled using `StandardScaler`, a crucial step to ensure that all features contribute equally to the clustering process. The `KMeans` algorithm is then applied to the scaled data, specifying three clusters.\n\n### Visualizing the Initial Results\n\nTo understand our initial clustering, we visualized the results:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nplt.scatter(scaled_features[:,0], scaled_features[:,1], c=clusters, cmap='viridis')\nplt.xlabel('Scaled AccX')\nplt.ylabel('Scaled AccY')\nplt.title('Initial Clustering with 3 Clusters')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=587 height=449}\n:::\n:::\n\n\nThis visualization plots the scaled acceleration data on the X and Y axes, colored by the assigned cluster. It provides a first glimpse into how the data points are grouped by the algorithm.\n\n### Limitations of the Initial Approach\n\nWhile starting with three clusters seemed logical, this approach had its limitations. Clustering, especially KMeans, doesn't inherently consider pre-existing labels. It simply groups data based on feature similarity. Moreover, the true complexity of driving behavior might not be accurately captured in just three categories. This realization led us to question our initial assumption: Were three clusters really sufficient to capture the diverse nuances of driving behavior?\n\nIn the following section, we explore how we addressed these limitations by seeking an optimal number of clusters beyond the confines of the initial labeled categories.\n\n## Realizing the Need for Optimization\n\nAfter the initial clustering, a crucial question arose: were three clusters truly representative of the driving behaviors in our dataset? This led us to the concept of cluster optimization, a pivotal step in ensuring that our machine learning model accurately reflects the complexities of the data.\n\n### The Essence of Cluster Optimization\n\nCluster optimization revolves around finding the 'just right' number of clusters in a dataset. Too few clusters, and the model might oversimplify the data, missing out on important nuances. Too many, and it might overfit, capturing random noise as meaningful patterns. The optimal number of clusters strikes a balance, grouping the data in a way that maximizes both similarity within clusters and differences between them.\n\n### Questioning the Three-Cluster Model\n\nOur initial model with three clusters was a natural starting point, mirroring the three labeled categories in the dataset. However, this approach had its pitfalls:\n\n1. **Assuming Label Completeness**: The labeled categories in the dataset might not encompass all the distinct driving behaviors present. Real-world driving is complex and might not fit neatly into just three categories.\n2. **Ignoring Unsupervised Learning Nature**: Clustering, especially KMeans, is an unsupervised technique. It groups data based on feature similarity, independent of any pre-existing labels. Our model needed to reflect the data's structure, not just its labels.\n\nThese considerations led us to explore beyond the confines of the three predefined categories, seeking a more data-driven approach to determine the number of clusters.\n\n### Embracing the Elbow Method\n\nThe Elbow Method emerged as our tool of choice for finding the optimal number of clusters. It involves plotting the model's performance (measured in terms of Sum of Squared Errors, or SSE) against a range of cluster numbers. The 'elbow point' in this plot, where the rate of decrease in SSE sharply changes, indicates the optimal cluster count.\n\nIn the next section, we'll delve into how we applied the Elbow Method to our dataset, using a combination of scikit-learn and the `kneed` package to not just visualize but also programmatically identify the elbow point, marking a significant step in refining our clustering model.\n\n## Implementing the Elbow Method\n\nWith the realization that our initial model might not optimally capture the driving behaviors in our dataset, we turned to the Elbow Method. This technique is a cornerstone in determining the appropriate number of clusters in unsupervised learning, particularly in KMeans clustering.\n\n### Understanding the Elbow Method\n\nThe Elbow Method is a heuristic used in cluster analysis to determine the number of clusters in a dataset. The method involves running the clustering algorithm multiple times, each time with a different number of clusters ($$ k $$), and calculating the Sum of Squared Errors (SSE) for each. SSE is defined as the sum of the squared distances between each member of a cluster and its centroid. As $$ k $$ increases, SSE tends to decrease as the clusters will be smaller and tighter. However, after a certain point, the marginal decrease in SSE with increasing $$ k $$ becomes insignificant, forming an 'elbow' in the plot. This point is considered the optimal number of clusters.\n\n### Applying the Elbow Method\n\nTo apply the Elbow Method to our dataset, we first needed to calculate the SSE for a range of cluster numbers. Here's how we did it:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nsse = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(scaled_features)\n    sse.append(kmeans.inertia_)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n:::\n\n\nIn this code block, we looped over a range of cluster numbers (from 1 to 10) and for each number, we applied the KMeans algorithm to our scaled features. The inertia_ attribute of KMeans gives us the SSE for that particular cluster count. We stored these SSE values in a list.\n\n### Visualizing SSE vs. Number of Clusters\n\nTo find the elbow point, we plotted the SSE against the number of clusters:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nplt.plot(range(1, 11), sse, marker='o')\nplt.title('Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Sum of Squared Errors (SSE)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=610 height=449}\n:::\n:::\n\n\nThis visualization was crucial as it allowed us to observe the point at which the decrease in SSE becomes less sharp, indicating the optimal number of clusters. In the next section, we'll discuss how we used the `kneed` package to programmatically identify this elbow point, further refining our clustering approach.\n\n## Programmatic Elbow Point Detection with `kneed`\n\nHaving visualized the Sum of Squared Errors (SSE) against various cluster counts using the Elbow Method, our next step was to pinpoint the elusive 'elbow point' programmatically. This is where the `kneed` Python package comes into play, offering a sophisticated yet straightforward approach to identifying the optimal number of clusters in our dataset.\n\n### The Role of `kneed` in Cluster Analysis\n\n`kneed` is a Python library specifically designed to identify the knee point or elbow point in a dataset, which is often subjective and hard to pinpoint manually. It works by fitting a piecewise linear function to the data points and identifying the point of maximum curvature, which is the knee or elbow. This is particularly useful in KMeans clustering, as it takes the guesswork out of selecting the number of clusters.\n\n### Implementing `kneed` to Find the Optimal Clusters\n\nTo utilize `kneed` in our analysis, we first calculated the SSE for different cluster counts, just as we did for the Elbow Method. Then, `kneed` took over to programmatically identify the elbow point:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom kneed import KneeLocator\n\n# Calculating SSE for a range of cluster counts\nsse = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(scaled_features)\n    sse.append(kmeans.inertia_)\n\n# Using kneed to find the elbow point\nknee_locator = KneeLocator(range(1, 11), sse, curve='convex', direction='decreasing')\nelbow_point = knee_locator.elbow\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n:::\n\n\nIn this code snippet, we first computed the SSE for cluster counts from 1 to 10. We then instantiated the `KneeLocator` class from the `kneed` package, passing the range of cluster counts and the corresponding SSE values. The `curve='convex'` and `direction='decreasing'` parameters helped `kneed` understand the nature of our SSE plot. The `elbow` attribute of the `KneeLocator` object gave us the optimal cluster count.\n\n### Determining the Optimal Number of Clusters\n\nTo our surprise, `kneed` identified 4 as the optimal number of clusters, deviating from our initial assumption of 3 clusters based on the dataset's labels. This revelation was pivotal as it highlighted the importance of relying on data-driven methods rather than assumptions or external labeling.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nprint(f\"The optimal number of clusters identified by kneed: {elbow_point}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe optimal number of clusters identified by kneed: 4\n```\n:::\n:::\n\n\nThis insight led us to reapply our clustering model with four clusters, a decision driven entirely by the underlying data structure rather than pre-defined categories. In the next section, we will revisit our clustering approach with this newfound understanding and analyze how this adjustment impacts our analysis of driving behaviors.\n\n## Re-Clustering with Optimized Cluster Count\n\nArmed with the knowledge that four clusters might better represent our driving behavior data, we embarked on a re-clustering journey. This step was crucial for our analysis as it aligned our clustering approach more closely with the inherent structure of the data, moving beyond our initial assumptions.\n\n### Reapplying KMeans with Four Clusters\n\nGuided by the `kneed` package's recommendation, we reconfigured our KMeans clustering model to partition the data into four clusters instead of three. Here's how we proceeded:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Applying KMeans with the optimized number of clusters\noptimized_kmeans = KMeans(n_clusters=4, random_state=42)\noptimized_clusters = optimized_kmeans.fit_predict(scaled_features)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n:::\n\n\nIn this code, the `KMeans` class from scikit-learn was re-initialized with `n_clusters` set to 4. We then fit the model to our scaled features and predicted the cluster for each data point.\n\n### Visualizing the New Clusters\n\nVisualization plays a key role in understanding the implications of our clustering:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nplt.scatter(scaled_features[:,0], scaled_features[:,1], c=optimized_clusters, cmap='viridis')\nplt.xlabel('Scaled AccX')\nplt.ylabel('Scaled AccY')\nplt.title('Clustering with 4 Clusters')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=587 height=449}\n:::\n:::\n\n\nIn this plot, we observed how the data points were grouped into four distinct clusters, providing a fresh perspective on the driving behaviors represented in our dataset.\n\n### Interpreting the New Clustering Results\n\nWith four clusters, the model had the flexibility to identify a more nuanced pattern within the data. This could mean uncovering a behavior that was previously merged with others in the three-cluster model or providing a clearer separation between behaviors.\n\n### Comparing the Four-Cluster Model with the Initial Three-Cluster Model\n\nThe transition from three to four clusters was not just a numerical change but also a conceptual shift. It underscored the significance of letting the data guide our analysis:\n\n- **Increased Granularity**: The four-cluster model offered a finer categorization of driving behaviors, potentially leading to more accurate insights.\n- **Data-Driven Approach**: This shift highlighted the importance of relying on data-driven techniques, such as the Elbow Method and `kneed`, in machine learning tasks.\n\nThe decision to increase the number of clusters was a pivotal moment in our project. It exemplified the dynamic nature of machine learning, where initial hypotheses are continuously tested and refined in light of new evidence.\n\nIn the final section, we will summarize our findings, reflect on the journey, and discuss the potential applications and implications of our analysis in real-world scenarios.\n\n## Conclusions and Insights\n\nAs we conclude our exploration into clustering driving behaviors using the Kaggle dataset, it's crucial to reflect on the key insights gained and the broader implications of our findings.\n\n### Key Findings from the Optimized Clustering\n\nOur journey through clustering revealed several important insights:\n\n- **Beyond Assumptions**: While assumptions and labels can provide direction, they should not be the sole basis for decision-making in clustering problems.\n- **Embracing Flexibility in Analysis**: Machine learning, especially unsupervised learning like clustering, requires an openness to adapt analysis strategies based on what the data reveals.\n- **Iterative Process**: Optimization in machine learning is an iterative process. It often involves revisiting and refining models as new data or techniques become available.\n\nOur journey through clustering driving behaviors illustrates the dynamic and iterative nature of machine learning. By embracing a data-driven approach, we were able to uncover more about the dataset than what was initially apparent. Such methodologies are not just limited to driving behavior analysis but are applicable across various domains where data-driven insights are crucial.\n\nAs we continue to advance in the field of machine learning, the lessons learned here about the importance of flexibility, iteration, and data-driven analysis will remain fundamental to unlocking the full potential of our data.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}