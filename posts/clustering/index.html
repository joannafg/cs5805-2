<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Joanna Fang">
<meta name="dcterms.date" content="2023-11-23">

<title>Ziming’s Journey in Machine Learning - 2. Clustering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Ziming’s Journey in Machine Learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/joannafg/cs5805-2" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/joanna-fang-6122ba182/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">2. Clustering</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i></button></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">ml</div>
                <div class="quarto-category">code</div>
                <div class="quarto-category">clustering</div>
                <div class="quarto-category">driving</div>
                <div class="quarto-category">kaggle</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Joanna Fang </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 23, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#clustering-optimizing-driver-behavior-analysis-with-machine-learning-clustering" id="toc-clustering-optimizing-driver-behavior-analysis-with-machine-learning-clustering" class="nav-link active" data-scroll-target="#clustering-optimizing-driver-behavior-analysis-with-machine-learning-clustering">Clustering: Optimizing Driver Behavior Analysis with Machine Learning Clustering</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#setting-the-stage-with-data-and-tools" id="toc-setting-the-stage-with-data-and-tools" class="nav-link" data-scroll-target="#setting-the-stage-with-data-and-tools">Setting the Stage with Data and Tools</a>
  <ul class="collapse">
  <li><a href="#the-kaggle-driving-behavior-dataset" id="toc-the-kaggle-driving-behavior-dataset" class="nav-link" data-scroll-target="#the-kaggle-driving-behavior-dataset">The Kaggle ‘Driving Behavior’ Dataset</a></li>
  </ul></li>
  <li><a href="#initial-clustering-approach" id="toc-initial-clustering-approach" class="nav-link" data-scroll-target="#initial-clustering-approach">Initial Clustering Approach</a>
  <ul class="collapse">
  <li><a href="#why-start-with-three-clusters" id="toc-why-start-with-three-clusters" class="nav-link" data-scroll-target="#why-start-with-three-clusters">Why Start with Three Clusters?</a></li>
  <li><a href="#applying-kmeans-clustering" id="toc-applying-kmeans-clustering" class="nav-link" data-scroll-target="#applying-kmeans-clustering">Applying KMeans Clustering</a></li>
  <li><a href="#visualizing-the-initial-results" id="toc-visualizing-the-initial-results" class="nav-link" data-scroll-target="#visualizing-the-initial-results">Visualizing the Initial Results</a></li>
  <li><a href="#limitations-of-the-initial-approach" id="toc-limitations-of-the-initial-approach" class="nav-link" data-scroll-target="#limitations-of-the-initial-approach">Limitations of the Initial Approach</a></li>
  </ul></li>
  <li><a href="#the-need-for-optimization" id="toc-the-need-for-optimization" class="nav-link" data-scroll-target="#the-need-for-optimization">The Need for Optimization</a>
  <ul class="collapse">
  <li><a href="#the-essence-of-cluster-optimization" id="toc-the-essence-of-cluster-optimization" class="nav-link" data-scroll-target="#the-essence-of-cluster-optimization">The Essence of Cluster Optimization</a></li>
  <li><a href="#questioning-the-three-cluster-model" id="toc-questioning-the-three-cluster-model" class="nav-link" data-scroll-target="#questioning-the-three-cluster-model">Questioning the Three-Cluster Model</a></li>
  </ul></li>
  <li><a href="#implementing-the-elbow-method" id="toc-implementing-the-elbow-method" class="nav-link" data-scroll-target="#implementing-the-elbow-method">Implementing the Elbow Method</a>
  <ul class="collapse">
  <li><a href="#understanding-the-elbow-method" id="toc-understanding-the-elbow-method" class="nav-link" data-scroll-target="#understanding-the-elbow-method">Understanding the Elbow Method</a></li>
  <li><a href="#applying-the-elbow-method" id="toc-applying-the-elbow-method" class="nav-link" data-scroll-target="#applying-the-elbow-method">Applying the Elbow Method</a></li>
  <li><a href="#visualizing-sse-vs.-number-of-clusters" id="toc-visualizing-sse-vs.-number-of-clusters" class="nav-link" data-scroll-target="#visualizing-sse-vs.-number-of-clusters">Visualizing SSE vs.&nbsp;Number of Clusters</a></li>
  </ul></li>
  <li><a href="#programmatic-elbow-point-detection-with-kneed" id="toc-programmatic-elbow-point-detection-with-kneed" class="nav-link" data-scroll-target="#programmatic-elbow-point-detection-with-kneed">Programmatic Elbow Point Detection with <code>kneed</code></a>
  <ul class="collapse">
  <li><a href="#the-role-of-kneed-in-cluster-analysis" id="toc-the-role-of-kneed-in-cluster-analysis" class="nav-link" data-scroll-target="#the-role-of-kneed-in-cluster-analysis">The Role of <code>kneed</code> in Cluster Analysis</a></li>
  <li><a href="#implementing-kneed-to-find-the-optimal-clusters" id="toc-implementing-kneed-to-find-the-optimal-clusters" class="nav-link" data-scroll-target="#implementing-kneed-to-find-the-optimal-clusters">Implementing <code>kneed</code> to Find the Optimal Clusters</a></li>
  <li><a href="#determining-the-optimal-number-of-clusters" id="toc-determining-the-optimal-number-of-clusters" class="nav-link" data-scroll-target="#determining-the-optimal-number-of-clusters">Determining the Optimal Number of Clusters</a></li>
  </ul></li>
  <li><a href="#re-clustering-with-optimized-cluster-count" id="toc-re-clustering-with-optimized-cluster-count" class="nav-link" data-scroll-target="#re-clustering-with-optimized-cluster-count">Re-Clustering with Optimized Cluster Count</a>
  <ul class="collapse">
  <li><a href="#reapplying-kmeans-with-four-clusters" id="toc-reapplying-kmeans-with-four-clusters" class="nav-link" data-scroll-target="#reapplying-kmeans-with-four-clusters">Reapplying KMeans with Four Clusters</a></li>
  <li><a href="#visualizing-the-new-clusters" id="toc-visualizing-the-new-clusters" class="nav-link" data-scroll-target="#visualizing-the-new-clusters">Visualizing the New Clusters</a></li>
  <li><a href="#interpreting-the-new-clustering-results" id="toc-interpreting-the-new-clustering-results" class="nav-link" data-scroll-target="#interpreting-the-new-clustering-results">Interpreting the New Clustering Results</a></li>
  <li><a href="#comparing-the-four-cluster-model-with-the-initial-three-cluster-model" id="toc-comparing-the-four-cluster-model-with-the-initial-three-cluster-model" class="nav-link" data-scroll-target="#comparing-the-four-cluster-model-with-the-initial-three-cluster-model">Comparing the Four-Cluster Model with the Initial Three-Cluster Model</a></li>
  </ul></li>
  <li><a href="#conclusions-and-insights" id="toc-conclusions-and-insights" class="nav-link" data-scroll-target="#conclusions-and-insights">Conclusions and Insights</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/joannafg/cs5805-2/edit/main/posts/clustering/index.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/joannafg/cs5805-2/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="clustering-optimizing-driver-behavior-analysis-with-machine-learning-clustering" class="level1">
<h1>Clustering: Optimizing Driver Behavior Analysis with Machine Learning Clustering</h1>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="thumbnail.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>The analysis of driving behaviors, a task of both complexity and nuance, is increasingly significant in the realms of traffic safety and autonomous vehicle development. Machine learning, particularly clustering algorithms, stands at the forefront of this analysis, offering the ability to dissect and understand the myriad patterns hidden within driving data. Clustering, a form of unsupervised learning, excels in finding structure within unlabeled data, grouping similar data points together based on shared characteristics.</p>
<p>In this blog, we dive into a practical application of clustering using a dataset from Kaggle titled ‘Driving Behavior’. This dataset is intriguing because it comes with labeled data, categorizing driving behaviors into three types: SLOW, NORMAL, and AGGRESSIVE. While clustering typically operates on unlabeled data, we use this labeled dataset to our advantage. It will allow us to compare the clusters formed by our machine learning algorithm against the pre-labeled categories, providing an insightful backdrop to understand and validate our clustering results.</p>
<p>Our aim is to illustrate the entire process of applying clustering to analyze driving behavior. We start with a predefined notion of having three clusters based on the labels present in the dataset. However, as we delve deeper into the analysis, we realize the necessity to step back and question our initial assumptions. We employ the Elbow Method, supplemented by the <code>kneed</code> Python package, to determine the optimal number of clusters in a more data-driven manner. This journey from assumption to optimization is not just about applying a machine learning technique but also about understanding the nuances and intricacies that come with real-world data analysis.</p>
<p>Join us as we unravel the complexities of driving behaviors through clustering, starting with predefined labels and moving towards an optimized, data-driven approach.</p>
</section>
<section id="setting-the-stage-with-data-and-tools" class="level2">
<h2 class="anchored" data-anchor-id="setting-the-stage-with-data-and-tools">Setting the Stage with Data and Tools</h2>
<p>In the world of machine learning, the choice of dataset is as crucial as the algorithm itself. For our endeavor into clustering driving behaviors, we’ve chosen a dataset from Kaggle named ‘Driving Behavior’. This dataset is particularly interesting for a few reasons. Firstly, it’s labeled with specific driving behaviors, which, while not necessary for clustering, provides us with a unique opportunity to validate and understand our clustering results in a more tangible way.</p>
<section id="the-kaggle-driving-behavior-dataset" class="level3">
<h3 class="anchored" data-anchor-id="the-kaggle-driving-behavior-dataset">The Kaggle ‘Driving Behavior’ Dataset</h3>
<p>This dataset offers a detailed glimpse into various aspects of driving, captured through different features:</p>
<ul>
<li><strong>Acceleration (AccX, AccY, AccZ)</strong>: These features measure the vehicle’s acceleration in meters per second squared (<span class="math inline">\(m/s^2\)</span>) along the X, Y, and Z axes. Acceleration data is crucial for understanding sudden movements and changes in driving speed.</li>
<li><strong>Rotation (GyroX, GyroY, GyroZ)</strong>: Here, we have the vehicle’s angular velocity around the X, Y, and Z axes, measured in degrees per second (<span class="math inline">\(°/s\)</span>). This data helps in identifying steering patterns and gauging the stability of the vehicle during various maneuvers.</li>
<li><strong>Classification Labels (SLOW, NORMAL, AGGRESSIVE)</strong>: Each data point is tagged with one of these labels. It’s important to note that in clustering, such labels are not typically required. However, in our case, these labels will serve as a benchmark, enabling us to compare our unsupervised learning results with pre-defined categories.</li>
</ul>
<p>This dataset not only provides a foundation for applying clustering techniques but also allows us to explore how well unsupervised learning can mirror human-labeled classifications.</p>
<p>In the next sections, we’ll dive into the process of applying these tools to our dataset, starting with our initial approach and gradually moving towards a more refined, data-driven methodology.</p>
</section>
</section>
<section id="initial-clustering-approach" class="level2">
<h2 class="anchored" data-anchor-id="initial-clustering-approach">Initial Clustering Approach</h2>
<p>Embarking on our journey to unravel the driving patterns hidden in the dataset, we initially gravitated towards a straightforward approach: clustering the data into three groups. This decision was influenced by the dataset’s labels - SLOW, NORMAL, and AGGRESSIVE. It seemed logical to align our clusters with these pre-defined categories, under the assumption that they would naturally encapsulate the essence of the driving behaviors.</p>
<section id="why-start-with-three-clusters" class="level3">
<h3 class="anchored" data-anchor-id="why-start-with-three-clusters">Why Start with Three Clusters?</h3>
<p>The rationale was straightforward: the dataset labels suggested three distinct types of driving behaviors. Clustering aims to group similar data points, and with these labels as a guide, it seemed reasonable to start our analysis by segmenting the data into three clusters, hoping they would align with the labeled behaviors.</p>
</section>
<section id="applying-kmeans-clustering" class="level3">
<h3 class="anchored" data-anchor-id="applying-kmeans-clustering">Applying KMeans Clustering</h3>
<p>The KMeans algorithm was chosen for its simplicity and effectiveness in many clustering scenarios. Here’s a breakdown of how we applied it:</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install numpy</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install matplotlib</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install scipy</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install scikit<span class="op">-</span>learn</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install pandas</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install kneed</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the datasets</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>data_1 <span class="op">=</span> pd.read_csv(<span class="st">'motion_data_1.csv'</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>data_2 <span class="op">=</span> pd.read_csv(<span class="st">'motion_data_2.csv'</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine datasets</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.concat([data_1, data_2])</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Data preprocessing</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> data[[<span class="st">'AccX'</span>, <span class="st">'AccY'</span>, <span class="st">'AccZ'</span>, <span class="st">'GyroX'</span>, <span class="st">'GyroY'</span>, <span class="st">'GyroZ'</span>]]</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>scaled_features <span class="op">=</span> scaler.fit_transform(features)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Applying KMeans with 3 clusters</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>clusters <span class="op">=</span> kmeans.fit_predict(scaled_features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: numpy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.26.2)
WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.
You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: matplotlib in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (3.8.2)
Requirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)
Requirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.1.1)
Requirement already satisfied: numpy&lt;2,&gt;=1.21 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.2)
Requirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.1.0)
Requirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)
Requirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)
Requirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.0)
Requirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.45.1)
Requirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.1)
Requirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (23.2)
Requirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib) (3.17.0)
Requirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.15.0)
WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.
You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: scipy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.11.4)
Requirement already satisfied: numpy&lt;1.28.0,&gt;=1.21.6 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scipy) (1.26.2)
WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.
You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: scikit-learn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.3.2)
Requirement already satisfied: scipy&gt;=1.5.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.11.4)
Requirement already satisfied: joblib&gt;=1.1.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.3.2)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.2.0)
Requirement already satisfied: numpy&lt;2.0,&gt;=1.17.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.2)
WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.
You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: pandas in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (2.1.3)
Requirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)
Requirement already satisfied: numpy&lt;2,&gt;=1.22.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)
Requirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)
Requirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)
Requirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.15.0)
WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.
You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: kneed in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.8.5)
Requirement already satisfied: scipy&gt;=1.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from kneed) (1.11.4)
Requirement already satisfied: numpy&gt;=1.14.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from kneed) (1.26.2)
WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.
You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)</code></pre>
</div>
</div>
<p>In this code snippet, we begin by loading the dataset and selecting our features of interest, namely the acceleration and rotation measurements. These features are then scaled using <code>StandardScaler</code>, a crucial step to ensure that all features contribute equally to the clustering process. The <code>KMeans</code> algorithm is then applied to the scaled data, specifying three clusters.</p>
</section>
<section id="visualizing-the-initial-results" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-the-initial-results">Visualizing the Initial Results</h3>
<p>To understand our initial clustering, we visualized the results:</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(scaled_features[:,<span class="dv">0</span>], scaled_features[:,<span class="dv">1</span>], c<span class="op">=</span>clusters, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Scaled AccX'</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Scaled AccY'</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Initial Clustering with 3 Clusters'</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-3-output-1.png" width="587" height="449"></p>
</div>
</div>
<p>This visualization plots the scaled acceleration data on the X and Y axes, colored by the assigned cluster.</p>
</section>
<section id="limitations-of-the-initial-approach" class="level3">
<h3 class="anchored" data-anchor-id="limitations-of-the-initial-approach">Limitations of the Initial Approach</h3>
<p>While starting with three clusters seemed logical, this approach had its limitations. Clustering, especially KMeans, doesn’t inherently consider pre-existing labels. It simply groups data based on feature similarity. Moreover, the true complexity of driving behavior might not be accurately captured in just three categories. This realization led us to question our initial assumption: Were three clusters really sufficient to capture the diverse nuances of driving behavior?</p>
<p>In the following section, we explore how we addressed these limitations by seeking an optimal number of clusters beyond the confines of the initial labeled categories.</p>
</section>
</section>
<section id="the-need-for-optimization" class="level2">
<h2 class="anchored" data-anchor-id="the-need-for-optimization">The Need for Optimization</h2>
<p>After the initial clustering, a crucial question arose: were three clusters truly representative of the driving behaviors in our dataset? This led us to the concept of cluster optimization, a pivotal step in ensuring that our machine learning model accurately reflects the complexities of the data.</p>
<section id="the-essence-of-cluster-optimization" class="level3">
<h3 class="anchored" data-anchor-id="the-essence-of-cluster-optimization">The Essence of Cluster Optimization</h3>
<p>Cluster optimization revolves around finding the ‘just right’ number of clusters in a dataset. Too few clusters, and the model might oversimplify the data, missing out on important nuances. Too many, and it might overfit, capturing random noise as meaningful patterns. The optimal number of clusters strikes a balance, grouping the data in a way that maximizes both similarity within clusters and differences between them.</p>
</section>
<section id="questioning-the-three-cluster-model" class="level3">
<h3 class="anchored" data-anchor-id="questioning-the-three-cluster-model">Questioning the Three-Cluster Model</h3>
<p>Our initial model with three clusters was a natural starting point, mirroring the three labeled categories in the dataset. However, this approach had its pitfalls:</p>
<ol type="1">
<li><strong>Assuming Label Completeness</strong>: The labeled categories in the dataset might not encompass all the distinct driving behaviors present. Real-world driving is complex and might not fit neatly into just three categories.</li>
<li><strong>Ignoring Unsupervised Learning Nature</strong>: Clustering, especially KMeans, is an unsupervised technique. It groups data based on feature similarity, independent of any pre-existing labels. Our model needed to reflect the data’s structure, not just its labels.</li>
</ol>
<p>These considerations led us to explore beyond the confines of the three predefined categories, seeking a more data-driven approach to determine the number of clusters.</p>
</section>
</section>
<section id="implementing-the-elbow-method" class="level2">
<h2 class="anchored" data-anchor-id="implementing-the-elbow-method">Implementing the Elbow Method</h2>
<p>With the realization that our initial model might not optimally capture the driving behaviors in our dataset, we turned to the Elbow Method. This technique is a cornerstone in determining the appropriate number of clusters in unsupervised learning, particularly in KMeans clustering.</p>
<section id="understanding-the-elbow-method" class="level3">
<h3 class="anchored" data-anchor-id="understanding-the-elbow-method">Understanding the Elbow Method</h3>
<p>The Elbow Method is a heuristic used in cluster analysis to determine the number of clusters in a dataset. The method involves running the clustering algorithm multiple times, each time with a different number of clusters ($ k $), and calculating the Sum of Squared Errors (SSE) for each. SSE is defined as the sum of the squared distances between each member of a cluster and its centroid. As $ k $ increases, SSE tends to decrease as the clusters will be smaller and tighter. However, after a certain point, the marginal decrease in SSE with increasing $ k $ becomes insignificant, forming an ‘elbow’ in the plot. This point is considered the optimal number of clusters.</p>
</section>
<section id="applying-the-elbow-method" class="level3">
<h3 class="anchored" data-anchor-id="applying-the-elbow-method">Applying the Elbow Method</h3>
<p>To apply the Elbow Method to our dataset, we first needed to calculate the SSE for a range of cluster numbers. Here’s how we did it:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>sse <span class="op">=</span> []</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    kmeans.fit(scaled_features)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    sse.append(kmeans.inertia_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)</code></pre>
</div>
</div>
<p>In this code block, we looped over a range of cluster numbers (from 1 to 10) and for each number, we applied the KMeans algorithm to our scaled features. The inertia_ attribute of KMeans gives us the SSE for that particular cluster count. We stored these SSE values in a list.</p>
</section>
<section id="visualizing-sse-vs.-number-of-clusters" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-sse-vs.-number-of-clusters">Visualizing SSE vs.&nbsp;Number of Clusters</h3>
<p>To find the elbow point, we plotted the SSE against the number of clusters:</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>), sse, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Elbow Method'</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Clusters'</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Sum of Squared Errors (SSE)'</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-5-output-1.png" width="610" height="449"></p>
</div>
</div>
<p>This visualization was crucial as it allowed us to observe the point at which the decrease in SSE becomes less sharp, indicating the optimal number of clusters. In the next section, we’ll discuss how we used the <code>kneed</code> package to programmatically identify this elbow point, further refining our clustering approach.</p>
</section>
</section>
<section id="programmatic-elbow-point-detection-with-kneed" class="level2">
<h2 class="anchored" data-anchor-id="programmatic-elbow-point-detection-with-kneed">Programmatic Elbow Point Detection with <code>kneed</code></h2>
<p>Having visualized the Sum of Squared Errors (SSE) against various cluster counts using the Elbow Method, our next step was to pinpoint the elusive ‘elbow point’ programmatically. This is where the <code>kneed</code> Python package comes into play, offering a sophisticated yet straightforward approach to identifying the optimal number of clusters in our dataset.</p>
<section id="the-role-of-kneed-in-cluster-analysis" class="level3">
<h3 class="anchored" data-anchor-id="the-role-of-kneed-in-cluster-analysis">The Role of <code>kneed</code> in Cluster Analysis</h3>
<p><code>kneed</code> is a Python library specifically designed to identify the knee point or elbow point in a dataset, which is often subjective and hard to pinpoint manually. It works by fitting a piecewise linear function to the data points and identifying the point of maximum curvature, which is the knee or elbow. This is particularly useful in KMeans clustering, as it takes the guesswork out of selecting the number of clusters.</p>
</section>
<section id="implementing-kneed-to-find-the-optimal-clusters" class="level3">
<h3 class="anchored" data-anchor-id="implementing-kneed-to-find-the-optimal-clusters">Implementing <code>kneed</code> to Find the Optimal Clusters</h3>
<p>To utilize <code>kneed</code> in our analysis, we first calculated the SSE for different cluster counts, just as we did for the Elbow Method. Then, <code>kneed</code> took over to programmatically identify the elbow point:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> kneed <span class="im">import</span> KneeLocator</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculating SSE for a range of cluster counts</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>sse <span class="op">=</span> []</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>):</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    kmeans.fit(scaled_features)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    sse.append(kmeans.inertia_)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Using kneed to find the elbow point</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>knee_locator <span class="op">=</span> KneeLocator(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>), sse, curve<span class="op">=</span><span class="st">'convex'</span>, direction<span class="op">=</span><span class="st">'decreasing'</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>elbow_point <span class="op">=</span> knee_locator.elbow</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)</code></pre>
</div>
</div>
<p>In this code snippet, we first computed the SSE for cluster counts from 1 to 10. We then instantiated the <code>KneeLocator</code> class from the <code>kneed</code> package, passing the range of cluster counts and the corresponding SSE values. The <code>curve='convex'</code> and <code>direction='decreasing'</code> parameters helped <code>kneed</code> understand the nature of our SSE plot. The <code>elbow</code> attribute of the <code>KneeLocator</code> object gave us the optimal cluster count.</p>
</section>
<section id="determining-the-optimal-number-of-clusters" class="level3">
<h3 class="anchored" data-anchor-id="determining-the-optimal-number-of-clusters">Determining the Optimal Number of Clusters</h3>
<p>To our surprise, <code>kneed</code> identified 4 as the optimal number of clusters, deviating from our initial assumption of 3 clusters based on the dataset’s labels. This revelation was pivotal as it highlighted the importance of relying on data-driven methods rather than assumptions or external labeling.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The optimal number of clusters identified by kneed: </span><span class="sc">{</span>elbow_point<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The optimal number of clusters identified by kneed: 4</code></pre>
</div>
</div>
<p>This insight led us to reapply our clustering model with four clusters, a decision driven entirely by the underlying data structure rather than pre-defined categories. In the next section, we will revisit our clustering approach with this newfound understanding and analyze how this adjustment impacts our analysis of driving behaviors.</p>
</section>
</section>
<section id="re-clustering-with-optimized-cluster-count" class="level2">
<h2 class="anchored" data-anchor-id="re-clustering-with-optimized-cluster-count">Re-Clustering with Optimized Cluster Count</h2>
<p>Armed with the knowledge that four clusters might better represent our driving behavior data, we embarked on a re-clustering journey. This step was crucial for our analysis as it aligned our clustering approach more closely with the inherent structure of the data, moving beyond our initial assumptions.</p>
<section id="reapplying-kmeans-with-four-clusters" class="level3">
<h3 class="anchored" data-anchor-id="reapplying-kmeans-with-four-clusters">Reapplying KMeans with Four Clusters</h3>
<p>Guided by the <code>kneed</code> package’s recommendation, we reconfigured our KMeans clustering model to partition the data into four clusters instead of three. Here’s how we proceeded:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Applying KMeans with the optimized number of clusters</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>optimized_kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>optimized_clusters <span class="op">=</span> optimized_kmeans.fit_predict(scaled_features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)</code></pre>
</div>
</div>
<p>In this code, the <code>KMeans</code> class from scikit-learn was re-initialized with <code>n_clusters</code> set to 4. We then fit the model to our scaled features and predicted the cluster for each data point.</p>
</section>
<section id="visualizing-the-new-clusters" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-the-new-clusters">Visualizing the New Clusters</h3>
<p>Visualization plays a key role in understanding the implications of our clustering:</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(scaled_features[:,<span class="dv">0</span>], scaled_features[:,<span class="dv">1</span>], c<span class="op">=</span>optimized_clusters, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Scaled AccX'</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Scaled AccY'</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Clustering with 4 Clusters'</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-9-output-1.png" width="587" height="449"></p>
</div>
</div>
<p>In this plot, we observed how the data points were grouped into four distinct clusters, providing a fresh perspective on the driving behaviors represented in our dataset.</p>
</section>
<section id="interpreting-the-new-clustering-results" class="level3">
<h3 class="anchored" data-anchor-id="interpreting-the-new-clustering-results">Interpreting the New Clustering Results</h3>
<p>With four clusters, the model had the flexibility to identify a more nuanced pattern within the data. This could mean uncovering a behavior that was previously merged with others in the three-cluster model or providing a clearer separation between behaviors.</p>
</section>
<section id="comparing-the-four-cluster-model-with-the-initial-three-cluster-model" class="level3">
<h3 class="anchored" data-anchor-id="comparing-the-four-cluster-model-with-the-initial-three-cluster-model">Comparing the Four-Cluster Model with the Initial Three-Cluster Model</h3>
<p>The transition from three to four clusters was not just a numerical change but also a conceptual shift. It underscored the significance of letting the data guide our analysis:</p>
<ul>
<li><strong>Increased Granularity</strong>: The four-cluster model offered a finer categorization of driving behaviors, potentially leading to more accurate insights.</li>
<li><strong>Data-Driven Approach</strong>: This shift highlighted the importance of relying on data-driven techniques, such as the Elbow Method and <code>kneed</code>, in machine learning tasks.</li>
</ul>
<p>The decision to increase the number of clusters was a pivotal moment in our project. It exemplified the dynamic nature of machine learning, where initial hypotheses are continuously tested and refined in light of new evidence.</p>
<p>In the final section, we will summarize our findings, reflect on the journey, and discuss the potential applications and implications of our analysis in real-world scenarios.</p>
</section>
</section>
<section id="conclusions-and-insights" class="level2">
<h2 class="anchored" data-anchor-id="conclusions-and-insights">Conclusions and Insights</h2>
<p>Our journey through clustering revealed several important insights:</p>
<ul>
<li><strong>Beyond Assumptions</strong>: While assumptions and labels can provide direction, they should not be the sole basis for decision-making in clustering problems.</li>
<li><strong>Embracing Flexibility in Analysis</strong>: Machine learning, especially unsupervised learning like clustering, requires an openness to adapt analysis strategies based on what the data reveals.</li>
<li><strong>Iterative Process</strong>: Optimization in machine learning is an iterative process. It often involves revisiting and refining models as new data or techniques become available.</li>
</ul>
<p>Our journey through clustering driving behaviors illustrates the dynamic and iterative nature of machine learning. By embracing a data-driven approach, we were able to uncover more about the dataset than what was initially apparent. Such methodologies are not just limited to driving behavior analysis but are applicable across various domains where data-driven insights are crucial.</p>
<p>As we continue to advance in the field of machine learning, the lessons learned here about the importance of flexibility, iteration, and data-driven analysis will remain fundamental to unlocking the full potential of our data.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li>https://developers.google.com/machine-learning/clustering/overview</li>
<li>https://www.geeksforgeeks.org/clustering-in-machine-learning/</li>
<li>https://machinelearningmastery.com/clustering-algorithms-with-python/</li>
<li>https://realpython.com/k-means-clustering-python/</li>
<li>https://www.youtube.com/watch?v=ht7geyMAFfA</li>
<li>OpenAI. (2023). ChatGPT [Large language model]. https://chat.openai.com</li>
<li>https://www.kaggle.com/datasets/outofskills/driving-behavior</li>
</ul>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb15" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "2\\. Clustering"</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Joanna Fang"</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2023-11-23"</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [ml, code, clustering, driving, kaggle]</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co">  html: </span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co">    code-block-bg: "#FFFFFF"</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co">    code-block-border-left: "#E83283"</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools:</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co">      source: true</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co">      toggle: false</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="co">      caption: none</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="fu"># Clustering: Optimizing Driver Behavior Analysis with Machine Learning Clustering </span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a><span class="al">![](thumbnail.jpg)</span>{width="50%" fig-align="center"}</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>The analysis of driving behaviors, a task of both complexity and nuance, is increasingly significant in the realms of traffic safety and autonomous vehicle development. Machine learning, particularly clustering algorithms, stands at the forefront of this analysis, offering the ability to dissect and understand the myriad patterns hidden within driving data. Clustering, a form of unsupervised learning, excels in finding structure within unlabeled data, grouping similar data points together based on shared characteristics.</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>In this blog, we dive into a practical application of clustering using a dataset from Kaggle titled 'Driving Behavior'. This dataset is intriguing because it comes with labeled data, categorizing driving behaviors into three types: SLOW, NORMAL, and AGGRESSIVE. While clustering typically operates on unlabeled data, we use this labeled dataset to our advantage. It will allow us to compare the clusters formed by our machine learning algorithm against the pre-labeled categories, providing an insightful backdrop to understand and validate our clustering results.</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>Our aim is to illustrate the entire process of applying clustering to analyze driving behavior. We start with a predefined notion of having three clusters based on the labels present in the dataset. However, as we delve deeper into the analysis, we realize the necessity to step back and question our initial assumptions. We employ the Elbow Method, supplemented by the <span class="in">`kneed`</span> Python package, to determine the optimal number of clusters in a more data-driven manner. This journey from assumption to optimization is not just about applying a machine learning technique but also about understanding the nuances and intricacies that come with real-world data analysis.</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>Join us as we unravel the complexities of driving behaviors through clustering, starting with predefined labels and moving towards an optimized, data-driven approach.</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a><span class="fu">## Setting the Stage with Data and Tools</span></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>In the world of machine learning, the choice of dataset is as crucial as the algorithm itself. For our endeavor into clustering driving behaviors, we've chosen a dataset from Kaggle named 'Driving Behavior'. This dataset is particularly interesting for a few reasons. Firstly, it's labeled with specific driving behaviors, which, while not necessary for clustering, provides us with a unique opportunity to validate and understand our clustering results in a more tangible way.</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Kaggle 'Driving Behavior' Dataset</span></span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>This dataset offers a detailed glimpse into various aspects of driving, captured through different features:</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Acceleration (AccX, AccY, AccZ)**: These features measure the vehicle's acceleration in meters per second squared ($m/s^2$) along the X, Y, and Z axes. Acceleration data is crucial for understanding sudden movements and changes in driving speed.</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Rotation (GyroX, GyroY, GyroZ)**: Here, we have the vehicle's angular velocity around the X, Y, and Z axes, measured in degrees per second ($°/s$). This data helps in identifying steering patterns and gauging the stability of the vehicle during various maneuvers.</span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Classification Labels (SLOW, NORMAL, AGGRESSIVE)**: Each data point is tagged with one of these labels. It's important to note that in clustering, such labels are not typically required. However, in our case, these labels will serve as a benchmark, enabling us to compare our unsupervised learning results with pre-defined categories.</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>This dataset not only provides a foundation for applying clustering techniques but also allows us to explore how well unsupervised learning can mirror human-labeled classifications.</span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>In the next sections, we'll dive into the process of applying these tools to our dataset, starting with our initial approach and gradually moving towards a more refined, data-driven methodology.</span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a><span class="fu">## Initial Clustering Approach</span></span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>Embarking on our journey to unravel the driving patterns hidden in the dataset, we initially gravitated towards a straightforward approach: clustering the data into three groups. This decision was influenced by the dataset's labels - SLOW, NORMAL, and AGGRESSIVE. It seemed logical to align our clusters with these pre-defined categories, under the assumption that they would naturally encapsulate the essence of the driving behaviors.</span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why Start with Three Clusters?</span></span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a>The rationale was straightforward: the dataset labels suggested three distinct types of driving behaviors. Clustering aims to group similar data points, and with these labels as a guide, it seemed reasonable to start our analysis by segmenting the data into three clusters, hoping they would align with the labeled behaviors.</span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a><span class="fu">### Applying KMeans Clustering</span></span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a>The KMeans algorithm was chosen for its simplicity and effectiveness in many clustering scenarios. Here's a breakdown of how we applied it:</span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install numpy</span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install matplotlib</span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install scipy</span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install scikit<span class="op">-</span>learn</span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install pandas</span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install kneed</span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the datasets</span></span>
<span id="cb15-77"><a href="#cb15-77" aria-hidden="true" tabindex="-1"></a>data_1 <span class="op">=</span> pd.read_csv(<span class="st">'motion_data_1.csv'</span>)</span>
<span id="cb15-78"><a href="#cb15-78" aria-hidden="true" tabindex="-1"></a>data_2 <span class="op">=</span> pd.read_csv(<span class="st">'motion_data_2.csv'</span>)</span>
<span id="cb15-79"><a href="#cb15-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-80"><a href="#cb15-80" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine datasets</span></span>
<span id="cb15-81"><a href="#cb15-81" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.concat([data_1, data_2])</span>
<span id="cb15-82"><a href="#cb15-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-83"><a href="#cb15-83" aria-hidden="true" tabindex="-1"></a><span class="co"># Data preprocessing</span></span>
<span id="cb15-84"><a href="#cb15-84" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb15-85"><a href="#cb15-85" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> data[[<span class="st">'AccX'</span>, <span class="st">'AccY'</span>, <span class="st">'AccZ'</span>, <span class="st">'GyroX'</span>, <span class="st">'GyroY'</span>, <span class="st">'GyroZ'</span>]]</span>
<span id="cb15-86"><a href="#cb15-86" aria-hidden="true" tabindex="-1"></a>scaled_features <span class="op">=</span> scaler.fit_transform(features)</span>
<span id="cb15-87"><a href="#cb15-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-88"><a href="#cb15-88" aria-hidden="true" tabindex="-1"></a><span class="co"># Applying KMeans with 3 clusters</span></span>
<span id="cb15-89"><a href="#cb15-89" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb15-90"><a href="#cb15-90" aria-hidden="true" tabindex="-1"></a>clusters <span class="op">=</span> kmeans.fit_predict(scaled_features)</span>
<span id="cb15-91"><a href="#cb15-91" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-92"><a href="#cb15-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-93"><a href="#cb15-93" aria-hidden="true" tabindex="-1"></a>In this code snippet, we begin by loading the dataset and selecting our features of interest, namely the acceleration and rotation measurements. These features are then scaled using <span class="in">`StandardScaler`</span>, a crucial step to ensure that all features contribute equally to the clustering process. The <span class="in">`KMeans`</span> algorithm is then applied to the scaled data, specifying three clusters.</span>
<span id="cb15-94"><a href="#cb15-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-95"><a href="#cb15-95" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visualizing the Initial Results</span></span>
<span id="cb15-96"><a href="#cb15-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-97"><a href="#cb15-97" aria-hidden="true" tabindex="-1"></a>To understand our initial clustering, we visualized the results:</span>
<span id="cb15-98"><a href="#cb15-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-101"><a href="#cb15-101" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-102"><a href="#cb15-102" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb15-103"><a href="#cb15-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-104"><a href="#cb15-104" aria-hidden="true" tabindex="-1"></a>plt.scatter(scaled_features[:,<span class="dv">0</span>], scaled_features[:,<span class="dv">1</span>], c<span class="op">=</span>clusters, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb15-105"><a href="#cb15-105" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Scaled AccX'</span>)</span>
<span id="cb15-106"><a href="#cb15-106" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Scaled AccY'</span>)</span>
<span id="cb15-107"><a href="#cb15-107" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Initial Clustering with 3 Clusters'</span>)</span>
<span id="cb15-108"><a href="#cb15-108" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb15-109"><a href="#cb15-109" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-110"><a href="#cb15-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-111"><a href="#cb15-111" aria-hidden="true" tabindex="-1"></a>This visualization plots the scaled acceleration data on the X and Y axes, colored by the assigned cluster. </span>
<span id="cb15-112"><a href="#cb15-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-113"><a href="#cb15-113" aria-hidden="true" tabindex="-1"></a><span class="fu">### Limitations of the Initial Approach</span></span>
<span id="cb15-114"><a href="#cb15-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-115"><a href="#cb15-115" aria-hidden="true" tabindex="-1"></a>While starting with three clusters seemed logical, this approach had its limitations. Clustering, especially KMeans, doesn't inherently consider pre-existing labels. It simply groups data based on feature similarity. Moreover, the true complexity of driving behavior might not be accurately captured in just three categories. This realization led us to question our initial assumption: Were three clusters really sufficient to capture the diverse nuances of driving behavior?</span>
<span id="cb15-116"><a href="#cb15-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-117"><a href="#cb15-117" aria-hidden="true" tabindex="-1"></a>In the following section, we explore how we addressed these limitations by seeking an optimal number of clusters beyond the confines of the initial labeled categories.</span>
<span id="cb15-118"><a href="#cb15-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-119"><a href="#cb15-119" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Need for Optimization</span></span>
<span id="cb15-120"><a href="#cb15-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-121"><a href="#cb15-121" aria-hidden="true" tabindex="-1"></a>After the initial clustering, a crucial question arose: were three clusters truly representative of the driving behaviors in our dataset? This led us to the concept of cluster optimization, a pivotal step in ensuring that our machine learning model accurately reflects the complexities of the data.</span>
<span id="cb15-122"><a href="#cb15-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-123"><a href="#cb15-123" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Essence of Cluster Optimization</span></span>
<span id="cb15-124"><a href="#cb15-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-125"><a href="#cb15-125" aria-hidden="true" tabindex="-1"></a>Cluster optimization revolves around finding the 'just right' number of clusters in a dataset. Too few clusters, and the model might oversimplify the data, missing out on important nuances. Too many, and it might overfit, capturing random noise as meaningful patterns. The optimal number of clusters strikes a balance, grouping the data in a way that maximizes both similarity within clusters and differences between them.</span>
<span id="cb15-126"><a href="#cb15-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-127"><a href="#cb15-127" aria-hidden="true" tabindex="-1"></a><span class="fu">### Questioning the Three-Cluster Model</span></span>
<span id="cb15-128"><a href="#cb15-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-129"><a href="#cb15-129" aria-hidden="true" tabindex="-1"></a>Our initial model with three clusters was a natural starting point, mirroring the three labeled categories in the dataset. However, this approach had its pitfalls:</span>
<span id="cb15-130"><a href="#cb15-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-131"><a href="#cb15-131" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Assuming Label Completeness**: The labeled categories in the dataset might not encompass all the distinct driving behaviors present. Real-world driving is complex and might not fit neatly into just three categories.</span>
<span id="cb15-132"><a href="#cb15-132" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Ignoring Unsupervised Learning Nature**: Clustering, especially KMeans, is an unsupervised technique. It groups data based on feature similarity, independent of any pre-existing labels. Our model needed to reflect the data's structure, not just its labels.</span>
<span id="cb15-133"><a href="#cb15-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-134"><a href="#cb15-134" aria-hidden="true" tabindex="-1"></a>These considerations led us to explore beyond the confines of the three predefined categories, seeking a more data-driven approach to determine the number of clusters.</span>
<span id="cb15-135"><a href="#cb15-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-136"><a href="#cb15-136" aria-hidden="true" tabindex="-1"></a><span class="fu">## Implementing the Elbow Method</span></span>
<span id="cb15-137"><a href="#cb15-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-138"><a href="#cb15-138" aria-hidden="true" tabindex="-1"></a>With the realization that our initial model might not optimally capture the driving behaviors in our dataset, we turned to the Elbow Method. This technique is a cornerstone in determining the appropriate number of clusters in unsupervised learning, particularly in KMeans clustering.</span>
<span id="cb15-139"><a href="#cb15-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-140"><a href="#cb15-140" aria-hidden="true" tabindex="-1"></a><span class="fu">### Understanding the Elbow Method</span></span>
<span id="cb15-141"><a href="#cb15-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-142"><a href="#cb15-142" aria-hidden="true" tabindex="-1"></a>The Elbow Method is a heuristic used in cluster analysis to determine the number of clusters in a dataset. The method involves running the clustering algorithm multiple times, each time with a different number of clusters ($ k $), and calculating the Sum of Squared Errors (SSE) for each. SSE is defined as the sum of the squared distances between each member of a cluster and its centroid. As $ k $ increases, SSE tends to decrease as the clusters will be smaller and tighter. However, after a certain point, the marginal decrease in SSE with increasing $ k $ becomes insignificant, forming an 'elbow' in the plot. This point is considered the optimal number of clusters.</span>
<span id="cb15-143"><a href="#cb15-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-144"><a href="#cb15-144" aria-hidden="true" tabindex="-1"></a><span class="fu">### Applying the Elbow Method</span></span>
<span id="cb15-145"><a href="#cb15-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-146"><a href="#cb15-146" aria-hidden="true" tabindex="-1"></a>To apply the Elbow Method to our dataset, we first needed to calculate the SSE for a range of cluster numbers. Here's how we did it:</span>
<span id="cb15-147"><a href="#cb15-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-150"><a href="#cb15-150" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-151"><a href="#cb15-151" aria-hidden="true" tabindex="-1"></a>sse <span class="op">=</span> []</span>
<span id="cb15-152"><a href="#cb15-152" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>):</span>
<span id="cb15-153"><a href="#cb15-153" aria-hidden="true" tabindex="-1"></a>    kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb15-154"><a href="#cb15-154" aria-hidden="true" tabindex="-1"></a>    kmeans.fit(scaled_features)</span>
<span id="cb15-155"><a href="#cb15-155" aria-hidden="true" tabindex="-1"></a>    sse.append(kmeans.inertia_)</span>
<span id="cb15-156"><a href="#cb15-156" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-157"><a href="#cb15-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-158"><a href="#cb15-158" aria-hidden="true" tabindex="-1"></a>In this code block, we looped over a range of cluster numbers (from 1 to 10) and for each number, we applied the KMeans algorithm to our scaled features. The inertia_ attribute of KMeans gives us the SSE for that particular cluster count. We stored these SSE values in a list.</span>
<span id="cb15-159"><a href="#cb15-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-160"><a href="#cb15-160" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visualizing SSE vs. Number of Clusters</span></span>
<span id="cb15-161"><a href="#cb15-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-162"><a href="#cb15-162" aria-hidden="true" tabindex="-1"></a>To find the elbow point, we plotted the SSE against the number of clusters:</span>
<span id="cb15-163"><a href="#cb15-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-166"><a href="#cb15-166" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-167"><a href="#cb15-167" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>), sse, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb15-168"><a href="#cb15-168" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Elbow Method'</span>)</span>
<span id="cb15-169"><a href="#cb15-169" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Clusters'</span>)</span>
<span id="cb15-170"><a href="#cb15-170" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Sum of Squared Errors (SSE)'</span>)</span>
<span id="cb15-171"><a href="#cb15-171" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb15-172"><a href="#cb15-172" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-173"><a href="#cb15-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-174"><a href="#cb15-174" aria-hidden="true" tabindex="-1"></a>This visualization was crucial as it allowed us to observe the point at which the decrease in SSE becomes less sharp, indicating the optimal number of clusters. In the next section, we'll discuss how we used the <span class="in">`kneed`</span> package to programmatically identify this elbow point, further refining our clustering approach.</span>
<span id="cb15-175"><a href="#cb15-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-176"><a href="#cb15-176" aria-hidden="true" tabindex="-1"></a><span class="fu">## Programmatic Elbow Point Detection with `kneed`</span></span>
<span id="cb15-177"><a href="#cb15-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-178"><a href="#cb15-178" aria-hidden="true" tabindex="-1"></a>Having visualized the Sum of Squared Errors (SSE) against various cluster counts using the Elbow Method, our next step was to pinpoint the elusive 'elbow point' programmatically. This is where the <span class="in">`kneed`</span> Python package comes into play, offering a sophisticated yet straightforward approach to identifying the optimal number of clusters in our dataset.</span>
<span id="cb15-179"><a href="#cb15-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-180"><a href="#cb15-180" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Role of `kneed` in Cluster Analysis</span></span>
<span id="cb15-181"><a href="#cb15-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-182"><a href="#cb15-182" aria-hidden="true" tabindex="-1"></a><span class="in">`kneed`</span> is a Python library specifically designed to identify the knee point or elbow point in a dataset, which is often subjective and hard to pinpoint manually. It works by fitting a piecewise linear function to the data points and identifying the point of maximum curvature, which is the knee or elbow. This is particularly useful in KMeans clustering, as it takes the guesswork out of selecting the number of clusters.</span>
<span id="cb15-183"><a href="#cb15-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-184"><a href="#cb15-184" aria-hidden="true" tabindex="-1"></a><span class="fu">### Implementing `kneed` to Find the Optimal Clusters</span></span>
<span id="cb15-185"><a href="#cb15-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-186"><a href="#cb15-186" aria-hidden="true" tabindex="-1"></a>To utilize <span class="in">`kneed`</span> in our analysis, we first calculated the SSE for different cluster counts, just as we did for the Elbow Method. Then, <span class="in">`kneed`</span> took over to programmatically identify the elbow point:</span>
<span id="cb15-187"><a href="#cb15-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-190"><a href="#cb15-190" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-191"><a href="#cb15-191" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> kneed <span class="im">import</span> KneeLocator</span>
<span id="cb15-192"><a href="#cb15-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-193"><a href="#cb15-193" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculating SSE for a range of cluster counts</span></span>
<span id="cb15-194"><a href="#cb15-194" aria-hidden="true" tabindex="-1"></a>sse <span class="op">=</span> []</span>
<span id="cb15-195"><a href="#cb15-195" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>):</span>
<span id="cb15-196"><a href="#cb15-196" aria-hidden="true" tabindex="-1"></a>    kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb15-197"><a href="#cb15-197" aria-hidden="true" tabindex="-1"></a>    kmeans.fit(scaled_features)</span>
<span id="cb15-198"><a href="#cb15-198" aria-hidden="true" tabindex="-1"></a>    sse.append(kmeans.inertia_)</span>
<span id="cb15-199"><a href="#cb15-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-200"><a href="#cb15-200" aria-hidden="true" tabindex="-1"></a><span class="co"># Using kneed to find the elbow point</span></span>
<span id="cb15-201"><a href="#cb15-201" aria-hidden="true" tabindex="-1"></a>knee_locator <span class="op">=</span> KneeLocator(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>), sse, curve<span class="op">=</span><span class="st">'convex'</span>, direction<span class="op">=</span><span class="st">'decreasing'</span>)</span>
<span id="cb15-202"><a href="#cb15-202" aria-hidden="true" tabindex="-1"></a>elbow_point <span class="op">=</span> knee_locator.elbow</span>
<span id="cb15-203"><a href="#cb15-203" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-204"><a href="#cb15-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-205"><a href="#cb15-205" aria-hidden="true" tabindex="-1"></a>In this code snippet, we first computed the SSE for cluster counts from 1 to 10. We then instantiated the <span class="in">`KneeLocator`</span> class from the <span class="in">`kneed`</span> package, passing the range of cluster counts and the corresponding SSE values. The <span class="in">`curve='convex'`</span> and <span class="in">`direction='decreasing'`</span> parameters helped <span class="in">`kneed`</span> understand the nature of our SSE plot. The <span class="in">`elbow`</span> attribute of the <span class="in">`KneeLocator`</span> object gave us the optimal cluster count.</span>
<span id="cb15-206"><a href="#cb15-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-207"><a href="#cb15-207" aria-hidden="true" tabindex="-1"></a><span class="fu">### Determining the Optimal Number of Clusters</span></span>
<span id="cb15-208"><a href="#cb15-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-209"><a href="#cb15-209" aria-hidden="true" tabindex="-1"></a>To our surprise, <span class="in">`kneed`</span> identified 4 as the optimal number of clusters, deviating from our initial assumption of 3 clusters based on the dataset's labels. This revelation was pivotal as it highlighted the importance of relying on data-driven methods rather than assumptions or external labeling.</span>
<span id="cb15-210"><a href="#cb15-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-213"><a href="#cb15-213" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-214"><a href="#cb15-214" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The optimal number of clusters identified by kneed: </span><span class="sc">{</span>elbow_point<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-215"><a href="#cb15-215" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-216"><a href="#cb15-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-217"><a href="#cb15-217" aria-hidden="true" tabindex="-1"></a>This insight led us to reapply our clustering model with four clusters, a decision driven entirely by the underlying data structure rather than pre-defined categories. In the next section, we will revisit our clustering approach with this newfound understanding and analyze how this adjustment impacts our analysis of driving behaviors.</span>
<span id="cb15-218"><a href="#cb15-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-219"><a href="#cb15-219" aria-hidden="true" tabindex="-1"></a><span class="fu">## Re-Clustering with Optimized Cluster Count</span></span>
<span id="cb15-220"><a href="#cb15-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-221"><a href="#cb15-221" aria-hidden="true" tabindex="-1"></a>Armed with the knowledge that four clusters might better represent our driving behavior data, we embarked on a re-clustering journey. This step was crucial for our analysis as it aligned our clustering approach more closely with the inherent structure of the data, moving beyond our initial assumptions.</span>
<span id="cb15-222"><a href="#cb15-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-223"><a href="#cb15-223" aria-hidden="true" tabindex="-1"></a><span class="fu">### Reapplying KMeans with Four Clusters</span></span>
<span id="cb15-224"><a href="#cb15-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-225"><a href="#cb15-225" aria-hidden="true" tabindex="-1"></a>Guided by the <span class="in">`kneed`</span> package's recommendation, we reconfigured our KMeans clustering model to partition the data into four clusters instead of three. Here's how we proceeded:</span>
<span id="cb15-226"><a href="#cb15-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-229"><a href="#cb15-229" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-230"><a href="#cb15-230" aria-hidden="true" tabindex="-1"></a><span class="co"># Applying KMeans with the optimized number of clusters</span></span>
<span id="cb15-231"><a href="#cb15-231" aria-hidden="true" tabindex="-1"></a>optimized_kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb15-232"><a href="#cb15-232" aria-hidden="true" tabindex="-1"></a>optimized_clusters <span class="op">=</span> optimized_kmeans.fit_predict(scaled_features)</span>
<span id="cb15-233"><a href="#cb15-233" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-234"><a href="#cb15-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-235"><a href="#cb15-235" aria-hidden="true" tabindex="-1"></a>In this code, the <span class="in">`KMeans`</span> class from scikit-learn was re-initialized with <span class="in">`n_clusters`</span> set to 4. We then fit the model to our scaled features and predicted the cluster for each data point.</span>
<span id="cb15-236"><a href="#cb15-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-237"><a href="#cb15-237" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visualizing the New Clusters</span></span>
<span id="cb15-238"><a href="#cb15-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-239"><a href="#cb15-239" aria-hidden="true" tabindex="-1"></a>Visualization plays a key role in understanding the implications of our clustering:</span>
<span id="cb15-240"><a href="#cb15-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-243"><a href="#cb15-243" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-244"><a href="#cb15-244" aria-hidden="true" tabindex="-1"></a>plt.scatter(scaled_features[:,<span class="dv">0</span>], scaled_features[:,<span class="dv">1</span>], c<span class="op">=</span>optimized_clusters, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb15-245"><a href="#cb15-245" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Scaled AccX'</span>)</span>
<span id="cb15-246"><a href="#cb15-246" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Scaled AccY'</span>)</span>
<span id="cb15-247"><a href="#cb15-247" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Clustering with 4 Clusters'</span>)</span>
<span id="cb15-248"><a href="#cb15-248" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb15-249"><a href="#cb15-249" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-250"><a href="#cb15-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-251"><a href="#cb15-251" aria-hidden="true" tabindex="-1"></a>In this plot, we observed how the data points were grouped into four distinct clusters, providing a fresh perspective on the driving behaviors represented in our dataset.</span>
<span id="cb15-252"><a href="#cb15-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-253"><a href="#cb15-253" aria-hidden="true" tabindex="-1"></a><span class="fu">### Interpreting the New Clustering Results</span></span>
<span id="cb15-254"><a href="#cb15-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-255"><a href="#cb15-255" aria-hidden="true" tabindex="-1"></a>With four clusters, the model had the flexibility to identify a more nuanced pattern within the data. This could mean uncovering a behavior that was previously merged with others in the three-cluster model or providing a clearer separation between behaviors.</span>
<span id="cb15-256"><a href="#cb15-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-257"><a href="#cb15-257" aria-hidden="true" tabindex="-1"></a><span class="fu">### Comparing the Four-Cluster Model with the Initial Three-Cluster Model</span></span>
<span id="cb15-258"><a href="#cb15-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-259"><a href="#cb15-259" aria-hidden="true" tabindex="-1"></a>The transition from three to four clusters was not just a numerical change but also a conceptual shift. It underscored the significance of letting the data guide our analysis:</span>
<span id="cb15-260"><a href="#cb15-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-261"><a href="#cb15-261" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Increased Granularity**: The four-cluster model offered a finer categorization of driving behaviors, potentially leading to more accurate insights.</span>
<span id="cb15-262"><a href="#cb15-262" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Data-Driven Approach**: This shift highlighted the importance of relying on data-driven techniques, such as the Elbow Method and <span class="in">`kneed`</span>, in machine learning tasks.</span>
<span id="cb15-263"><a href="#cb15-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-264"><a href="#cb15-264" aria-hidden="true" tabindex="-1"></a>The decision to increase the number of clusters was a pivotal moment in our project. It exemplified the dynamic nature of machine learning, where initial hypotheses are continuously tested and refined in light of new evidence.</span>
<span id="cb15-265"><a href="#cb15-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-266"><a href="#cb15-266" aria-hidden="true" tabindex="-1"></a>In the final section, we will summarize our findings, reflect on the journey, and discuss the potential applications and implications of our analysis in real-world scenarios.</span>
<span id="cb15-267"><a href="#cb15-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-268"><a href="#cb15-268" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusions and Insights</span></span>
<span id="cb15-269"><a href="#cb15-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-270"><a href="#cb15-270" aria-hidden="true" tabindex="-1"></a>Our journey through clustering revealed several important insights:</span>
<span id="cb15-271"><a href="#cb15-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-272"><a href="#cb15-272" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Beyond Assumptions**: While assumptions and labels can provide direction, they should not be the sole basis for decision-making in clustering problems.</span>
<span id="cb15-273"><a href="#cb15-273" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Embracing Flexibility in Analysis**: Machine learning, especially unsupervised learning like clustering, requires an openness to adapt analysis strategies based on what the data reveals.</span>
<span id="cb15-274"><a href="#cb15-274" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Iterative Process**: Optimization in machine learning is an iterative process. It often involves revisiting and refining models as new data or techniques become available.</span>
<span id="cb15-275"><a href="#cb15-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-276"><a href="#cb15-276" aria-hidden="true" tabindex="-1"></a>Our journey through clustering driving behaviors illustrates the dynamic and iterative nature of machine learning. By embracing a data-driven approach, we were able to uncover more about the dataset than what was initially apparent. Such methodologies are not just limited to driving behavior analysis but are applicable across various domains where data-driven insights are crucial.</span>
<span id="cb15-277"><a href="#cb15-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-278"><a href="#cb15-278" aria-hidden="true" tabindex="-1"></a>As we continue to advance in the field of machine learning, the lessons learned here about the importance of flexibility, iteration, and data-driven analysis will remain fundamental to unlocking the full potential of our data.</span>
<span id="cb15-279"><a href="#cb15-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-280"><a href="#cb15-280" aria-hidden="true" tabindex="-1"></a><span class="fu">## References </span></span>
<span id="cb15-281"><a href="#cb15-281" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>https://developers.google.com/machine-learning/clustering/overview</span>
<span id="cb15-282"><a href="#cb15-282" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>https://www.geeksforgeeks.org/clustering-in-machine-learning/</span>
<span id="cb15-283"><a href="#cb15-283" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>https://machinelearningmastery.com/clustering-algorithms-with-python/</span>
<span id="cb15-284"><a href="#cb15-284" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>https://realpython.com/k-means-clustering-python/</span>
<span id="cb15-285"><a href="#cb15-285" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>https://www.youtube.com/watch?v=ht7geyMAFfA</span>
<span id="cb15-286"><a href="#cb15-286" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>OpenAI. (2023). ChatGPT <span class="co">[</span><span class="ot">Large language model</span><span class="co">]</span>. https://chat.openai.com</span>
<span id="cb15-287"><a href="#cb15-287" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>https://www.kaggle.com/datasets/outofskills/driving-behavior</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>