<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Joanna Fang">
<meta name="dcterms.date" content="2024-05-08">

<title>Ziming’s Journey in Machine Learning - Clustering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Ziming’s Journey in Machine Learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/joannafg/cs5805" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/joanna-fang-6122ba182/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Clustering</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i></button></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">ml</div>
                <div class="quarto-category">code</div>
                <div class="quarto-category">clustering</div>
                <div class="quarto-category">driving</div>
                <div class="quarto-category">kaggle</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Joanna Fang </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 8, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#clustering-optimizing-driver-behavior-analysis-with-machine-learning-clustering" id="toc-clustering-optimizing-driver-behavior-analysis-with-machine-learning-clustering" class="nav-link active" data-scroll-target="#clustering-optimizing-driver-behavior-analysis-with-machine-learning-clustering">Clustering: Optimizing Driver Behavior Analysis with Machine Learning Clustering</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#setting-the-stage-with-data-and-tools" id="toc-setting-the-stage-with-data-and-tools" class="nav-link" data-scroll-target="#setting-the-stage-with-data-and-tools">Setting the Stage with Data and Tools</a>
  <ul class="collapse">
  <li><a href="#the-kaggle-driving-behavior-dataset" id="toc-the-kaggle-driving-behavior-dataset" class="nav-link" data-scroll-target="#the-kaggle-driving-behavior-dataset">The Kaggle ‘Driving Behavior’ Dataset</a></li>
  <li><a href="#tools-and-libraries" id="toc-tools-and-libraries" class="nav-link" data-scroll-target="#tools-and-libraries">Tools and Libraries</a></li>
  </ul></li>
  <li><a href="#initial-clustering-approach" id="toc-initial-clustering-approach" class="nav-link" data-scroll-target="#initial-clustering-approach">Initial Clustering Approach</a>
  <ul class="collapse">
  <li><a href="#why-start-with-three-clusters" id="toc-why-start-with-three-clusters" class="nav-link" data-scroll-target="#why-start-with-three-clusters">Why Start with Three Clusters?</a></li>
  <li><a href="#applying-kmeans-clustering" id="toc-applying-kmeans-clustering" class="nav-link" data-scroll-target="#applying-kmeans-clustering">Applying KMeans Clustering</a></li>
  <li><a href="#visualizing-the-initial-results" id="toc-visualizing-the-initial-results" class="nav-link" data-scroll-target="#visualizing-the-initial-results">Visualizing the Initial Results</a></li>
  <li><a href="#limitations-of-the-initial-approach" id="toc-limitations-of-the-initial-approach" class="nav-link" data-scroll-target="#limitations-of-the-initial-approach">Limitations of the Initial Approach</a></li>
  </ul></li>
  <li><a href="#realizing-the-need-for-optimization" id="toc-realizing-the-need-for-optimization" class="nav-link" data-scroll-target="#realizing-the-need-for-optimization">Realizing the Need for Optimization</a>
  <ul class="collapse">
  <li><a href="#the-essence-of-cluster-optimization" id="toc-the-essence-of-cluster-optimization" class="nav-link" data-scroll-target="#the-essence-of-cluster-optimization">The Essence of Cluster Optimization</a></li>
  <li><a href="#questioning-the-three-cluster-model" id="toc-questioning-the-three-cluster-model" class="nav-link" data-scroll-target="#questioning-the-three-cluster-model">Questioning the Three-Cluster Model</a></li>
  <li><a href="#embracing-the-elbow-method" id="toc-embracing-the-elbow-method" class="nav-link" data-scroll-target="#embracing-the-elbow-method">Embracing the Elbow Method</a></li>
  </ul></li>
  <li><a href="#implementing-the-elbow-method" id="toc-implementing-the-elbow-method" class="nav-link" data-scroll-target="#implementing-the-elbow-method">Implementing the Elbow Method</a>
  <ul class="collapse">
  <li><a href="#understanding-the-elbow-method" id="toc-understanding-the-elbow-method" class="nav-link" data-scroll-target="#understanding-the-elbow-method">Understanding the Elbow Method</a></li>
  <li><a href="#applying-the-elbow-method" id="toc-applying-the-elbow-method" class="nav-link" data-scroll-target="#applying-the-elbow-method">Applying the Elbow Method</a></li>
  <li><a href="#visualizing-sse-vs.-number-of-clusters" id="toc-visualizing-sse-vs.-number-of-clusters" class="nav-link" data-scroll-target="#visualizing-sse-vs.-number-of-clusters">Visualizing SSE vs.&nbsp;Number of Clusters</a></li>
  </ul></li>
  <li><a href="#programmatic-elbow-point-detection-with-kneed" id="toc-programmatic-elbow-point-detection-with-kneed" class="nav-link" data-scroll-target="#programmatic-elbow-point-detection-with-kneed">Programmatic Elbow Point Detection with <code>kneed</code></a>
  <ul class="collapse">
  <li><a href="#the-role-of-kneed-in-cluster-analysis" id="toc-the-role-of-kneed-in-cluster-analysis" class="nav-link" data-scroll-target="#the-role-of-kneed-in-cluster-analysis">The Role of <code>kneed</code> in Cluster Analysis</a></li>
  <li><a href="#implementing-kneed-to-find-the-optimal-clusters" id="toc-implementing-kneed-to-find-the-optimal-clusters" class="nav-link" data-scroll-target="#implementing-kneed-to-find-the-optimal-clusters">Implementing <code>kneed</code> to Find the Optimal Clusters</a></li>
  <li><a href="#determining-the-optimal-number-of-clusters" id="toc-determining-the-optimal-number-of-clusters" class="nav-link" data-scroll-target="#determining-the-optimal-number-of-clusters">Determining the Optimal Number of Clusters</a></li>
  </ul></li>
  <li><a href="#re-clustering-with-optimized-cluster-count" id="toc-re-clustering-with-optimized-cluster-count" class="nav-link" data-scroll-target="#re-clustering-with-optimized-cluster-count">Re-Clustering with Optimized Cluster Count</a>
  <ul class="collapse">
  <li><a href="#reapplying-kmeans-with-four-clusters" id="toc-reapplying-kmeans-with-four-clusters" class="nav-link" data-scroll-target="#reapplying-kmeans-with-four-clusters">Reapplying KMeans with Four Clusters</a></li>
  <li><a href="#visualizing-the-new-clusters" id="toc-visualizing-the-new-clusters" class="nav-link" data-scroll-target="#visualizing-the-new-clusters">Visualizing the New Clusters</a></li>
  <li><a href="#interpreting-the-new-clustering-results" id="toc-interpreting-the-new-clustering-results" class="nav-link" data-scroll-target="#interpreting-the-new-clustering-results">Interpreting the New Clustering Results</a></li>
  <li><a href="#comparing-the-four-cluster-model-with-the-initial-three-cluster-model" id="toc-comparing-the-four-cluster-model-with-the-initial-three-cluster-model" class="nav-link" data-scroll-target="#comparing-the-four-cluster-model-with-the-initial-three-cluster-model">Comparing the Four-Cluster Model with the Initial Three-Cluster Model</a></li>
  </ul></li>
  <li><a href="#conclusions-and-insights" id="toc-conclusions-and-insights" class="nav-link" data-scroll-target="#conclusions-and-insights">Conclusions and Insights</a>
  <ul class="collapse">
  <li><a href="#key-findings-from-the-optimized-clustering" id="toc-key-findings-from-the-optimized-clustering" class="nav-link" data-scroll-target="#key-findings-from-the-optimized-clustering">Key Findings from the Optimized Clustering</a></li>
  <li><a href="#practical-implications-in-real-world-scenarios" id="toc-practical-implications-in-real-world-scenarios" class="nav-link" data-scroll-target="#practical-implications-in-real-world-scenarios">Practical Implications in Real-World Scenarios</a></li>
  <li><a href="#the-importance-of-data-driven-decision-making" id="toc-the-importance-of-data-driven-decision-making" class="nav-link" data-scroll-target="#the-importance-of-data-driven-decision-making">The Importance of Data-Driven Decision-Making</a></li>
  <li><a href="#final-thoughts" id="toc-final-thoughts" class="nav-link" data-scroll-target="#final-thoughts">Final Thoughts</a></li>
  </ul></li>
  <li><a href="#section-8-future-directions" id="toc-section-8-future-directions" class="nav-link" data-scroll-target="#section-8-future-directions">Section 8: Future Directions</a>
  <ul class="collapse">
  <li><a href="#expanding-the-research-horizon" id="toc-expanding-the-research-horizon" class="nav-link" data-scroll-target="#expanding-the-research-horizon">Expanding the Research Horizon</a></li>
  <li><a href="#leveraging-other-machine-learning-models" id="toc-leveraging-other-machine-learning-models" class="nav-link" data-scroll-target="#leveraging-other-machine-learning-models">Leveraging Other Machine Learning Models</a></li>
  <li><a href="#the-evolving-role-of-machine-learning-in-driving-behavior-analysis" id="toc-the-evolving-role-of-machine-learning-in-driving-behavior-analysis" class="nav-link" data-scroll-target="#the-evolving-role-of-machine-learning-in-driving-behavior-analysis">The Evolving Role of Machine Learning in Driving Behavior Analysis</a></li>
  <li><a href="#concluding-thoughts" id="toc-concluding-thoughts" class="nav-link" data-scroll-target="#concluding-thoughts">Concluding Thoughts</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/joannafg/cs5805/edit/main/posts/clustering/index.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/joannafg/cs5805/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="clustering-optimizing-driver-behavior-analysis-with-machine-learning-clustering" class="level1">
<h1>Clustering: Optimizing Driver Behavior Analysis with Machine Learning Clustering</h1>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="thumbnail.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>The analysis of driving behaviors, a task of both complexity and nuance, is increasingly significant in the realms of traffic safety and autonomous vehicle development. Machine learning, particularly clustering algorithms, stands at the forefront of this analysis, offering the ability to dissect and understand the myriad patterns hidden within driving data. Clustering, a form of unsupervised learning, excels in finding structure within unlabeled data, grouping similar data points together based on shared characteristics.</p>
<p>In this blog, we dive into a practical application of clustering using a dataset from Kaggle titled ‘Driving Behavior’. This dataset is intriguing because it comes with labeled data, categorizing driving behaviors into three types: SLOW, NORMAL, and AGGRESSIVE. While clustering typically operates on unlabeled data, we use this labeled dataset to our advantage. It will allow us to compare the clusters formed by our machine learning algorithm against the pre-labeled categories, providing an insightful backdrop to understand and validate our clustering results.</p>
<p>Our aim is to illustrate the entire process of applying clustering to analyze driving behavior. We start with a predefined notion of having three clusters based on the labels present in the dataset. However, as we delve deeper into the analysis, we realize the necessity to step back and question our initial assumptions. We employ the Elbow Method, supplemented by the <code>kneed</code> Python package, to determine the optimal number of clusters in a more data-driven manner. This journey from assumption to optimization is not just about applying a machine learning technique but also about understanding the nuances and intricacies that come with real-world data analysis.</p>
<p>Join us as we unravel the complexities of driving behaviors through clustering, starting with predefined labels and moving towards an optimized, data-driven approach.</p>
</section>
<section id="setting-the-stage-with-data-and-tools" class="level2">
<h2 class="anchored" data-anchor-id="setting-the-stage-with-data-and-tools">Setting the Stage with Data and Tools</h2>
<p>In the world of machine learning, the choice of dataset is as crucial as the algorithm itself. For our endeavor into clustering driving behaviors, we’ve chosen a dataset from Kaggle named ‘Driving Behavior’. This dataset is particularly interesting for a few reasons. Firstly, it’s labeled with specific driving behaviors, which, while not necessary for clustering, provides us with a unique opportunity to validate and understand our clustering results in a more tangible way.</p>
<section id="the-kaggle-driving-behavior-dataset" class="level3">
<h3 class="anchored" data-anchor-id="the-kaggle-driving-behavior-dataset">The Kaggle ‘Driving Behavior’ Dataset</h3>
<p>This dataset offers a detailed glimpse into various aspects of driving, captured through different features:</p>
<ul>
<li><strong>Acceleration (AccX, AccY, AccZ)</strong>: These features measure the vehicle’s acceleration in meters per second squared (<span class="math display">\[m/s^2\]</span>) along the X, Y, and Z axes. Acceleration data is crucial for understanding sudden movements and changes in driving speed.</li>
<li><strong>Rotation (GyroX, GyroY, GyroZ)</strong>: Here, we have the vehicle’s angular velocity around the X, Y, and Z axes, measured in degrees per second (<span class="math display">\[°/s\]</span>). This data helps in identifying steering patterns and gauging the stability of the vehicle during various maneuvers.</li>
<li><strong>Classification Labels (SLOW, NORMAL, AGGRESSIVE)</strong>: Each data point is tagged with one of these labels. It’s important to note that in clustering, such labels are not typically required. However, in our case, these labels will serve as a benchmark, enabling us to compare our unsupervised learning results with pre-defined categories.</li>
</ul>
<p>This dataset not only provides a foundation for applying clustering techniques but also allows us to explore how well unsupervised learning can mirror human-labeled classifications.</p>
</section>
<section id="tools-and-libraries" class="level3">
<h3 class="anchored" data-anchor-id="tools-and-libraries">Tools and Libraries</h3>
<p>Our analysis will be powered by several key tools and libraries, each bringing its own strengths to the table:</p>
<ul>
<li><strong>Python</strong>: Our language of choice, renowned for its ease of use and strong community support, especially in data science.</li>
<li><strong>Scikit-learn</strong>: A powerful Python library for machine learning. We will use its clustering algorithms, specifically KMeans, to group our data.</li>
<li><strong>Matplotlib</strong>: This library will help us in visualizing our data and the results of our clustering, making the insights more accessible.</li>
<li><strong>kneed</strong>: An invaluable tool when it comes to determining the optimal number of clusters. It programmatically identifies the ‘elbow point’ in our dataset, a crucial step in ensuring our clustering approach is as effective as possible.</li>
</ul>
<p>In the next sections, we’ll dive into the process of applying these tools to our dataset, starting with our initial approach and gradually moving towards a more refined, data-driven methodology.</p>
</section>
</section>
<section id="initial-clustering-approach" class="level2">
<h2 class="anchored" data-anchor-id="initial-clustering-approach">Initial Clustering Approach</h2>
<p>Embarking on our journey to unravel the driving patterns hidden in the dataset, we initially gravitated towards a straightforward approach: clustering the data into three groups. This decision was influenced by the dataset’s labels - SLOW, NORMAL, and AGGRESSIVE. It seemed logical to align our clusters with these pre-defined categories, under the assumption that they would naturally encapsulate the essence of the driving behaviors.</p>
<section id="why-start-with-three-clusters" class="level3">
<h3 class="anchored" data-anchor-id="why-start-with-three-clusters">Why Start with Three Clusters?</h3>
<p>The rationale was straightforward: the dataset labels suggested three distinct types of driving behaviors. Clustering aims to group similar data points, and with these labels as a guide, it seemed reasonable to start our analysis by segmenting the data into three clusters, hoping they would align with the labeled behaviors.</p>
</section>
<section id="applying-kmeans-clustering" class="level3">
<h3 class="anchored" data-anchor-id="applying-kmeans-clustering">Applying KMeans Clustering</h3>
<p>The KMeans algorithm was chosen for its simplicity and effectiveness in many clustering scenarios. Here’s a breakdown of how we applied it:</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install numpy</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install matplotlib</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install scipy</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install scikit<span class="op">-</span>learn</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install pandas</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install kneed</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the datasets</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>data_1 <span class="op">=</span> pd.read_csv(<span class="st">'motion_data_1.csv'</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>data_2 <span class="op">=</span> pd.read_csv(<span class="st">'motion_data_2.csv'</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine datasets</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.concat([data_1, data_2])</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Data preprocessing</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> data[[<span class="st">'AccX'</span>, <span class="st">'AccY'</span>, <span class="st">'AccZ'</span>, <span class="st">'GyroX'</span>, <span class="st">'GyroY'</span>, <span class="st">'GyroZ'</span>]]</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>scaled_features <span class="op">=</span> scaler.fit_transform(features)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Applying KMeans with 3 clusters</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>clusters <span class="op">=</span> kmeans.fit_predict(scaled_features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: numpy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.26.2)
WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.
You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: matplotlib in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (3.8.2)
Requirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)
Requirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.1.1)
Requirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.0)
Requirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.1)
Requirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (23.2)
Requirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)
Requirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.45.1)
Requirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)
Requirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.1.0)
Requirement already satisfied: numpy&lt;2,&gt;=1.21 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.2)
Requirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib) (3.17.0)
Requirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.15.0)
WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.
You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: scipy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.11.4)
Requirement already satisfied: numpy&lt;1.28.0,&gt;=1.21.6 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scipy) (1.26.2)
WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.
You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: scikit-learn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.3.2)
Requirement already satisfied: scipy&gt;=1.5.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.11.4)
Requirement already satisfied: numpy&lt;2.0,&gt;=1.17.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.2)
Requirement already satisfied: joblib&gt;=1.1.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.3.2)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.2.0)
WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.
You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: pandas in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (2.1.3)
Requirement already satisfied: numpy&lt;2,&gt;=1.22.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)
Requirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)
Requirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)
Requirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)
Requirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.15.0)
WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.
You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: kneed in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.8.5)
Requirement already satisfied: scipy&gt;=1.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from kneed) (1.11.4)
Requirement already satisfied: numpy&gt;=1.14.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from kneed) (1.26.2)
WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.
You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)</code></pre>
</div>
</div>
<p>In this code snippet, we begin by loading the dataset and selecting our features of interest, namely the acceleration and rotation measurements. These features are then scaled using <code>StandardScaler</code>, a crucial step to ensure that all features contribute equally to the clustering process. The <code>KMeans</code> algorithm is then applied to the scaled data, specifying three clusters.</p>
</section>
<section id="visualizing-the-initial-results" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-the-initial-results">Visualizing the Initial Results</h3>
<p>To understand our initial clustering, we visualized the results:</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(scaled_features[:,<span class="dv">0</span>], scaled_features[:,<span class="dv">1</span>], c<span class="op">=</span>clusters, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Scaled AccX'</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Scaled AccY'</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Initial Clustering with 3 Clusters'</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-3-output-1.png" width="587" height="449"></p>
</div>
</div>
<p>This visualization plots the scaled acceleration data on the X and Y axes, colored by the assigned cluster. It provides a first glimpse into how the data points are grouped by the algorithm.</p>
</section>
<section id="limitations-of-the-initial-approach" class="level3">
<h3 class="anchored" data-anchor-id="limitations-of-the-initial-approach">Limitations of the Initial Approach</h3>
<p>While starting with three clusters seemed logical, this approach had its limitations. Clustering, especially KMeans, doesn’t inherently consider pre-existing labels. It simply groups data based on feature similarity. Moreover, the true complexity of driving behavior might not be accurately captured in just three categories. This realization led us to question our initial assumption: Were three clusters really sufficient to capture the diverse nuances of driving behavior?</p>
<p>In the following section, we explore how we addressed these limitations by seeking an optimal number of clusters beyond the confines of the initial labeled categories.</p>
</section>
</section>
<section id="realizing-the-need-for-optimization" class="level2">
<h2 class="anchored" data-anchor-id="realizing-the-need-for-optimization">Realizing the Need for Optimization</h2>
<p>After the initial clustering, a crucial question arose: were three clusters truly representative of the driving behaviors in our dataset? This led us to the concept of cluster optimization, a pivotal step in ensuring that our machine learning model accurately reflects the complexities of the data.</p>
<section id="the-essence-of-cluster-optimization" class="level3">
<h3 class="anchored" data-anchor-id="the-essence-of-cluster-optimization">The Essence of Cluster Optimization</h3>
<p>Cluster optimization revolves around finding the ‘just right’ number of clusters in a dataset. Too few clusters, and the model might oversimplify the data, missing out on important nuances. Too many, and it might overfit, capturing random noise as meaningful patterns. The optimal number of clusters strikes a balance, grouping the data in a way that maximizes both similarity within clusters and differences between them.</p>
</section>
<section id="questioning-the-three-cluster-model" class="level3">
<h3 class="anchored" data-anchor-id="questioning-the-three-cluster-model">Questioning the Three-Cluster Model</h3>
<p>Our initial model with three clusters was a natural starting point, mirroring the three labeled categories in the dataset. However, this approach had its pitfalls:</p>
<ol type="1">
<li><strong>Assuming Label Completeness</strong>: The labeled categories in the dataset might not encompass all the distinct driving behaviors present. Real-world driving is complex and might not fit neatly into just three categories.</li>
<li><strong>Ignoring Unsupervised Learning Nature</strong>: Clustering, especially KMeans, is an unsupervised technique. It groups data based on feature similarity, independent of any pre-existing labels. Our model needed to reflect the data’s structure, not just its labels.</li>
</ol>
<p>These considerations led us to explore beyond the confines of the three predefined categories, seeking a more data-driven approach to determine the number of clusters.</p>
</section>
<section id="embracing-the-elbow-method" class="level3">
<h3 class="anchored" data-anchor-id="embracing-the-elbow-method">Embracing the Elbow Method</h3>
<p>The Elbow Method emerged as our tool of choice for finding the optimal number of clusters. It involves plotting the model’s performance (measured in terms of Sum of Squared Errors, or SSE) against a range of cluster numbers. The ‘elbow point’ in this plot, where the rate of decrease in SSE sharply changes, indicates the optimal cluster count.</p>
<p>In the next section, we’ll delve into how we applied the Elbow Method to our dataset, using a combination of scikit-learn and the <code>kneed</code> package to not just visualize but also programmatically identify the elbow point, marking a significant step in refining our clustering model.</p>
</section>
</section>
<section id="implementing-the-elbow-method" class="level2">
<h2 class="anchored" data-anchor-id="implementing-the-elbow-method">Implementing the Elbow Method</h2>
<p>With the realization that our initial model might not optimally capture the driving behaviors in our dataset, we turned to the Elbow Method. This technique is a cornerstone in determining the appropriate number of clusters in unsupervised learning, particularly in KMeans clustering.</p>
<section id="understanding-the-elbow-method" class="level3">
<h3 class="anchored" data-anchor-id="understanding-the-elbow-method">Understanding the Elbow Method</h3>
<p>The Elbow Method is a heuristic used in cluster analysis to determine the number of clusters in a dataset. The method involves running the clustering algorithm multiple times, each time with a different number of clusters (<span class="math display">\[ k \]</span>), and calculating the Sum of Squared Errors (SSE) for each. SSE is defined as the sum of the squared distances between each member of a cluster and its centroid. As <span class="math display">\[ k \]</span> increases, SSE tends to decrease as the clusters will be smaller and tighter. However, after a certain point, the marginal decrease in SSE with increasing <span class="math display">\[ k \]</span> becomes insignificant, forming an ‘elbow’ in the plot. This point is considered the optimal number of clusters.</p>
</section>
<section id="applying-the-elbow-method" class="level3">
<h3 class="anchored" data-anchor-id="applying-the-elbow-method">Applying the Elbow Method</h3>
<p>To apply the Elbow Method to our dataset, we first needed to calculate the SSE for a range of cluster numbers. Here’s how we did it:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>sse <span class="op">=</span> []</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    kmeans.fit(scaled_features)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    sse.append(kmeans.inertia_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)</code></pre>
</div>
</div>
<p>In this code block, we looped over a range of cluster numbers (from 1 to 10) and for each number, we applied the KMeans algorithm to our scaled features. The inertia_ attribute of KMeans gives us the SSE for that particular cluster count. We stored these SSE values in a list.</p>
</section>
<section id="visualizing-sse-vs.-number-of-clusters" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-sse-vs.-number-of-clusters">Visualizing SSE vs.&nbsp;Number of Clusters</h3>
<p>To find the elbow point, we plotted the SSE against the number of clusters:</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>), sse, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Elbow Method'</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Clusters'</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Sum of Squared Errors (SSE)'</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-5-output-1.png" width="610" height="449"></p>
</div>
</div>
<p>This visualization was crucial as it allowed us to observe the point at which the decrease in SSE becomes less sharp, indicating the optimal number of clusters. In the next section, we’ll discuss how we used the <code>kneed</code> package to programmatically identify this elbow point, further refining our clustering approach.</p>
</section>
</section>
<section id="programmatic-elbow-point-detection-with-kneed" class="level2">
<h2 class="anchored" data-anchor-id="programmatic-elbow-point-detection-with-kneed">Programmatic Elbow Point Detection with <code>kneed</code></h2>
<p>Having visualized the Sum of Squared Errors (SSE) against various cluster counts using the Elbow Method, our next step was to pinpoint the elusive ‘elbow point’ programmatically. This is where the <code>kneed</code> Python package comes into play, offering a sophisticated yet straightforward approach to identifying the optimal number of clusters in our dataset.</p>
<section id="the-role-of-kneed-in-cluster-analysis" class="level3">
<h3 class="anchored" data-anchor-id="the-role-of-kneed-in-cluster-analysis">The Role of <code>kneed</code> in Cluster Analysis</h3>
<p><code>kneed</code> is a Python library specifically designed to identify the knee point or elbow point in a dataset, which is often subjective and hard to pinpoint manually. It works by fitting a piecewise linear function to the data points and identifying the point of maximum curvature, which is the knee or elbow. This is particularly useful in KMeans clustering, as it takes the guesswork out of selecting the number of clusters.</p>
</section>
<section id="implementing-kneed-to-find-the-optimal-clusters" class="level3">
<h3 class="anchored" data-anchor-id="implementing-kneed-to-find-the-optimal-clusters">Implementing <code>kneed</code> to Find the Optimal Clusters</h3>
<p>To utilize <code>kneed</code> in our analysis, we first calculated the SSE for different cluster counts, just as we did for the Elbow Method. Then, <code>kneed</code> took over to programmatically identify the elbow point:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> kneed <span class="im">import</span> KneeLocator</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculating SSE for a range of cluster counts</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>sse <span class="op">=</span> []</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>):</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    kmeans.fit(scaled_features)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    sse.append(kmeans.inertia_)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Using kneed to find the elbow point</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>knee_locator <span class="op">=</span> KneeLocator(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>), sse, curve<span class="op">=</span><span class="st">'convex'</span>, direction<span class="op">=</span><span class="st">'decreasing'</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>elbow_point <span class="op">=</span> knee_locator.elbow</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)</code></pre>
</div>
</div>
<p>In this code snippet, we first computed the SSE for cluster counts from 1 to 10. We then instantiated the <code>KneeLocator</code> class from the <code>kneed</code> package, passing the range of cluster counts and the corresponding SSE values. The <code>curve='convex'</code> and <code>direction='decreasing'</code> parameters helped <code>kneed</code> understand the nature of our SSE plot. The <code>elbow</code> attribute of the <code>KneeLocator</code> object gave us the optimal cluster count.</p>
</section>
<section id="determining-the-optimal-number-of-clusters" class="level3">
<h3 class="anchored" data-anchor-id="determining-the-optimal-number-of-clusters">Determining the Optimal Number of Clusters</h3>
<p>To our surprise, <code>kneed</code> identified 4 as the optimal number of clusters, deviating from our initial assumption of 3 clusters based on the dataset’s labels. This revelation was pivotal as it highlighted the importance of relying on data-driven methods rather than assumptions or external labeling.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The optimal number of clusters identified by kneed: </span><span class="sc">{</span>elbow_point<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The optimal number of clusters identified by kneed: 4</code></pre>
</div>
</div>
<p>This insight led us to reapply our clustering model with four clusters, a decision driven entirely by the underlying data structure rather than pre-defined categories. In the next section, we will revisit our clustering approach with this newfound understanding and analyze how this adjustment impacts our analysis of driving behaviors.</p>
</section>
</section>
<section id="re-clustering-with-optimized-cluster-count" class="level2">
<h2 class="anchored" data-anchor-id="re-clustering-with-optimized-cluster-count">Re-Clustering with Optimized Cluster Count</h2>
<p>Armed with the knowledge that four clusters might better represent our driving behavior data, we embarked on a re-clustering journey. This step was crucial for our analysis as it aligned our clustering approach more closely with the inherent structure of the data, moving beyond our initial assumptions.</p>
<section id="reapplying-kmeans-with-four-clusters" class="level3">
<h3 class="anchored" data-anchor-id="reapplying-kmeans-with-four-clusters">Reapplying KMeans with Four Clusters</h3>
<p>Guided by the <code>kneed</code> package’s recommendation, we reconfigured our KMeans clustering model to partition the data into four clusters instead of three. Here’s how we proceeded:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Applying KMeans with the optimized number of clusters</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>optimized_kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>optimized_clusters <span class="op">=</span> optimized_kmeans.fit_predict(scaled_features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)</code></pre>
</div>
</div>
<p>In this code, the <code>KMeans</code> class from scikit-learn was re-initialized with <code>n_clusters</code> set to 4. We then fit the model to our scaled features and predicted the cluster for each data point.</p>
</section>
<section id="visualizing-the-new-clusters" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-the-new-clusters">Visualizing the New Clusters</h3>
<p>Visualization plays a key role in understanding the implications of our clustering:</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(scaled_features[:,<span class="dv">0</span>], scaled_features[:,<span class="dv">1</span>], c<span class="op">=</span>optimized_clusters, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Scaled AccX'</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Scaled AccY'</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Clustering with 4 Clusters'</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-9-output-1.png" width="587" height="449"></p>
</div>
</div>
<p>In this plot, we observed how the data points were grouped into four distinct clusters, providing a fresh perspective on the driving behaviors represented in our dataset.</p>
</section>
<section id="interpreting-the-new-clustering-results" class="level3">
<h3 class="anchored" data-anchor-id="interpreting-the-new-clustering-results">Interpreting the New Clustering Results</h3>
<p>With four clusters, the model had the flexibility to identify a more nuanced pattern within the data. This could mean uncovering a behavior that was previously merged with others in the three-cluster model or providing a clearer separation between behaviors.</p>
</section>
<section id="comparing-the-four-cluster-model-with-the-initial-three-cluster-model" class="level3">
<h3 class="anchored" data-anchor-id="comparing-the-four-cluster-model-with-the-initial-three-cluster-model">Comparing the Four-Cluster Model with the Initial Three-Cluster Model</h3>
<p>The transition from three to four clusters was not just a numerical change but also a conceptual shift. It underscored the significance of letting the data guide our analysis:</p>
<ul>
<li><strong>Increased Granularity</strong>: The four-cluster model offered a finer categorization of driving behaviors, potentially leading to more accurate insights.</li>
<li><strong>Data-Driven Approach</strong>: This shift highlighted the importance of relying on data-driven techniques, such as the Elbow Method and <code>kneed</code>, in machine learning tasks.</li>
</ul>
<p>The decision to increase the number of clusters was a pivotal moment in our project. It exemplified the dynamic nature of machine learning, where initial hypotheses are continuously tested and refined in light of new evidence.</p>
<p>In the final section, we will summarize our findings, reflect on the journey, and discuss the potential applications and implications of our analysis in real-world scenarios.</p>
</section>
</section>
<section id="conclusions-and-insights" class="level2">
<h2 class="anchored" data-anchor-id="conclusions-and-insights">Conclusions and Insights</h2>
<p>As we conclude our exploration into clustering driving behaviors using the Kaggle dataset, it’s crucial to reflect on the key insights gained and the broader implications of our findings.</p>
<section id="key-findings-from-the-optimized-clustering" class="level3">
<h3 class="anchored" data-anchor-id="key-findings-from-the-optimized-clustering">Key Findings from the Optimized Clustering</h3>
<p>Our journey through clustering revealed several important insights:</p>
<ol type="1">
<li><strong>Optimal Cluster Number</strong>: Moving from a 3-cluster model to a 4-cluster model, as suggested by the Elbow Method and validated by <code>kneed</code>, allowed us to uncover a more intricate structure within the driving behavior data.</li>
<li><strong>Granularity in Clustering</strong>: The additional cluster provided a finer categorization of driving behaviors, which could lead to more nuanced and accurate insights into different driving styles.</li>
<li><strong>Data-Driven Approach</strong>: This project highlighted the importance of a data-driven approach in clustering. While initial assumptions based on labeled data provided a starting point, it was the analysis of the inherent data structure that ultimately guided our clustering strategy.</li>
</ol>
</section>
<section id="practical-implications-in-real-world-scenarios" class="level3">
<h3 class="anchored" data-anchor-id="practical-implications-in-real-world-scenarios">Practical Implications in Real-World Scenarios</h3>
<p>The implications of our findings extend far beyond this analysis:</p>
<ul>
<li><strong>Enhanced Driver Safety Systems</strong>: By understanding nuanced driving behaviors, automotive manufacturers and researchers can develop more sophisticated driver safety systems that cater to different driving styles.</li>
<li><strong>Targeted Driver Training Programs</strong>: The insights from clustering can inform targeted training programs that address specific driving behavior patterns, thereby enhancing road safety.</li>
<li><strong>Insurance and Risk Assessment</strong>: The identification of different driving behaviors can be instrumental in risk assessment for car insurance companies, enabling them to tailor policies based on individual driving patterns.</li>
</ul>
</section>
<section id="the-importance-of-data-driven-decision-making" class="level3">
<h3 class="anchored" data-anchor-id="the-importance-of-data-driven-decision-making">The Importance of Data-Driven Decision-Making</h3>
<p>This project served as a reminder of the critical role of data-driven decision-making in machine learning:</p>
<ul>
<li><strong>Beyond Assumptions</strong>: While assumptions and labels can provide direction, they should not be the sole basis for decision-making in clustering problems.</li>
<li><strong>Embracing Flexibility in Analysis</strong>: Machine learning, especially unsupervised learning like clustering, requires an openness to adapt analysis strategies based on what the data reveals.</li>
<li><strong>Iterative Process</strong>: Optimization in machine learning is an iterative process. It often involves revisiting and refining models as new data or techniques become available.</li>
</ul>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>Our journey through clustering driving behaviors illustrates the dynamic and iterative nature of machine learning. By embracing a data-driven approach, we were able to uncover more about the dataset than what was initially apparent. Such methodologies are not just limited to driving behavior analysis but are applicable across various domains where data-driven insights are crucial.</p>
<p>As we continue to advance in the field of machine learning, the lessons learned here about the importance of flexibility, iteration, and data-driven analysis will remain fundamental to unlocking the full potential of our data.</p>
</section>
</section>
<section id="section-8-future-directions" class="level2">
<h2 class="anchored" data-anchor-id="section-8-future-directions">Section 8: Future Directions</h2>
<p>As we wrap up our exploration into clustering driving behaviors, it’s essential to look ahead and consider the avenues for further research and development in this field. The journey we’ve embarked on opens up a myriad of possibilities, each holding the potential to deepen our understanding of driving behaviors and enhance the applications of machine learning in this domain.</p>
<section id="expanding-the-research-horizon" class="level3">
<h3 class="anchored" data-anchor-id="expanding-the-research-horizon">Expanding the Research Horizon</h3>
<ol type="1">
<li><strong>Integrating More Complex Data</strong>: Future studies could integrate more complex data types, such as video feeds, GPS data, or real-time traffic information. This would allow for a more comprehensive analysis of driving behaviors, taking into account environmental and situational variables.</li>
<li><strong>Exploring Time-Series Analysis</strong>: Given that driving data is inherently time-sequential, applying time-series analysis could uncover patterns that static clustering might miss. Techniques like recurrent neural networks (RNNs) or Long Short-Term Memory (LSTM) networks could be particularly useful.</li>
<li><strong>Incorporating Weather and Road Conditions</strong>: Adding weather and road condition data could offer insights into how external factors influence driving behaviors, leading to more robust driver safety systems.</li>
</ol>
</section>
<section id="leveraging-other-machine-learning-models" class="level3">
<h3 class="anchored" data-anchor-id="leveraging-other-machine-learning-models">Leveraging Other Machine Learning Models</h3>
<p>While KMeans has proven effective in our analysis, other machine learning models might offer different perspectives:</p>
<ul>
<li><strong>Hierarchical Clustering</strong>: This method could provide a more nuanced view of how different driving behaviors are related or clustered in a hierarchical manner.</li>
<li><strong>Density-Based Clustering (like DBSCAN)</strong>: Such models could be more adept at handling outliers and varying cluster densities, which are common in real-world driving data.</li>
</ul>
</section>
<section id="the-evolving-role-of-machine-learning-in-driving-behavior-analysis" class="level3">
<h3 class="anchored" data-anchor-id="the-evolving-role-of-machine-learning-in-driving-behavior-analysis">The Evolving Role of Machine Learning in Driving Behavior Analysis</h3>
<p>The field of machine learning is continuously evolving, and its application in understanding driving behaviors is no exception. As technology advances, so too does our ability to capture and analyze increasingly complex data. This evolution promises not only more sophisticated analytical techniques but also more personalized and adaptive applications in automotive technology.</p>
<ul>
<li><strong>Personalized Driver Assistance Systems</strong>: By understanding individual driving patterns, future driver assistance systems could adapt to the unique style of each driver, enhancing both safety and driving experience.</li>
<li><strong>Contribution to Autonomous Vehicle Development</strong>: Insights from clustering driving behaviors can inform the development of more sophisticated and safer autonomous driving systems.</li>
<li><strong>Dynamic Insurance Models</strong>: Machine learning could enable the development of dynamic insurance models that adapt to individual driving behaviors, offering more personalized insurance policies.</li>
</ul>
</section>
<section id="concluding-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="concluding-thoughts">Concluding Thoughts</h3>
<p>Our journey through clustering driving behaviors is just a glimpse into the potential of machine learning in this field. As we continue to gather more data and develop more advanced analytical tools, our understanding and application of these insights will only deepen. The road ahead is promising, and the continuous evolution of machine learning will undoubtedly play a central role in shaping the future of driving behavior analysis.</p>


<!-- -->

</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb15" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Clustering"</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Joanna Fang"</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2023-11-98"</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [ml, code, clustering, driving, kaggle]</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co">  html: </span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co">    code-block-bg: "#FFFFFF"</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co">    code-block-border-left: "#E83283"</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools:</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co">      source: true</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co">      toggle: false</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="co">      caption: none</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="fu"># Clustering: Optimizing Driver Behavior Analysis with Machine Learning Clustering </span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a><span class="al">![](thumbnail.jpg)</span>{width="50%" fig-align="center"}</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>The analysis of driving behaviors, a task of both complexity and nuance, is increasingly significant in the realms of traffic safety and autonomous vehicle development. Machine learning, particularly clustering algorithms, stands at the forefront of this analysis, offering the ability to dissect and understand the myriad patterns hidden within driving data. Clustering, a form of unsupervised learning, excels in finding structure within unlabeled data, grouping similar data points together based on shared characteristics.</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>In this blog, we dive into a practical application of clustering using a dataset from Kaggle titled 'Driving Behavior'. This dataset is intriguing because it comes with labeled data, categorizing driving behaviors into three types: SLOW, NORMAL, and AGGRESSIVE. While clustering typically operates on unlabeled data, we use this labeled dataset to our advantage. It will allow us to compare the clusters formed by our machine learning algorithm against the pre-labeled categories, providing an insightful backdrop to understand and validate our clustering results.</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>Our aim is to illustrate the entire process of applying clustering to analyze driving behavior. We start with a predefined notion of having three clusters based on the labels present in the dataset. However, as we delve deeper into the analysis, we realize the necessity to step back and question our initial assumptions. We employ the Elbow Method, supplemented by the <span class="in">`kneed`</span> Python package, to determine the optimal number of clusters in a more data-driven manner. This journey from assumption to optimization is not just about applying a machine learning technique but also about understanding the nuances and intricacies that come with real-world data analysis.</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>Join us as we unravel the complexities of driving behaviors through clustering, starting with predefined labels and moving towards an optimized, data-driven approach.</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a><span class="fu">## Setting the Stage with Data and Tools</span></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>In the world of machine learning, the choice of dataset is as crucial as the algorithm itself. For our endeavor into clustering driving behaviors, we've chosen a dataset from Kaggle named 'Driving Behavior'. This dataset is particularly interesting for a few reasons. Firstly, it's labeled with specific driving behaviors, which, while not necessary for clustering, provides us with a unique opportunity to validate and understand our clustering results in a more tangible way.</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Kaggle 'Driving Behavior' Dataset</span></span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>This dataset offers a detailed glimpse into various aspects of driving, captured through different features:</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Acceleration (AccX, AccY, AccZ)**: These features measure the vehicle's acceleration in meters per second squared ($$m/s^2$$) along the X, Y, and Z axes. Acceleration data is crucial for understanding sudden movements and changes in driving speed.</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Rotation (GyroX, GyroY, GyroZ)**: Here, we have the vehicle's angular velocity around the X, Y, and Z axes, measured in degrees per second ($$°/s$$). This data helps in identifying steering patterns and gauging the stability of the vehicle during various maneuvers.</span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Classification Labels (SLOW, NORMAL, AGGRESSIVE)**: Each data point is tagged with one of these labels. It's important to note that in clustering, such labels are not typically required. However, in our case, these labels will serve as a benchmark, enabling us to compare our unsupervised learning results with pre-defined categories.</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>This dataset not only provides a foundation for applying clustering techniques but also allows us to explore how well unsupervised learning can mirror human-labeled classifications.</span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a><span class="fu">### Tools and Libraries</span></span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>Our analysis will be powered by several key tools and libraries, each bringing its own strengths to the table:</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Python**: Our language of choice, renowned for its ease of use and strong community support, especially in data science.</span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Scikit-learn**: A powerful Python library for machine learning. We will use its clustering algorithms, specifically KMeans, to group our data.</span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Matplotlib**: This library will help us in visualizing our data and the results of our clustering, making the insights more accessible.</span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**kneed**: An invaluable tool when it comes to determining the optimal number of clusters. It programmatically identifies the 'elbow point' in our dataset, a crucial step in ensuring our clustering approach is as effective as possible.</span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a>In the next sections, we'll dive into the process of applying these tools to our dataset, starting with our initial approach and gradually moving towards a more refined, data-driven methodology.</span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a><span class="fu">## Initial Clustering Approach</span></span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a>Embarking on our journey to unravel the driving patterns hidden in the dataset, we initially gravitated towards a straightforward approach: clustering the data into three groups. This decision was influenced by the dataset's labels - SLOW, NORMAL, and AGGRESSIVE. It seemed logical to align our clusters with these pre-defined categories, under the assumption that they would naturally encapsulate the essence of the driving behaviors.</span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why Start with Three Clusters?</span></span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a>The rationale was straightforward: the dataset labels suggested three distinct types of driving behaviors. Clustering aims to group similar data points, and with these labels as a guide, it seemed reasonable to start our analysis by segmenting the data into three clusters, hoping they would align with the labeled behaviors.</span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a><span class="fu">### Applying KMeans Clustering</span></span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a>The KMeans algorithm was chosen for its simplicity and effectiveness in many clustering scenarios. Here's a breakdown of how we applied it:</span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install numpy</span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install matplotlib</span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install scipy</span>
<span id="cb15-77"><a href="#cb15-77" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install scikit<span class="op">-</span>learn</span>
<span id="cb15-78"><a href="#cb15-78" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install pandas</span>
<span id="cb15-79"><a href="#cb15-79" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install kneed</span>
<span id="cb15-80"><a href="#cb15-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-81"><a href="#cb15-81" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb15-82"><a href="#cb15-82" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb15-83"><a href="#cb15-83" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb15-84"><a href="#cb15-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-85"><a href="#cb15-85" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the datasets</span></span>
<span id="cb15-86"><a href="#cb15-86" aria-hidden="true" tabindex="-1"></a>data_1 <span class="op">=</span> pd.read_csv(<span class="st">'motion_data_1.csv'</span>)</span>
<span id="cb15-87"><a href="#cb15-87" aria-hidden="true" tabindex="-1"></a>data_2 <span class="op">=</span> pd.read_csv(<span class="st">'motion_data_2.csv'</span>)</span>
<span id="cb15-88"><a href="#cb15-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-89"><a href="#cb15-89" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine datasets</span></span>
<span id="cb15-90"><a href="#cb15-90" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.concat([data_1, data_2])</span>
<span id="cb15-91"><a href="#cb15-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-92"><a href="#cb15-92" aria-hidden="true" tabindex="-1"></a><span class="co"># Data preprocessing</span></span>
<span id="cb15-93"><a href="#cb15-93" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb15-94"><a href="#cb15-94" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> data[[<span class="st">'AccX'</span>, <span class="st">'AccY'</span>, <span class="st">'AccZ'</span>, <span class="st">'GyroX'</span>, <span class="st">'GyroY'</span>, <span class="st">'GyroZ'</span>]]</span>
<span id="cb15-95"><a href="#cb15-95" aria-hidden="true" tabindex="-1"></a>scaled_features <span class="op">=</span> scaler.fit_transform(features)</span>
<span id="cb15-96"><a href="#cb15-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-97"><a href="#cb15-97" aria-hidden="true" tabindex="-1"></a><span class="co"># Applying KMeans with 3 clusters</span></span>
<span id="cb15-98"><a href="#cb15-98" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb15-99"><a href="#cb15-99" aria-hidden="true" tabindex="-1"></a>clusters <span class="op">=</span> kmeans.fit_predict(scaled_features)</span>
<span id="cb15-100"><a href="#cb15-100" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-101"><a href="#cb15-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-102"><a href="#cb15-102" aria-hidden="true" tabindex="-1"></a>In this code snippet, we begin by loading the dataset and selecting our features of interest, namely the acceleration and rotation measurements. These features are then scaled using <span class="in">`StandardScaler`</span>, a crucial step to ensure that all features contribute equally to the clustering process. The <span class="in">`KMeans`</span> algorithm is then applied to the scaled data, specifying three clusters.</span>
<span id="cb15-103"><a href="#cb15-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-104"><a href="#cb15-104" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visualizing the Initial Results</span></span>
<span id="cb15-105"><a href="#cb15-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-106"><a href="#cb15-106" aria-hidden="true" tabindex="-1"></a>To understand our initial clustering, we visualized the results:</span>
<span id="cb15-107"><a href="#cb15-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-110"><a href="#cb15-110" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-111"><a href="#cb15-111" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb15-112"><a href="#cb15-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-113"><a href="#cb15-113" aria-hidden="true" tabindex="-1"></a>plt.scatter(scaled_features[:,<span class="dv">0</span>], scaled_features[:,<span class="dv">1</span>], c<span class="op">=</span>clusters, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb15-114"><a href="#cb15-114" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Scaled AccX'</span>)</span>
<span id="cb15-115"><a href="#cb15-115" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Scaled AccY'</span>)</span>
<span id="cb15-116"><a href="#cb15-116" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Initial Clustering with 3 Clusters'</span>)</span>
<span id="cb15-117"><a href="#cb15-117" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb15-118"><a href="#cb15-118" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-119"><a href="#cb15-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-120"><a href="#cb15-120" aria-hidden="true" tabindex="-1"></a>This visualization plots the scaled acceleration data on the X and Y axes, colored by the assigned cluster. It provides a first glimpse into how the data points are grouped by the algorithm.</span>
<span id="cb15-121"><a href="#cb15-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-122"><a href="#cb15-122" aria-hidden="true" tabindex="-1"></a><span class="fu">### Limitations of the Initial Approach</span></span>
<span id="cb15-123"><a href="#cb15-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-124"><a href="#cb15-124" aria-hidden="true" tabindex="-1"></a>While starting with three clusters seemed logical, this approach had its limitations. Clustering, especially KMeans, doesn't inherently consider pre-existing labels. It simply groups data based on feature similarity. Moreover, the true complexity of driving behavior might not be accurately captured in just three categories. This realization led us to question our initial assumption: Were three clusters really sufficient to capture the diverse nuances of driving behavior?</span>
<span id="cb15-125"><a href="#cb15-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-126"><a href="#cb15-126" aria-hidden="true" tabindex="-1"></a>In the following section, we explore how we addressed these limitations by seeking an optimal number of clusters beyond the confines of the initial labeled categories.</span>
<span id="cb15-127"><a href="#cb15-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-128"><a href="#cb15-128" aria-hidden="true" tabindex="-1"></a><span class="fu">## Realizing the Need for Optimization</span></span>
<span id="cb15-129"><a href="#cb15-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-130"><a href="#cb15-130" aria-hidden="true" tabindex="-1"></a>After the initial clustering, a crucial question arose: were three clusters truly representative of the driving behaviors in our dataset? This led us to the concept of cluster optimization, a pivotal step in ensuring that our machine learning model accurately reflects the complexities of the data.</span>
<span id="cb15-131"><a href="#cb15-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-132"><a href="#cb15-132" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Essence of Cluster Optimization</span></span>
<span id="cb15-133"><a href="#cb15-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-134"><a href="#cb15-134" aria-hidden="true" tabindex="-1"></a>Cluster optimization revolves around finding the 'just right' number of clusters in a dataset. Too few clusters, and the model might oversimplify the data, missing out on important nuances. Too many, and it might overfit, capturing random noise as meaningful patterns. The optimal number of clusters strikes a balance, grouping the data in a way that maximizes both similarity within clusters and differences between them.</span>
<span id="cb15-135"><a href="#cb15-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-136"><a href="#cb15-136" aria-hidden="true" tabindex="-1"></a><span class="fu">### Questioning the Three-Cluster Model</span></span>
<span id="cb15-137"><a href="#cb15-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-138"><a href="#cb15-138" aria-hidden="true" tabindex="-1"></a>Our initial model with three clusters was a natural starting point, mirroring the three labeled categories in the dataset. However, this approach had its pitfalls:</span>
<span id="cb15-139"><a href="#cb15-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-140"><a href="#cb15-140" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Assuming Label Completeness**: The labeled categories in the dataset might not encompass all the distinct driving behaviors present. Real-world driving is complex and might not fit neatly into just three categories.</span>
<span id="cb15-141"><a href="#cb15-141" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Ignoring Unsupervised Learning Nature**: Clustering, especially KMeans, is an unsupervised technique. It groups data based on feature similarity, independent of any pre-existing labels. Our model needed to reflect the data's structure, not just its labels.</span>
<span id="cb15-142"><a href="#cb15-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-143"><a href="#cb15-143" aria-hidden="true" tabindex="-1"></a>These considerations led us to explore beyond the confines of the three predefined categories, seeking a more data-driven approach to determine the number of clusters.</span>
<span id="cb15-144"><a href="#cb15-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-145"><a href="#cb15-145" aria-hidden="true" tabindex="-1"></a><span class="fu">### Embracing the Elbow Method</span></span>
<span id="cb15-146"><a href="#cb15-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-147"><a href="#cb15-147" aria-hidden="true" tabindex="-1"></a>The Elbow Method emerged as our tool of choice for finding the optimal number of clusters. It involves plotting the model's performance (measured in terms of Sum of Squared Errors, or SSE) against a range of cluster numbers. The 'elbow point' in this plot, where the rate of decrease in SSE sharply changes, indicates the optimal cluster count.</span>
<span id="cb15-148"><a href="#cb15-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-149"><a href="#cb15-149" aria-hidden="true" tabindex="-1"></a>In the next section, we'll delve into how we applied the Elbow Method to our dataset, using a combination of scikit-learn and the <span class="in">`kneed`</span> package to not just visualize but also programmatically identify the elbow point, marking a significant step in refining our clustering model.</span>
<span id="cb15-150"><a href="#cb15-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-151"><a href="#cb15-151" aria-hidden="true" tabindex="-1"></a><span class="fu">## Implementing the Elbow Method</span></span>
<span id="cb15-152"><a href="#cb15-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-153"><a href="#cb15-153" aria-hidden="true" tabindex="-1"></a>With the realization that our initial model might not optimally capture the driving behaviors in our dataset, we turned to the Elbow Method. This technique is a cornerstone in determining the appropriate number of clusters in unsupervised learning, particularly in KMeans clustering.</span>
<span id="cb15-154"><a href="#cb15-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-155"><a href="#cb15-155" aria-hidden="true" tabindex="-1"></a><span class="fu">### Understanding the Elbow Method</span></span>
<span id="cb15-156"><a href="#cb15-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-157"><a href="#cb15-157" aria-hidden="true" tabindex="-1"></a>The Elbow Method is a heuristic used in cluster analysis to determine the number of clusters in a dataset. The method involves running the clustering algorithm multiple times, each time with a different number of clusters ($$ k $$), and calculating the Sum of Squared Errors (SSE) for each. SSE is defined as the sum of the squared distances between each member of a cluster and its centroid. As $$ k $$ increases, SSE tends to decrease as the clusters will be smaller and tighter. However, after a certain point, the marginal decrease in SSE with increasing $$ k $$ becomes insignificant, forming an 'elbow' in the plot. This point is considered the optimal number of clusters.</span>
<span id="cb15-158"><a href="#cb15-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-159"><a href="#cb15-159" aria-hidden="true" tabindex="-1"></a><span class="fu">### Applying the Elbow Method</span></span>
<span id="cb15-160"><a href="#cb15-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-161"><a href="#cb15-161" aria-hidden="true" tabindex="-1"></a>To apply the Elbow Method to our dataset, we first needed to calculate the SSE for a range of cluster numbers. Here's how we did it:</span>
<span id="cb15-162"><a href="#cb15-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-165"><a href="#cb15-165" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-166"><a href="#cb15-166" aria-hidden="true" tabindex="-1"></a>sse <span class="op">=</span> []</span>
<span id="cb15-167"><a href="#cb15-167" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>):</span>
<span id="cb15-168"><a href="#cb15-168" aria-hidden="true" tabindex="-1"></a>    kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb15-169"><a href="#cb15-169" aria-hidden="true" tabindex="-1"></a>    kmeans.fit(scaled_features)</span>
<span id="cb15-170"><a href="#cb15-170" aria-hidden="true" tabindex="-1"></a>    sse.append(kmeans.inertia_)</span>
<span id="cb15-171"><a href="#cb15-171" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-172"><a href="#cb15-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-173"><a href="#cb15-173" aria-hidden="true" tabindex="-1"></a>In this code block, we looped over a range of cluster numbers (from 1 to 10) and for each number, we applied the KMeans algorithm to our scaled features. The inertia_ attribute of KMeans gives us the SSE for that particular cluster count. We stored these SSE values in a list.</span>
<span id="cb15-174"><a href="#cb15-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-175"><a href="#cb15-175" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visualizing SSE vs. Number of Clusters</span></span>
<span id="cb15-176"><a href="#cb15-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-177"><a href="#cb15-177" aria-hidden="true" tabindex="-1"></a>To find the elbow point, we plotted the SSE against the number of clusters:</span>
<span id="cb15-178"><a href="#cb15-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-181"><a href="#cb15-181" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-182"><a href="#cb15-182" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>), sse, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb15-183"><a href="#cb15-183" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Elbow Method'</span>)</span>
<span id="cb15-184"><a href="#cb15-184" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Clusters'</span>)</span>
<span id="cb15-185"><a href="#cb15-185" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Sum of Squared Errors (SSE)'</span>)</span>
<span id="cb15-186"><a href="#cb15-186" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb15-187"><a href="#cb15-187" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-188"><a href="#cb15-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-189"><a href="#cb15-189" aria-hidden="true" tabindex="-1"></a>This visualization was crucial as it allowed us to observe the point at which the decrease in SSE becomes less sharp, indicating the optimal number of clusters. In the next section, we'll discuss how we used the <span class="in">`kneed`</span> package to programmatically identify this elbow point, further refining our clustering approach.</span>
<span id="cb15-190"><a href="#cb15-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-191"><a href="#cb15-191" aria-hidden="true" tabindex="-1"></a><span class="fu">## Programmatic Elbow Point Detection with `kneed`</span></span>
<span id="cb15-192"><a href="#cb15-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-193"><a href="#cb15-193" aria-hidden="true" tabindex="-1"></a>Having visualized the Sum of Squared Errors (SSE) against various cluster counts using the Elbow Method, our next step was to pinpoint the elusive 'elbow point' programmatically. This is where the <span class="in">`kneed`</span> Python package comes into play, offering a sophisticated yet straightforward approach to identifying the optimal number of clusters in our dataset.</span>
<span id="cb15-194"><a href="#cb15-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-195"><a href="#cb15-195" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Role of `kneed` in Cluster Analysis</span></span>
<span id="cb15-196"><a href="#cb15-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-197"><a href="#cb15-197" aria-hidden="true" tabindex="-1"></a><span class="in">`kneed`</span> is a Python library specifically designed to identify the knee point or elbow point in a dataset, which is often subjective and hard to pinpoint manually. It works by fitting a piecewise linear function to the data points and identifying the point of maximum curvature, which is the knee or elbow. This is particularly useful in KMeans clustering, as it takes the guesswork out of selecting the number of clusters.</span>
<span id="cb15-198"><a href="#cb15-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-199"><a href="#cb15-199" aria-hidden="true" tabindex="-1"></a><span class="fu">### Implementing `kneed` to Find the Optimal Clusters</span></span>
<span id="cb15-200"><a href="#cb15-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-201"><a href="#cb15-201" aria-hidden="true" tabindex="-1"></a>To utilize <span class="in">`kneed`</span> in our analysis, we first calculated the SSE for different cluster counts, just as we did for the Elbow Method. Then, <span class="in">`kneed`</span> took over to programmatically identify the elbow point:</span>
<span id="cb15-202"><a href="#cb15-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-205"><a href="#cb15-205" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-206"><a href="#cb15-206" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> kneed <span class="im">import</span> KneeLocator</span>
<span id="cb15-207"><a href="#cb15-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-208"><a href="#cb15-208" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculating SSE for a range of cluster counts</span></span>
<span id="cb15-209"><a href="#cb15-209" aria-hidden="true" tabindex="-1"></a>sse <span class="op">=</span> []</span>
<span id="cb15-210"><a href="#cb15-210" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>):</span>
<span id="cb15-211"><a href="#cb15-211" aria-hidden="true" tabindex="-1"></a>    kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb15-212"><a href="#cb15-212" aria-hidden="true" tabindex="-1"></a>    kmeans.fit(scaled_features)</span>
<span id="cb15-213"><a href="#cb15-213" aria-hidden="true" tabindex="-1"></a>    sse.append(kmeans.inertia_)</span>
<span id="cb15-214"><a href="#cb15-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-215"><a href="#cb15-215" aria-hidden="true" tabindex="-1"></a><span class="co"># Using kneed to find the elbow point</span></span>
<span id="cb15-216"><a href="#cb15-216" aria-hidden="true" tabindex="-1"></a>knee_locator <span class="op">=</span> KneeLocator(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>), sse, curve<span class="op">=</span><span class="st">'convex'</span>, direction<span class="op">=</span><span class="st">'decreasing'</span>)</span>
<span id="cb15-217"><a href="#cb15-217" aria-hidden="true" tabindex="-1"></a>elbow_point <span class="op">=</span> knee_locator.elbow</span>
<span id="cb15-218"><a href="#cb15-218" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-219"><a href="#cb15-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-220"><a href="#cb15-220" aria-hidden="true" tabindex="-1"></a>In this code snippet, we first computed the SSE for cluster counts from 1 to 10. We then instantiated the <span class="in">`KneeLocator`</span> class from the <span class="in">`kneed`</span> package, passing the range of cluster counts and the corresponding SSE values. The <span class="in">`curve='convex'`</span> and <span class="in">`direction='decreasing'`</span> parameters helped <span class="in">`kneed`</span> understand the nature of our SSE plot. The <span class="in">`elbow`</span> attribute of the <span class="in">`KneeLocator`</span> object gave us the optimal cluster count.</span>
<span id="cb15-221"><a href="#cb15-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-222"><a href="#cb15-222" aria-hidden="true" tabindex="-1"></a><span class="fu">### Determining the Optimal Number of Clusters</span></span>
<span id="cb15-223"><a href="#cb15-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-224"><a href="#cb15-224" aria-hidden="true" tabindex="-1"></a>To our surprise, <span class="in">`kneed`</span> identified 4 as the optimal number of clusters, deviating from our initial assumption of 3 clusters based on the dataset's labels. This revelation was pivotal as it highlighted the importance of relying on data-driven methods rather than assumptions or external labeling.</span>
<span id="cb15-225"><a href="#cb15-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-228"><a href="#cb15-228" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-229"><a href="#cb15-229" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The optimal number of clusters identified by kneed: </span><span class="sc">{</span>elbow_point<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-230"><a href="#cb15-230" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-231"><a href="#cb15-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-232"><a href="#cb15-232" aria-hidden="true" tabindex="-1"></a>This insight led us to reapply our clustering model with four clusters, a decision driven entirely by the underlying data structure rather than pre-defined categories. In the next section, we will revisit our clustering approach with this newfound understanding and analyze how this adjustment impacts our analysis of driving behaviors.</span>
<span id="cb15-233"><a href="#cb15-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-234"><a href="#cb15-234" aria-hidden="true" tabindex="-1"></a><span class="fu">## Re-Clustering with Optimized Cluster Count</span></span>
<span id="cb15-235"><a href="#cb15-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-236"><a href="#cb15-236" aria-hidden="true" tabindex="-1"></a>Armed with the knowledge that four clusters might better represent our driving behavior data, we embarked on a re-clustering journey. This step was crucial for our analysis as it aligned our clustering approach more closely with the inherent structure of the data, moving beyond our initial assumptions.</span>
<span id="cb15-237"><a href="#cb15-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-238"><a href="#cb15-238" aria-hidden="true" tabindex="-1"></a><span class="fu">### Reapplying KMeans with Four Clusters</span></span>
<span id="cb15-239"><a href="#cb15-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-240"><a href="#cb15-240" aria-hidden="true" tabindex="-1"></a>Guided by the <span class="in">`kneed`</span> package's recommendation, we reconfigured our KMeans clustering model to partition the data into four clusters instead of three. Here's how we proceeded:</span>
<span id="cb15-241"><a href="#cb15-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-244"><a href="#cb15-244" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-245"><a href="#cb15-245" aria-hidden="true" tabindex="-1"></a><span class="co"># Applying KMeans with the optimized number of clusters</span></span>
<span id="cb15-246"><a href="#cb15-246" aria-hidden="true" tabindex="-1"></a>optimized_kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb15-247"><a href="#cb15-247" aria-hidden="true" tabindex="-1"></a>optimized_clusters <span class="op">=</span> optimized_kmeans.fit_predict(scaled_features)</span>
<span id="cb15-248"><a href="#cb15-248" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-249"><a href="#cb15-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-250"><a href="#cb15-250" aria-hidden="true" tabindex="-1"></a>In this code, the <span class="in">`KMeans`</span> class from scikit-learn was re-initialized with <span class="in">`n_clusters`</span> set to 4. We then fit the model to our scaled features and predicted the cluster for each data point.</span>
<span id="cb15-251"><a href="#cb15-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-252"><a href="#cb15-252" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visualizing the New Clusters</span></span>
<span id="cb15-253"><a href="#cb15-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-254"><a href="#cb15-254" aria-hidden="true" tabindex="-1"></a>Visualization plays a key role in understanding the implications of our clustering:</span>
<span id="cb15-255"><a href="#cb15-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-258"><a href="#cb15-258" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb15-259"><a href="#cb15-259" aria-hidden="true" tabindex="-1"></a>plt.scatter(scaled_features[:,<span class="dv">0</span>], scaled_features[:,<span class="dv">1</span>], c<span class="op">=</span>optimized_clusters, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb15-260"><a href="#cb15-260" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Scaled AccX'</span>)</span>
<span id="cb15-261"><a href="#cb15-261" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Scaled AccY'</span>)</span>
<span id="cb15-262"><a href="#cb15-262" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Clustering with 4 Clusters'</span>)</span>
<span id="cb15-263"><a href="#cb15-263" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb15-264"><a href="#cb15-264" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-265"><a href="#cb15-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-266"><a href="#cb15-266" aria-hidden="true" tabindex="-1"></a>In this plot, we observed how the data points were grouped into four distinct clusters, providing a fresh perspective on the driving behaviors represented in our dataset.</span>
<span id="cb15-267"><a href="#cb15-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-268"><a href="#cb15-268" aria-hidden="true" tabindex="-1"></a><span class="fu">### Interpreting the New Clustering Results</span></span>
<span id="cb15-269"><a href="#cb15-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-270"><a href="#cb15-270" aria-hidden="true" tabindex="-1"></a>With four clusters, the model had the flexibility to identify a more nuanced pattern within the data. This could mean uncovering a behavior that was previously merged with others in the three-cluster model or providing a clearer separation between behaviors.</span>
<span id="cb15-271"><a href="#cb15-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-272"><a href="#cb15-272" aria-hidden="true" tabindex="-1"></a><span class="fu">### Comparing the Four-Cluster Model with the Initial Three-Cluster Model</span></span>
<span id="cb15-273"><a href="#cb15-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-274"><a href="#cb15-274" aria-hidden="true" tabindex="-1"></a>The transition from three to four clusters was not just a numerical change but also a conceptual shift. It underscored the significance of letting the data guide our analysis:</span>
<span id="cb15-275"><a href="#cb15-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-276"><a href="#cb15-276" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Increased Granularity**: The four-cluster model offered a finer categorization of driving behaviors, potentially leading to more accurate insights.</span>
<span id="cb15-277"><a href="#cb15-277" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Data-Driven Approach**: This shift highlighted the importance of relying on data-driven techniques, such as the Elbow Method and <span class="in">`kneed`</span>, in machine learning tasks.</span>
<span id="cb15-278"><a href="#cb15-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-279"><a href="#cb15-279" aria-hidden="true" tabindex="-1"></a>The decision to increase the number of clusters was a pivotal moment in our project. It exemplified the dynamic nature of machine learning, where initial hypotheses are continuously tested and refined in light of new evidence.</span>
<span id="cb15-280"><a href="#cb15-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-281"><a href="#cb15-281" aria-hidden="true" tabindex="-1"></a>In the final section, we will summarize our findings, reflect on the journey, and discuss the potential applications and implications of our analysis in real-world scenarios.</span>
<span id="cb15-282"><a href="#cb15-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-283"><a href="#cb15-283" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusions and Insights</span></span>
<span id="cb15-284"><a href="#cb15-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-285"><a href="#cb15-285" aria-hidden="true" tabindex="-1"></a>As we conclude our exploration into clustering driving behaviors using the Kaggle dataset, it's crucial to reflect on the key insights gained and the broader implications of our findings.</span>
<span id="cb15-286"><a href="#cb15-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-287"><a href="#cb15-287" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Findings from the Optimized Clustering</span></span>
<span id="cb15-288"><a href="#cb15-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-289"><a href="#cb15-289" aria-hidden="true" tabindex="-1"></a>Our journey through clustering revealed several important insights:</span>
<span id="cb15-290"><a href="#cb15-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-291"><a href="#cb15-291" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Optimal Cluster Number**: Moving from a 3-cluster model to a 4-cluster model, as suggested by the Elbow Method and validated by <span class="in">`kneed`</span>, allowed us to uncover a more intricate structure within the driving behavior data.</span>
<span id="cb15-292"><a href="#cb15-292" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Granularity in Clustering**: The additional cluster provided a finer categorization of driving behaviors, which could lead to more nuanced and accurate insights into different driving styles.</span>
<span id="cb15-293"><a href="#cb15-293" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Data-Driven Approach**: This project highlighted the importance of a data-driven approach in clustering. While initial assumptions based on labeled data provided a starting point, it was the analysis of the inherent data structure that ultimately guided our clustering strategy.</span>
<span id="cb15-294"><a href="#cb15-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-295"><a href="#cb15-295" aria-hidden="true" tabindex="-1"></a><span class="fu">### Practical Implications in Real-World Scenarios</span></span>
<span id="cb15-296"><a href="#cb15-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-297"><a href="#cb15-297" aria-hidden="true" tabindex="-1"></a>The implications of our findings extend far beyond this analysis:</span>
<span id="cb15-298"><a href="#cb15-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-299"><a href="#cb15-299" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Enhanced Driver Safety Systems**: By understanding nuanced driving behaviors, automotive manufacturers and researchers can develop more sophisticated driver safety systems that cater to different driving styles.</span>
<span id="cb15-300"><a href="#cb15-300" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Targeted Driver Training Programs**: The insights from clustering can inform targeted training programs that address specific driving behavior patterns, thereby enhancing road safety.</span>
<span id="cb15-301"><a href="#cb15-301" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Insurance and Risk Assessment**: The identification of different driving behaviors can be instrumental in risk assessment for car insurance companies, enabling them to tailor policies based on individual driving patterns.</span>
<span id="cb15-302"><a href="#cb15-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-303"><a href="#cb15-303" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Importance of Data-Driven Decision-Making</span></span>
<span id="cb15-304"><a href="#cb15-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-305"><a href="#cb15-305" aria-hidden="true" tabindex="-1"></a>This project served as a reminder of the critical role of data-driven decision-making in machine learning:</span>
<span id="cb15-306"><a href="#cb15-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-307"><a href="#cb15-307" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Beyond Assumptions**: While assumptions and labels can provide direction, they should not be the sole basis for decision-making in clustering problems.</span>
<span id="cb15-308"><a href="#cb15-308" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Embracing Flexibility in Analysis**: Machine learning, especially unsupervised learning like clustering, requires an openness to adapt analysis strategies based on what the data reveals.</span>
<span id="cb15-309"><a href="#cb15-309" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Iterative Process**: Optimization in machine learning is an iterative process. It often involves revisiting and refining models as new data or techniques become available.</span>
<span id="cb15-310"><a href="#cb15-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-311"><a href="#cb15-311" aria-hidden="true" tabindex="-1"></a><span class="fu">### Final Thoughts</span></span>
<span id="cb15-312"><a href="#cb15-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-313"><a href="#cb15-313" aria-hidden="true" tabindex="-1"></a>Our journey through clustering driving behaviors illustrates the dynamic and iterative nature of machine learning. By embracing a data-driven approach, we were able to uncover more about the dataset than what was initially apparent. Such methodologies are not just limited to driving behavior analysis but are applicable across various domains where data-driven insights are crucial.</span>
<span id="cb15-314"><a href="#cb15-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-315"><a href="#cb15-315" aria-hidden="true" tabindex="-1"></a>As we continue to advance in the field of machine learning, the lessons learned here about the importance of flexibility, iteration, and data-driven analysis will remain fundamental to unlocking the full potential of our data.</span>
<span id="cb15-316"><a href="#cb15-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-317"><a href="#cb15-317" aria-hidden="true" tabindex="-1"></a><span class="fu">## Future Directions</span></span>
<span id="cb15-318"><a href="#cb15-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-319"><a href="#cb15-319" aria-hidden="true" tabindex="-1"></a>As we wrap up our exploration into clustering driving behaviors, it's essential to look ahead and consider the avenues for further research and development in this field. The journey we've embarked on opens up a myriad of possibilities, each holding the potential to deepen our understanding of driving behaviors and enhance the applications of machine learning in this domain.</span>
<span id="cb15-320"><a href="#cb15-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-321"><a href="#cb15-321" aria-hidden="true" tabindex="-1"></a><span class="fu">### Expanding the Research Horizon</span></span>
<span id="cb15-322"><a href="#cb15-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-323"><a href="#cb15-323" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Integrating More Complex Data**: Future studies could integrate more complex data types, such as video feeds, GPS data, or real-time traffic information. This would allow for a more comprehensive analysis of driving behaviors, taking into account environmental and situational variables.</span>
<span id="cb15-324"><a href="#cb15-324" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Exploring Time-Series Analysis**: Given that driving data is inherently time-sequential, applying time-series analysis could uncover patterns that static clustering might miss. Techniques like recurrent neural networks (RNNs) or Long Short-Term Memory (LSTM) networks could be particularly useful.</span>
<span id="cb15-325"><a href="#cb15-325" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Incorporating Weather and Road Conditions**: Adding weather and road condition data could offer insights into how external factors influence driving behaviors, leading to more robust driver safety systems.</span>
<span id="cb15-326"><a href="#cb15-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-327"><a href="#cb15-327" aria-hidden="true" tabindex="-1"></a><span class="fu">### Leveraging Other Machine Learning Models</span></span>
<span id="cb15-328"><a href="#cb15-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-329"><a href="#cb15-329" aria-hidden="true" tabindex="-1"></a>While KMeans has proven effective in our analysis, other machine learning models might offer different perspectives:</span>
<span id="cb15-330"><a href="#cb15-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-331"><a href="#cb15-331" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hierarchical Clustering**: This method could provide a more nuanced view of how different driving behaviors are related or clustered in a hierarchical manner.</span>
<span id="cb15-332"><a href="#cb15-332" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Density-Based Clustering (like DBSCAN)**: Such models could be more adept at handling outliers and varying cluster densities, which are common in real-world driving data.</span>
<span id="cb15-333"><a href="#cb15-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-334"><a href="#cb15-334" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Evolving Role of Machine Learning in Driving Behavior Analysis</span></span>
<span id="cb15-335"><a href="#cb15-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-336"><a href="#cb15-336" aria-hidden="true" tabindex="-1"></a>The field of machine learning is continuously evolving, and its application in understanding driving behaviors is no exception. As technology advances, so too does our ability to capture and analyze increasingly complex data. This evolution promises not only more sophisticated analytical techniques but also more personalized and adaptive applications in automotive technology.</span>
<span id="cb15-337"><a href="#cb15-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-338"><a href="#cb15-338" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Personalized Driver Assistance Systems**: By understanding individual driving patterns, future driver assistance systems could adapt to the unique style of each driver, enhancing both safety and driving experience.</span>
<span id="cb15-339"><a href="#cb15-339" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Contribution to Autonomous Vehicle Development**: Insights from clustering driving behaviors can inform the development of more sophisticated and safer autonomous driving systems.</span>
<span id="cb15-340"><a href="#cb15-340" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dynamic Insurance Models**: Machine learning could enable the development of dynamic insurance models that adapt to individual driving behaviors, offering more personalized insurance policies.</span>
<span id="cb15-341"><a href="#cb15-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-342"><a href="#cb15-342" aria-hidden="true" tabindex="-1"></a><span class="fu">### Concluding Thoughts</span></span>
<span id="cb15-343"><a href="#cb15-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-344"><a href="#cb15-344" aria-hidden="true" tabindex="-1"></a>Our journey through clustering driving behaviors is just a glimpse into the potential of machine learning in this field. As we continue to gather more data and develop more advanced analytical tools, our understanding and application of these insights will only deepen. The road ahead is promising, and the continuous evolution of machine learning will undoubtedly play a central role in shaping the future of driving behavior analysis.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>