---
title: "5\\. Anomaly/Outlier Detection"
author: "Joanna Fang"
date: "2023-12-06"
categories: [ml, code, anomaly detection, outlier detection, pollution]
jupyter: python3
format:
  html: 
    code-block-bg: "#FFFFFF"
    code-block-border-left: "#E83283"
    toc: true
    code-tools:
      source: true
      toggle: false
      caption: none

---


# Detecting Anomalies in Air Pollution Data: A Data Science Project

![](thumbnail.jpg){width="50%" fig-align="center"}

## Introduction

Welcome to our exploration of "Detecting Anomalies in Air Pollution Data," a vital project in the realm of environmental monitoring. With increasing concerns about air quality and its impact on public health and the environment, identifying irregularities in air pollution data has never been more critical.

This project leverages a comprehensive dataset from the Beijing Multi-site Air Quality Data, which offers a rich tapestry of air pollutant measurements and meteorological data across various sites in Beijing. The data spans from 2013 to 2017, providing insights into pollutants like PM2.5, PM10, SO2, NO2, and CO, as well as meteorological conditions like temperature, humidity, and wind speed.

Our primary goal is to detect unusual patterns or outliers in air quality data that might signify environmental hazards, technical errors in data collection, or significant meteorological impacts. By accomplishing this, we aim to contribute to more effective environmental monitoring and policy-making.

## Data Exploration and Preprocessing

### Understanding the Dataset
The first step in our data science journey involves getting acquainted with the dataset's structure and characteristics. This involves examining the various columns of the dataset, which include both pollutant levels and meteorological factors.

```{python}
# Install and import all libraries
import sys
!{sys.executable} -m pip install seaborn
!{sys.executable} -m pip install matplotlib
!{sys.executable} -m pip install scikit-learn
!{sys.executable} -m pip install pandas
!{sys.executable} -m pip install numpy

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.ensemble import IsolationForest
```

```{python}
sample_data_org = pd.read_csv('air_data_all.csv')
sample_data_org.head()
```

By running this code, we get a glimpse of the first few rows of our dataset, allowing us to understand the types of data we will be working with.

### Handling Missing Data and Categorical Variables
Dealing with missing data and categorical variables is a crucial part of data preprocessing. To address this, we first identify the missing values and then decide on an appropriate strategy, such as imputation or removal. However, in later processing, we realized we hadn't done enough here. 

```{python}
missing_values = sample_data_org.isnull().sum()
missing_values
```

```{python}
# Remove rows with missing values
sample_data = sample_data_org.dropna()

# Identify numerical columns
numerical_cols = sample_data.select_dtypes(include=['int64', 'float64']).columns

# Create a pipeline for imputing missing values and scaling
pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),  # Replace missing values with mean
    ('scaler', StandardScaler()),                # Scale the data
])

# Apply the pipeline to the numerical columns
scaled_data = pipeline.fit_transform(sample_data[numerical_cols])

# Apply PCA
pca = PCA(n_components=0.95)  # Retain 95% of the variance
principal_components = pca.fit_transform(scaled_data)

# Identify non-numeric (categorical) columns
categorical_cols = sample_data.select_dtypes(include=['object']).columns

# One-hot encode the categorical data
encoder = OneHotEncoder(sparse=False)
categorical_encoded = encoder.fit_transform(sample_data[categorical_cols])

# Check for 'get_feature_names_out' method for naming columns
if hasattr(encoder, 'get_feature_names_out'):
    encoded_columns = pd.DataFrame(categorical_encoded, columns=encoder.get_feature_names_out(categorical_cols))
else:
    # Fallback: manually create feature names
    encoded_columns = pd.DataFrame(categorical_encoded)
    encoded_columns.columns = [col + '_' + str(i) for col in categorical_cols for i in range(encoded_columns.shape[1])]

# Concatenate the encoded columns with the original dataset and drop the original categorical columns
sample_data_encoded = pd.concat([sample_data.drop(categorical_cols, axis=1), encoded_columns], axis=1)
```

For categorical variables like wind direction, we use encoding techniques to convert them into numerical form, making them suitable for analysis.

### Normalization and Standardization
Given the varying scales of our numerical features, normalization or standardization becomes necessary. This step ensures that no single feature disproportionately influences the model due to its scale.

```{python}
scaler = StandardScaler()
scaled_data = scaler.fit_transform(sample_data[['PM2.5', 'PM10', 'TEMP', 'PRES']])
```

### Feature Selection and Engineering
Finally, we perform feature selection and engineering. This process involves attempting to choose the most relevant features and possibly creating new features to improve our model's performance.

1. **Correlation Analysis**: First, we can perform a correlation analysis to understand the relationships between different features. This can help in identifying features that are strongly correlated with each other, from which we can select the most relevant ones.

```{python}
# Now perform the correlation analysis on the numerical data
corr = sample_data_encoded.corr()

# Generate a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, fmt=".2f")
plt.show()
```

The heatmap visualizes the correlation coefficients between different variables related to air quality and weather conditions. The scale on the right indicates the strength of the correlation, ranging from -1 (a perfect negative correlation) to 1 (a perfect positive correlation). Dark red shades indicate strong positive correlations, whereas dark blue shades represent strong negative correlations. Most variables do not exhibit a strong correlation with each other, as indicated by the shades of white (near-zero correlation). This kind of visualization helps to identify relationships between variables, which can be further investigated for causal connections or dependencies.

2. **Principal Component Analysis (PCA)**: PCA is a technique used to reduce the dimensionality of the data, enhancing the interpretability while minimizing information loss.

```{python}
pca = PCA(n_components=0.95) # Retain 95% of the variance
principal_components = pca.fit_transform(scaled_data)
```

This code applies PCA to the scaled data, reducing the number of features while retaining 95% of the variance in the data.

3. **Feature Engineering**: Here, we are attempting to create a new feature that might be more indicative of anomalies by creating a composite air quality index from multiple pollutants.

```{python}
sample_data['Air_Quality_Index'] = sample_data['PM2.5'] * 0.4 + sample_data['PM10'] * 0.2 + sample_data['NO2'] * 0.2 + sample_data['SO2'] * 0.1 + sample_data['CO'] * 0.1
```

This code creates a new feature, 'Air_Quality_Index', as a weighted sum of various pollutants, hypothesizing that this composite index might be a more effective predictor of anomalies.

Through these steps, we attempted to refine our dataset to include the most relevant features for anomaly detection, enhancing the model's accuracy and efficiency.

## Anomaly Detection Algorithms and Model Training and Evaluation

### Choosing the Anomaly Detection Algorithm: Isolation Forest
After trying out other models, which took more than 5 minutes to run, for our project on air pollution data, we have opted for the Isolation Forest algorithm due to its efficiency and effectiveness, especially in dealing with large and high-dimensional datasets like ours.  

```{python}
iso_forest = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)
```

### Data Preprocessing and Splitting

We split our dataset into training and test sets, ensuring that the model is evaluated on unseen data, reflecting its performance in real-world scenarios.

```{python}
# Handling NaN Values with Imputation
# Impute missing values and then scale the numerical columns
num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),  # Replace missing values with mean
    ('std_scaler', StandardScaler())
])
# Apply the pipeline to the numerical columns
numerical_cols = sample_data_org.select_dtypes(include=['int64', 'float64']).columns
sample_data_org[numerical_cols] = num_pipeline.fit_transform(sample_data_org[numerical_cols])
# One-hot encode the categorical columns
categorical_cols = sample_data_org.select_dtypes(include=['object']).columns
sample_data_org = pd.get_dummies(sample_data_org, columns=categorical_cols, drop_first=True)

# Spliting the Dataset 
X_train, X_test = train_test_split(sample_data_org, test_size=0.3, random_state=42)
```

### Training the Model
The training process involves fitting the Isolation Forest model to our training data.

```{python}
iso_forest.fit(X_train)
```

### Evaluation Metrics

### Model Evaluation and Insights
After training, we assess the model's performance on the test set. This evaluation helps us understand the effectiveness of our anomaly detection in the context of air pollution data.

In an unsupervised dataset scenario, where we don't have labeled data (`y_test`), the evaluation of an anomaly detection model like Isolation Forest is more about understanding and interpreting the anomalies it detects rather than calculating quantitative metrics. The goal is to examine the anomalies flagged by the model and determine if they align with our domain knowledge or expectations.

#### Detecting Anomalies
First, use the model to predict anomalies in your test set. The Isolation Forest model marks an anomaly with -1 and normal with 1.

```{python}
# Predict anomalies on the test set
anomalies = iso_forest.predict(X_test)

# Convert predictions: -1 (anomalies) to 1 and 1 (normal) to 0
anomalies = np.where(anomalies == -1, 1, 0)
```

#### Analyzing Detected Anomalies
The next step is to analyze these detected anomalies. 

```{python}
# Count the number of anomalies detected
num_anomalies = np.sum(anomalies)
total_points = len(anomalies)
print(f"Total data points: {total_points}")
print(f"Number of anomalies detected: {num_anomalies}")
print(f"Proportion of anomalies detected: {num_anomalies / total_points:.2%}")
```

#### Inspecting Anomalous Data Points
It can be insightful to examine the data points that the model flagged as anomalies. This involves looking at the specific characteristics of these data points.

```{python}
# Create a DataFrame of the test set with a column for anomaly labels
test_set_with_predictions = X_test.copy()
test_set_with_predictions['Anomaly'] = anomalies

# Display some of the anomalies
anomalous_data = test_set_with_predictions[test_set_with_predictions['Anomaly'] == 1]
print("Sample of detected anomalies:")
print(anomalous_data.sample(min(10, len(anomalous_data))))  # Display up to 10 anomalous points
```

## Visualization of Anomalies

### Creating Visualizations

To showcase the detected anomalies, we employ various types of visualizations. Here, we'll focus on two primary types: scatter plots and heatmaps. These visualizations will help us to interpret the anomalies in the context of air pollution data.

#### Scatter Plots
Scatter plots are excellent for visualizing the relationship between two variables and identifying points that stand out from the pattern.

```{python}
plt.figure(figsize=(10, 6))
sns.scatterplot(data=anomalous_data, x='TEMP', y='PM2.5', hue='Anomaly')
plt.title('Scatter Plot of PM2.5 vs Temperature')
plt.xlabel('Temperature')
plt.ylabel('PM2.5')
plt.show()
```

The scatter plot visualizes PM2.5 levels against temperature, with each point representing an observation. The data points are standardized, as indicated by the temperature axis ranging from approximately -2 to 2. A dense clustering of points suggests that lower PM2.5 levels are common across the temperature range, with a noticeable spread in higher PM2.5 levels at mid-range temperatures. Points that stand out from the dense cloud indicate potential anomalies with higher PM2.5 levels, which could be of interest for further investigation into air quality issues at different temperatures.

#### Heatmaps
Heatmaps are useful for understanding the distribution and concentration of data points across two dimensions.

```{python}
# Create a heatmap to show the concentration of anomalies
# Sample a subset of the anomalous data for quicker visualization
sampled_anomalous_data = anomalous_data.sample(min(500, len(anomalous_data)), random_state=42)

# Create a heatmap without annotations for quicker rendering
plt.figure(figsize=(10, 8))
sns.heatmap(data=sampled_anomalous_data[['PM2.5', 'PM10', 'SO2', 'NO2', 'CO']])
plt.title('Heatmap of Pollutant Levels in Anomalous Data (Sampled)')
plt.show()
```

The heatmap depicts pollutant levels in a subset of data identified as anomalous, with rows representing individual instances and columns for various pollutants. The color scale, ranging from dark purple to bright orange, illustrates the intensity of pollutant levels, with orange indicating higher concentrations. The stark contrast across the rows, primarily in the deep purple range, suggests that most selected data points do not have extremely high pollutant levels. However, occasional streaks of orange and red reveal instances where one or more pollutants reach notably higher levels, warranting closer scrutiny for potential environmental health concerns.

## Threshold Tuning

In the realm of anomaly detection, particularly with methods like the Isolation Forest, the concept of threshold tuning is pivotal. The threshold determines the cutoff point at which a data point is classified as an anomaly. Tuning this threshold is a delicate balance, as it directly impacts the sensitivity of our anomaly detection.

### The Process of Threshold Tuning

Threshold tuning involves adjusting the parameters that define what we consider to be anomalous. In the case of the Isolation Forest, this often revolves around the `contamination` parameter, which represents the proportion of outliers we expect in the data.

```{python}
# Adjusting the contamination parameter
contamination_rate = 0.05  # Example rate
iso_forest = IsolationForest(contamination=contamination_rate)
iso_forest.fit(X_train)
```

In this code snippet, we adjust the `contamination` parameter, which dictates the model's sensitivity to anomalies. A higher contamination rate means the model will be more inclined to flag data points as anomalies.

### Impact on False Positives and False Negatives

The setting of the threshold has a direct impact on the trade-off between false positives (normal points incorrectly identified as anomalies) and false negatives (actual anomalies not detected).

- **Higher Threshold (Lower Contamination)**: This setting reduces the number of anomalies detected, potentially leading to more false negatives. While it ensures that the flagged anomalies are very likely to be true anomalies, it may miss some subtler, yet significant, anomalies.

- **Lower Threshold (Higher Contamination)**: Conversely, a lower threshold increases the sensitivity, potentially leading to more false positives. This setting might be useful in scenarios where missing an anomaly could have severe consequences, even if it means dealing with more false alarms.

### Balancing the Threshold

Finding the right balance for the threshold is crucial:

```{python}

def evaluate_model(model, X_test):
    # Predict anomalies
    predictions = model.predict(X_test)

    # Convert predictions to a more readable format: -1 (anomalies) to 1, 1 (normal) to 0
    predictions = np.where(predictions == -1, 1, 0)

    # Count and print the number of anomalies detected
    num_anomalies = np.sum(predictions)
    print(f"Number of anomalies detected: {num_anomalies} out of {len(X_test)} data points")

# Experimenting with different contamination rates
for rate in [0.01, 0.05, 0.1]:
    iso_forest = IsolationForest(contamination=rate)
    iso_forest.fit(X_train)
    # Evaluate the model
    evaluate_model(iso_forest, X_test)
    print("\n")
```

In our exploration of the optimal contamination rate for the Isolation Forest model, we experimented with various rates and observed their impact on anomaly detection in our dataset of 126,231 data points. When we set the contamination rate at 0.01, our model identified 1,293 anomalies, suggesting a more conservative approach to anomaly detection. Increasing the rate to 0.05 led to a significant rise in detected anomalies, totaling 6,356, indicating a moderate level of sensitivity. Further amplifying the rate to 0.1 resulted in the detection of 12,702 anomalies, reflecting a highly sensitive setting that captures a broader spectrum of potential anomalies. These varying results illustrate the crucial influence of the contamination rate on the model's behavior, underscoring the importance of fine-tuning this parameter to strike a balance between identifying true anomalies and avoiding excessive false positives. Our analysis highlights the need for a thoughtful approach to setting this threshold, considering both the nature of our data and the specific requirements of our air quality monitoring objectives.

## Interpretation and Real-World Implications
Based on our analysis of the air pollution data using the Isolation Forest model, we've uncovered some intriguing insights. Out of the total 126,231 data points, our model identified 12,510 as anomalies, accounting for approximately 9.91% of the dataset. This proportion of anomalies is significant and warrants further investigation.

### Interpretation of Detected Anomalies

When we delve into the sample of detected anomalies, several key observations emerge:

1. **Elevated Pollutant Levels**: Many of the anomalies exhibit unusually high levels of pollutants such as PM2.5, PM10, NO2, and CO. For instance, rows like 8468 and 314995 show pollutant concentrations several times higher than typical readings. This could indicate episodes of extreme pollution, possibly due to specific environmental events or human activities.

2. **Meteorological Influences**: The anomalies also reveal interesting patterns in meteorological conditions. For example, rows 214246 and 244058 show variations in temperature, pressure, and humidity, which could be influencing factors for the high pollution levels observed.

3. **Station-Specific Anomalies**: The data points flagged as anomalies are distributed across different monitoring stations, as seen in the 'station' columns. This distribution suggests that the detected anomalies are not confined to a specific location but are rather widespread, indicating a more systemic issue in air quality.

4. **Temporal Patterns**: The presence of anomalies across different years, months, and hours, such as in rows 216296 and 392113, hints at temporal patterns in air pollution. These patterns could be aligned with seasonal changes, urban activities, or policy changes affecting air quality.

### Implications

- **Environmental Policy and Health**: The identified anomalies are crucial for understanding the dynamics of air pollution. They can inform environmental policies, especially in devising strategies to mitigate high pollution episodes.

- **Further Research**: These findings can be a starting point for more detailed research. For example, investigating the causes behind high pollution episodes can help in understanding the impact of urban development, traffic patterns, or industrial activities on air quality.

- **Public Awareness**: Disseminating information about such high pollution episodes can raise public awareness and encourage preventive measures, especially for vulnerable populations.

In summary, our analysis using the Isolation Forest model provides us with valuable insights into the air quality data, highlighting instances of unusually high pollution levels. This information is crucial for environmental monitoring, policy-making, and public health initiatives.

## References 
- https://www.knowledgehut.com/blog/data-science/machine-learning-for-anomaly-detection 
- https://www.techtarget.com/searchenterpriseai/definition/anomaly-detection
- https://medium.com/@corymaklin/isolation-forest-799fceacdda4
- OpenAI. (2023). ChatGPT [Large language model]. https://chat.openai.com
- https://archive.ics.uci.edu/dataset/501/beijing+multi+site+air+quality+data