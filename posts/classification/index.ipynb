{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Classification\n",
        "author: Joanna Fang\n",
        "date: '2023-11-30'\n",
        "categories:\n",
        "  - ml\n",
        "  - code\n",
        "  - classification\n",
        "  - driving\n",
        "  - kaggle\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    code-block-bg: '#FFFFFF'\n",
        "    code-block-border-left: '#E83283'\n",
        "    code-tools:\n",
        "      source: true\n",
        "      toggle: false\n",
        "      caption: none\n",
        "---"
      ],
      "id": "ce0012a4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification: Predictive Analytics for Driving Behavior Classification\n",
        "\n",
        "![](thumbnail.jpg){width=\"50%\" fig-align=\"center\"}\n",
        "\n",
        "## Project Introduction\n",
        "\n",
        "Welcome to our exploration into the world of machine learning and its application in predicting driving behaviors. In this project, we dive into the realm of vehicular safety, aiming to leverage sensor data to classify driving patterns into three categories: SLOW, NORMAL, and AGGRESSIVE. This endeavor is not just a technical challenge but a crucial step towards enhancing road safety and reducing traffic accidents.\n",
        "\n",
        "### Objective\n",
        "The primary goal of this project is to accurately predict driving behaviors using data from commonly available sensors in smartphones. By analyzing accelerometer and gyroscope data, we aim to classify driving styles into SLOW, NORMAL, or AGGRESSIVE, contributing significantly to the prevention of road mishaps.\n",
        "\n",
        "### Importance and Applications\n",
        "The importance of this project is underscored by the alarming statistics from the AAA Foundation for Traffic Safety, highlighting that over half of fatal crashes involve aggressive driving actions. Through this project, we offer a scalable and readily deployable solution to monitor and predict dangerous driving behaviors, potentially saving lives and making our roads safer. Applications of this model extend to insurance companies for risk assessment, ride-sharing services for driver monitoring, and personal safety apps to alert drivers of their driving patterns.\n",
        "\n",
        "## Data Collection and Description\n",
        "\n",
        "### Source of the Data\n",
        "The dataset for this project is derived from a real-world scenario, specifically designed to capture driving behaviors. Utilizing a data collector application on Android devices, we have gathered sensor readings directly relevant to driving dynamics.\n",
        "\n",
        "### Dataset Description\n",
        "The dataset is a rich collection of sensor data recorded from a Samsung Galaxy S21, chosen for its advanced sensor capabilities. Here's a breakdown of the dataset features:\n",
        "\n",
        "#### Acceleration Data\n",
        "- Axes: X, Y, Z\n",
        "- Unit: Meters per second squared (m/s²)\n",
        "- Note: Gravitational acceleration has been filtered out to focus on the acceleration caused by driving actions.\n",
        "\n",
        "#### Rotation Data\n",
        "- Axes: X, Y, Z\n",
        "- Unit: Degrees per second (°/s)\n",
        "- Purpose: Captures the angular changes during driving, indicative of turns and maneuvers.\n",
        "\n",
        "#### Classification Label\n",
        "- Categories: SLOW, NORMAL, AGGRESSIVE\n",
        "- Basis: The driving behavior classification based on the sensor data patterns.\n",
        "\n",
        "#### Additional Information\n",
        "- Sampling Rate: 2 samples per second, ensuring a fine-grained capture of driving dynamics.\n",
        "- Timestamp: Included for each sample, allowing for temporal analysis of driving patterns.\n",
        "\n",
        "In the following sections, we will delve into the preprocessing, exploratory analysis, and modeling of this dataset to build a robust classifier for driving behaviors. Stay tuned as we unravel the insights hidden within this data and develop a machine learning model with the potential to make a real-world impact.\n",
        "\n",
        "## Data Preprocessing\n",
        "\n",
        "In the realm of machine learning, data preprocessing is a critical step in preparing raw data for modeling. Our datasets, `motion_data_1.csv` (test data) and `motion_data_2.csv` (train data), contain acceleration and rotation measurements alongside driving behavior classifications. Let's walk through the preprocessing steps:\n",
        "\n",
        "### Handling Missing Values\n",
        "\n",
        "First, we'll check for missing values in both datasets. Missing data can significantly impact the performance of a machine learning model. If missing values are found, strategies such as imputation or removal of the affected rows can be considered.\n"
      ],
      "id": "3893c7b5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the datasets\n",
        "test_data = pd.read_csv('motion_data_1.csv')\n",
        "train_data = pd.read_csv('motion_data_2.csv')\n",
        "\n",
        "# Checking for missing values in both datasets\n",
        "missing_values_test = test_data.isnull().sum()\n",
        "missing_values_train = train_data.isnull().sum()"
      ],
      "id": "f996f90c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalizing or Scaling the Data\n",
        "\n",
        "Normalization or scaling is crucial when dealing with sensor data. It ensures that each feature contributes proportionately to the final prediction. Given the different scales of acceleration (in m/s²) and rotation (in °/s), applying a scaling method like Min-Max scaling or Standardization is important.\n"
      ],
      "id": "4a1e5b9b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardizing the data\n",
        "scaler = StandardScaler()\n",
        "train_data_scaled = scaler.fit_transform(train_data.iloc[:, :-2]) # Excluding 'Class' and 'Timestamp' columns\n",
        "test_data_scaled = scaler.transform(test_data.iloc[:, :-2])"
      ],
      "id": "1af38702",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Engineering \n",
        "\n",
        "Feature engineering might involve creating new features or modifying existing ones to improve model performance. In our case, we might consider deriving features like the total magnitude of acceleration or rotation.\n"
      ],
      "id": "27868246"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "# Adding a feature: Total magnitude of acceleration\n",
        "train_data['TotalAcc'] = np.sqrt(train_data['AccX']**2 + train_data['AccY']**2 + train_data['AccZ']**2)\n",
        "test_data['TotalAcc'] = np.sqrt(test_data['AccX']**2 + test_data['AccY']**2 + test_data['AccZ']**2)"
      ],
      "id": "e0e12836",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory Data Analysis (EDA)\n",
        "\n",
        "### Statistical Summary of the Dataset\n",
        "\n",
        "Understanding the basic statistics of the dataset is essential. This includes measures like mean, median, standard deviation, etc., providing insights into the data distribution.\n"
      ],
      "id": "8f825cae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Descriptive statistics of the training data\n",
        "train_data_description = train_data.describe()"
      ],
      "id": "cf5dd348",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization of Data Distribution\n",
        "\n",
        "#### Histograms or Box Plots for Acceleration and Rotation Data\n",
        "\n",
        "Histograms and box plots are effective for visualizing the distribution of sensor data and identifying outliers or skewness in the data.\n"
      ],
      "id": "f1505b81"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Setting up the figure for multiple subplots\n",
        "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 15))\n",
        "fig.suptitle('Histograms of Acceleration and Rotation Data', fontsize=16)\n",
        "\n",
        "# Plotting histograms for each sensor data column\n",
        "sns.histplot(train_data['AccX'], kde=True, ax=axes[0, 0], color='skyblue')\n",
        "axes[0, 0].set_title('Acceleration in X-axis (AccX)')\n",
        "\n",
        "sns.histplot(train_data['AccY'], kde=True, ax=axes[0, 1], color='olive')\n",
        "axes[0, 1].set_title('Acceleration in Y-axis (AccY)')\n",
        "\n",
        "sns.histplot(train_data['AccZ'], kde=True, ax=axes[1, 0], color='gold')\n",
        "axes[1, 0].set_title('Acceleration in Z-axis (AccZ)')\n",
        "\n",
        "sns.histplot(train_data['GyroX'], kde=True, ax=axes[1, 1], color='teal')\n",
        "axes[1, 1].set_title('Rotation in X-axis (GyroX)')\n",
        "\n",
        "sns.histplot(train_data['GyroY'], kde=True, ax=axes[2, 0], color='salmon')\n",
        "axes[2, 0].set_title('Rotation in Y-axis (GyroY)')\n",
        "\n",
        "sns.histplot(train_data['GyroZ'], kde=True, ax=axes[2, 1], color='violet')\n",
        "axes[2, 1].set_title('Rotation in Z-axis (GyroZ)')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "id": "2bf0065f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **Acceleration (AccX, AccY, AccZ)**: The distributions for acceleration on all three axes appear to be roughly bell-shaped, indicating that most of the readings are clustered around the mean, with fewer readings at the extreme ends. This suggests normal driving conditions with occasional variances that could indicate moments of acceleration or deceleration.\n",
        "\n",
        "2. **Rotation (GyroX, GyroY, GyroZ)**: The rotation data histograms show a similar bell-shaped distribution, especially for the X and Y axes, indicating consistent turning behavior with some outliers potentially representing more aggressive turns or corrections. The GyroZ histogram is notably narrower, which might suggest that rotation around the Z-axis (often corresponding to yaw movements) is less variable during normal driving conditions.\n",
        "\n",
        "#### Correlation Heatmaps\n",
        "\n",
        "A correlation heatmap helps in understanding the relationships between different sensor readings. It's crucial for identifying features that are highly correlated and might need to be addressed during feature selection.\n"
      ],
      "id": "48391588"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# For the correlation heatmap, we need to exclude non-numeric columns (Class and Timestamp)\n",
        "train_data_numeric = train_data.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Generating the correlation heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(train_data_numeric.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap of Numeric Features')\n",
        "plt.show()"
      ],
      "id": "6ad6149a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this heatmap:\n",
        "\n",
        "- Values close to 1 or -1 indicate a strong positive or negative correlation, respectively.\n",
        "- Values close to 0 suggest no linear correlation between the variables.\n",
        "\n",
        "It appears that:\n",
        "\n",
        "- There are no exceptionally strong correlations between the different axes of acceleration and rotation, which is desirable in a dataset used for behavior prediction as it indicates that the sensor readings provide unique information.\n",
        "- The strongest negative correlation is observed between GyroZ and AccX, which might suggest that certain types of aggressive driving behaviors cause inverse changes in these two measurements.\n",
        "\n",
        "The absence of very high correlations means that there may not be redundant features in the dataset, which is good for a machine learning model that relies on diverse data points to make predictions. However, the subtle correlations that do exist can still provide valuable insights when developing features and training models.\n",
        "\n",
        "These visualizations and their interpretations are key in understanding the underlying patterns in the driving behavior dataset, which will inform the feature selection and modeling phases of the machine learning project.\n",
        "\n",
        "In the next sections, we will delve into model selection, training, and evaluation, using the insights and data preparations we've just discussed. Stay tuned!\n",
        "\n",
        "## Data Preparation for Modeling\n",
        "\n",
        "Preparing the data for modeling is a crucial step in the machine learning pipeline. It ensures that the data fed into the model is clean, representative, and well-formatted.\n",
        "\n",
        "### Splitting Data into Training and Testing Sets\n",
        "\n",
        "We have two datasets: `train_data` and `test_data`, pre-split for our convenience. Typically, we would use a function like `train_test_split` from `scikit-learn` to divide our dataset into a training set and a test set, ensuring that both sets are representative of the overall distribution. However, in this scenario, that step is already accounted for.\n"
      ],
      "id": "2d1f25b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Since the data is pre-split, this step is not required for our current workflow.\n",
        "# Normally, we would do something like this:\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "id": "d3476fa6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handling Class Imbalance\n",
        "\n",
        "Class imbalance can significantly skew the performance of a classifier towards the majority class. It's important to check the balance of classes and apply techniques such as resampling if necessary.\n"
      ],
      "id": "ba6ab82f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Checking the balance of the classes\n",
        "class_counts = train_data['Class'].value_counts()\n",
        "\n",
        "# If imbalance is found, we might consider resampling strategies like:\n",
        "# - Upsampling minority classes\n",
        "# - Downsampling majority classes\n",
        "# - Using SMOTE (Synthetic Minority Over-sampling Technique)"
      ],
      "id": "6fe965e3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Selection\n",
        "\n",
        "Selecting the right model is about finding the balance between prediction accuracy, computational efficiency, and the ease of interpretation.\n",
        "\n",
        "### Overview of Potential Machine Learning Models for Classification\n",
        "\n",
        "For our classification task, we have several models at our disposal:\n",
        "\n",
        "- **Decision Tree**: A good baseline that is easy to interpret.\n",
        "- **Random Forest**: An ensemble method that can improve on the performance of a single decision tree.\n",
        "- **Support Vector Machine (SVM)**: Effective in high-dimensional spaces.\n",
        "- **Neural Networks**: Potentially high performance but may require more data and compute resources.\n",
        "\n",
        "### Criteria for Model Selection\n",
        "\n",
        "When selecting the model, we consider several criteria:\n",
        "\n",
        "- **Accuracy**: How often the model makes the correct prediction.\n",
        "- **Speed**: How quickly the model can be trained and used for prediction.\n",
        "- **Interpretability**: The ease with which we can understand the model's predictions.\n",
        "\n",
        "## Model Training\n",
        "\n",
        "Training our models is like teaching them to understand patterns within the data that distinguish between SLOW, NORMAL, and AGGRESSIVE driving behaviors. We will employ four different machine learning models to find the best predictor.\n",
        "\n",
        "### Training Different Models\n",
        "\n",
        "We start by training different types of models. Each has its own strengths and might capture different aspects of the driving behavior.\n"
      ],
      "id": "4371a58a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Initializing models with default parameters\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "random_forest = RandomForestClassifier()\n",
        "svm = SVC()\n",
        "neural_network = MLPClassifier()\n",
        "\n",
        "decision_tree.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\n",
        "random_forest.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\n",
        "svm.fit(train_data.drop(['Class'], axis=1), train_data['Class']) \n",
        "neural_network.fit(train_data.drop(['Class'], axis=1), train_data['Class'])"
      ],
      "id": "080f418d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `fit` method allows the models to learn from the training data. It's during this process that they identify which features are most important for predicting driving behavior.\n",
        "\n",
        "### Hyperparameter Tuning\n",
        "\n",
        "To optimize our models, we tweak their settings, known as hyperparameters. This process is akin to fine-tuning an instrument to ensure it plays the perfect note.\n"
      ],
      "id": "82ca8d07"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Setting up the parameter grid for a Decision Tree\n",
        "param_grid = {\n",
        "    'decision_tree': {\n",
        "        'max_depth': [10, 20, 30],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Hyperparameter tuning for the Decision Tree using GridSearchCV\n",
        "grid_search_dt = GridSearchCV(decision_tree, param_grid['decision_tree'], cv=5)\n",
        "grid_search_dt.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\n",
        "best_params_dt = grid_search_dt.best_params_\n",
        "\n",
        "# Define the parameter grid for Random Forest\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize the GridSearchCV object for Random Forest\n",
        "grid_search_rf = GridSearchCV(RandomForestClassifier(), rf_param_grid, cv=5, n_jobs=-1)\n",
        "grid_search_rf.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\n",
        "\n",
        "# Get the best parameters for Random Forest\n",
        "best_params_rf = grid_search_rf.best_params_\n",
        "\n",
        "# Define the parameter grid for SVM\n",
        "svm_param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'kernel': ['rbf', 'linear']\n",
        "}\n",
        "\n",
        "# Initialize the GridSearchCV object for SVM\n",
        "grid_search_svm = GridSearchCV(SVC(), svm_param_grid, cv=5, n_jobs=-1)\n",
        "grid_search_svm.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\n",
        "\n",
        "# Get the best parameters for SVM\n",
        "best_params_svm = grid_search_svm.best_params_\n",
        "\n",
        "# Define the parameter grid for Neural Network\n",
        "nn_param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'max_iter': [200, 400]\n",
        "}\n",
        "\n",
        "# Initialize the GridSearchCV object for Neural Network\n",
        "grid_search_nn = GridSearchCV(MLPClassifier(), nn_param_grid, cv=5, n_jobs=-1)\n",
        "grid_search_nn.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\n",
        "\n",
        "# Get the best parameters for Neural Network\n",
        "best_params_nn = grid_search_nn.best_params_\n",
        "\n",
        "# Now we can print or use the best_params_rf, best_params_svm, and best_params_nn for the best-found parameters"
      ],
      "id": "e73d6be4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using `GridSearchCV`, we systematically work through multiple combinations of parameter tunes, cross-validating as we go to determine which tune gives the best performance.\n",
        "\n",
        "By the end of these steps, we will have trained four different models and optimized their hyperparameters for best performance. This careful tuning is what sets the stage for the final step: evaluating these models to select the ultimate candidate for our driving behavior prediction task.\n"
      ],
      "id": "bf4fbec2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# After the GridSearchCV process is complete, we can print out the best parameters for each model\n",
        "\n",
        "# Best parameters for Decision Tree\n",
        "print(\"Best parameters for Decision Tree:\")\n",
        "print(best_params_dt)\n",
        "\n",
        "# Best parameters for Random Forest\n",
        "print(\"Best parameters for Random Forest:\")\n",
        "print(best_params_rf)\n",
        "\n",
        "# Best parameters for SVM\n",
        "print(\"Best parameters for SVM:\")\n",
        "print(best_params_svm)\n",
        "\n",
        "# Best parameters for Neural Network\n",
        "print(\"Best parameters for Neural Network:\")\n",
        "print(best_params_nn)\n",
        "\n",
        "# These parameters can then be used to initialize the respective models with the optimal settings\n",
        "decision_tree_optimized = DecisionTreeClassifier(**best_params_dt)\n",
        "random_forest_optimized = RandomForestClassifier(**best_params_rf)\n",
        "svm_optimized = SVC(**best_params_svm)\n",
        "neural_network_optimized = MLPClassifier(**best_params_nn)\n",
        "\n",
        "# If you wish to proceed with fitting the optimized models, you would then do:\n",
        "decision_tree_optimized.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\n",
        "random_forest_optimized.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\n",
        "svm_optimized.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\n",
        "neural_network_optimized.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\n",
        "\n",
        "# Remember to replace train_data.drop(['Class'], axis=1) and train_data['Class']\n",
        "# with your features and target variable if they are named differently."
      ],
      "id": "e07b9eed",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}