---
title: "Classification"
author: "Joanna Fang"
date: "2023-11-30"
categories: [ml, code, classification, driving, kaggle]
jupyter: python3
format:
  html: 
    toc: true
    code-block-bg: "#FFFFFF"
    code-block-border-left: "#E83283"
    code-tools:
      source: true
      toggle: false
      caption: none

---

# Classification: Predictive Analytics for Driving Behavior Classification

![](thumbnail.jpg){width="50%" fig-align="center"}

## Project Introduction

Welcome to our exploration into the world of machine learning and its application in predicting driving behaviors. In this project, we dive into the realm of vehicular safety, aiming to leverage sensor data to classify driving patterns into three categories: SLOW, NORMAL, and AGGRESSIVE. This endeavor is not just a technical challenge but a crucial step towards enhancing road safety and reducing traffic accidents.

### Objective
The primary goal of this project is to accurately predict driving behaviors using data from commonly available sensors in smartphones. By analyzing accelerometer and gyroscope data, we aim to classify driving styles into SLOW, NORMAL, or AGGRESSIVE, contributing significantly to the prevention of road mishaps.

### Importance and Applications
The importance of this project is underscored by the alarming statistics from the AAA Foundation for Traffic Safety, highlighting that over half of fatal crashes involve aggressive driving actions. Through this project, we offer a scalable and readily deployable solution to monitor and predict dangerous driving behaviors, potentially saving lives and making our roads safer. Applications of this model extend to insurance companies for risk assessment, ride-sharing services for driver monitoring, and personal safety apps to alert drivers of their driving patterns.

## Data Collection and Description

### Source of the Data
The dataset for this project is derived from a real-world scenario, specifically designed to capture driving behaviors. Utilizing a data collector application on Android devices, we have gathered sensor readings directly relevant to driving dynamics.

### Dataset Description
The dataset is a rich collection of sensor data recorded from a Samsung Galaxy S21, chosen for its advanced sensor capabilities. Here's a breakdown of the dataset features:

#### Acceleration Data
- Axes: X, Y, Z
- Unit: Meters per second squared (m/s²)
- Note: Gravitational acceleration has been filtered out to focus on the acceleration caused by driving actions.

#### Rotation Data
- Axes: X, Y, Z
- Unit: Degrees per second (°/s)
- Purpose: Captures the angular changes during driving, indicative of turns and maneuvers.

#### Classification Label
- Categories: SLOW, NORMAL, AGGRESSIVE
- Basis: The driving behavior classification based on the sensor data patterns.

#### Additional Information
- Sampling Rate: 2 samples per second, ensuring a fine-grained capture of driving dynamics.
- Timestamp: Included for each sample, allowing for temporal analysis of driving patterns.

In the following sections, we will delve into the preprocessing, exploratory analysis, and modeling of this dataset to build a robust classifier for driving behaviors. Stay tuned as we unravel the insights hidden within this data and develop a machine learning model with the potential to make a real-world impact.

## Data Preprocessing

In the realm of machine learning, data preprocessing is a critical step in preparing raw data for modeling. Our datasets, `motion_data_1.csv` (test data) and `motion_data_2.csv` (train data), contain acceleration and rotation measurements alongside driving behavior classifications. Let's walk through the preprocessing steps:

### Handling Missing Values

First, we'll check for missing values in both datasets. Missing data can significantly impact the performance of a machine learning model. If missing values are found, strategies such as imputation or removal of the affected rows can be considered.

```{python}
import pandas as pd

# Load the datasets
test_data = pd.read_csv('motion_data_1.csv')
train_data = pd.read_csv('motion_data_2.csv')

# Checking for missing values in both datasets
missing_values_test = test_data.isnull().sum()
missing_values_train = train_data.isnull().sum()
```

### Normalizing or Scaling the Data

Normalization or scaling is crucial when dealing with sensor data. It ensures that each feature contributes proportionately to the final prediction. Given the different scales of acceleration (in m/s²) and rotation (in °/s), applying a scaling method like Min-Max scaling or Standardization is important.

```{python}
from sklearn.preprocessing import StandardScaler

# Standardizing the data
scaler = StandardScaler()
train_data_scaled = scaler.fit_transform(train_data.iloc[:, :-2]) # Excluding 'Class' and 'Timestamp' columns
test_data_scaled = scaler.transform(test_data.iloc[:, :-2])
```

### Feature Engineering 

Feature engineering might involve creating new features or modifying existing ones to improve model performance. In our case, we might consider deriving features like the total magnitude of acceleration or rotation.

```{python}
import numpy as np

# Adding a feature: Total magnitude of acceleration
train_data['TotalAcc'] = np.sqrt(train_data['AccX']**2 + train_data['AccY']**2 + train_data['AccZ']**2)
test_data['TotalAcc'] = np.sqrt(test_data['AccX']**2 + test_data['AccY']**2 + test_data['AccZ']**2)
```

## Exploratory Data Analysis (EDA)

### Statistical Summary of the Dataset

Understanding the basic statistics of the dataset is essential. This includes measures like mean, median, standard deviation, etc., providing insights into the data distribution.

```{python}
# Descriptive statistics of the training data
train_data_description = train_data.describe()
```

### Visualization of Data Distribution

#### Histograms or Box Plots for Acceleration and Rotation Data

Histograms and box plots are effective for visualizing the distribution of sensor data and identifying outliers or skewness in the data.

```{python}
import matplotlib.pyplot as plt
import seaborn as sns

# Setting up the figure for multiple subplots
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 15))
fig.suptitle('Histograms of Acceleration and Rotation Data', fontsize=16)

# Plotting histograms for each sensor data column
sns.histplot(train_data['AccX'], kde=True, ax=axes[0, 0], color='skyblue')
axes[0, 0].set_title('Acceleration in X-axis (AccX)')

sns.histplot(train_data['AccY'], kde=True, ax=axes[0, 1], color='olive')
axes[0, 1].set_title('Acceleration in Y-axis (AccY)')

sns.histplot(train_data['AccZ'], kde=True, ax=axes[1, 0], color='gold')
axes[1, 0].set_title('Acceleration in Z-axis (AccZ)')

sns.histplot(train_data['GyroX'], kde=True, ax=axes[1, 1], color='teal')
axes[1, 1].set_title('Rotation in X-axis (GyroX)')

sns.histplot(train_data['GyroY'], kde=True, ax=axes[2, 0], color='salmon')
axes[2, 0].set_title('Rotation in Y-axis (GyroY)')

sns.histplot(train_data['GyroZ'], kde=True, ax=axes[2, 1], color='violet')
axes[2, 1].set_title('Rotation in Z-axis (GyroZ)')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()
```

1. **Acceleration (AccX, AccY, AccZ)**: The distributions for acceleration on all three axes appear to be roughly bell-shaped, indicating that most of the readings are clustered around the mean, with fewer readings at the extreme ends. This suggests normal driving conditions with occasional variances that could indicate moments of acceleration or deceleration.

2. **Rotation (GyroX, GyroY, GyroZ)**: The rotation data histograms show a similar bell-shaped distribution, especially for the X and Y axes, indicating consistent turning behavior with some outliers potentially representing more aggressive turns or corrections. The GyroZ histogram is notably narrower, which might suggest that rotation around the Z-axis (often corresponding to yaw movements) is less variable during normal driving conditions.

#### Correlation Heatmaps

A correlation heatmap helps in understanding the relationships between different sensor readings. It's crucial for identifying features that are highly correlated and might need to be addressed during feature selection.

```{python}
# For the correlation heatmap, we need to exclude non-numeric columns (Class and Timestamp)
train_data_numeric = train_data.select_dtypes(include=['float64', 'int64'])

# Generating the correlation heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(train_data_numeric.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap of Numeric Features')
plt.show()

```

In this heatmap:

- Values close to 1 or -1 indicate a strong positive or negative correlation, respectively.
- Values close to 0 suggest no linear correlation between the variables.

It appears that:

- There are no exceptionally strong correlations between the different axes of acceleration and rotation, which is desirable in a dataset used for behavior prediction as it indicates that the sensor readings provide unique information.
- The strongest negative correlation is observed between GyroZ and AccX, which might suggest that certain types of aggressive driving behaviors cause inverse changes in these two measurements.

The absence of very high correlations means that there may not be redundant features in the dataset, which is good for a machine learning model that relies on diverse data points to make predictions. However, the subtle correlations that do exist can still provide valuable insights when developing features and training models.

These visualizations and their interpretations are key in understanding the underlying patterns in the driving behavior dataset, which will inform the feature selection and modeling phases of the machine learning project.

In the next sections, we will delve into model selection, training, and evaluation, using the insights and data preparations we've just discussed. Stay tuned!

## Data Preparation for Modeling

Preparing the data for modeling is a crucial step in the machine learning pipeline. It ensures that the data fed into the model is clean, representative, and well-formatted.

### Splitting Data into Training and Testing Sets

We have two datasets: `train_data` and `test_data`, pre-split for our convenience. Typically, we would use a function like `train_test_split` from `scikit-learn` to divide our dataset into a training set and a test set, ensuring that both sets are representative of the overall distribution. However, in this scenario, that step is already accounted for.

```{python}
# Since the data is pre-split, this step is not required for our current workflow.
# Normally, we would do something like this:
# from sklearn.model_selection import train_test_split
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
```

### Handling Class Imbalance

Class imbalance can significantly skew the performance of a classifier towards the majority class. It's important to check the balance of classes and apply techniques such as resampling if necessary.

```{python}
# Checking the balance of the classes
class_counts = train_data['Class'].value_counts()

# If imbalance is found, we might consider resampling strategies like:
# - Upsampling minority classes
# - Downsampling majority classes
# - Using SMOTE (Synthetic Minority Over-sampling Technique)
```

## Model Selection

Selecting the right model is about finding the balance between prediction accuracy, computational efficiency, and the ease of interpretation.

### Overview of Potential Machine Learning Models for Classification

For our classification task, we have several models at our disposal:

- **Decision Tree**: A good baseline that is easy to interpret.
- **Random Forest**: An ensemble method that can improve on the performance of a single decision tree.
- **Support Vector Machine (SVM)**: Effective in high-dimensional spaces.
- **Neural Networks**: Potentially high performance but may require more data and compute resources.

### Criteria for Model Selection

When selecting the model, we consider several criteria:

- **Accuracy**: How often the model makes the correct prediction.
- **Speed**: How quickly the model can be trained and used for prediction.
- **Interpretability**: The ease with which we can understand the model's predictions.

## Model Training

Training the model involves feeding it the prepared data and allowing it to learn from it.

### Training Different Models

We will experiment with different machine learning algorithms to find the best performer for our specific task.

```{python}
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier

# Example of training a Decision Tree classifier
decision_tree_model = DecisionTreeClassifier()
decision_tree_model.fit(train_data.drop(['Class'], axis=1), train_data['Class'])

# We would repeat the fitting process for each of the classifiers we wish to evaluate.
```

### Hyperparameter Tuning Using Techniques Like GridSearchCV or RandomSearchCV

Hyperparameter tuning is essential to optimize the model performance.

```{python}
from sklearn.model_selection import GridSearchCV

# Example of using GridSearchCV with a Decision Tree classifier
parameters = {'max_depth': [10, 20, 30], 'min_samples_leaf': [1, 2, 4]}
clf = GridSearchCV(decision_tree_model, parameters)
clf.fit(train_data.drop(['Class'], axis=1), train_data['Class'])

# The best parameters from GridSearchCV can then be used to train the final model.
```

In the next sections, we will evaluate our trained models, compare their performances, and finally, select the best model for our driving behavior classification task. Stay tuned for the results!

## Model Evaluation

Once models are trained, the next step is to evaluate their performance. This is crucial to understand how well our models are likely to perform when making predictions on new, unseen data.

### Evaluating Model Performance

We use a variety of metrics to get a holistic view of our model's performance:

- **Accuracy**: This is the proportion of true results among the total number of cases examined.
- **Precision**: Precision is the ratio of true positive predictions to the total positive predictions (including false positives).
- **Recall**: This is the ratio of true positive predictions to the total actual positives.
- **F1-Score**: The F1-score is the harmonic mean of precision and recall, providing a balance between them.

Each metric provides different insights into the strengths and weaknesses of our models.

```{python}
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Predictions using one of the trained models, for example, the Decision Tree
predictions = decision_tree_model.predict(test_data.drop(['Class'], axis=1))

# Calculation of performance metrics
accuracy = accuracy_score(test_data['Class'], predictions)
precision = precision_score(test_data['Class'], predictions, average='weighted')
recall = recall_score(test_data['Class'], predictions, average='weighted')
f1 = f1_score(test_data['Class'], predictions, average='weighted')

# Display the calculated metrics
print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-Score: {f1}")
```

### Confusion Matrix

A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known.

```{python}
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Generating the confusion matrix for the Decision Tree model
conf_mat = confusion_matrix(test_data['Class'], predictions)

# Visualizing the confusion matrix
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.show()
```

### ROC Curve and AUC

For binary classification problems, the ROC (Receiver Operating Characteristic) curve and AUC (Area Under the Curve) are used. However, they can be extended to multi-class classification.

```{python}
from sklearn.metrics import roc_auc_score

# Assuming a binary classification for simplicity. In multi-class cases, we would need to adjust the approach.
# auc = roc_auc_score(test_data['Class'], predictions, multi_class='ovr')
```

## Model Comparison and Selection

After evaluating all models, we compare their performance to select the best one.

### Comparing the Performance of Different Models

We compare the models based on the evaluation metrics we computed. This comparison is typically visualized using bar charts or tables summarizing the performance of each model.

### Selecting the Best Model

The best model is chosen based on the evaluation metrics. The selection also considers the specific use case requirements like interpretability, computational efficiency, and ease of deployment.

```{python}
# Assuming we have stored all our performance metrics in a DataFrame
models_performance = pd.DataFrame({
    'Model': ['Decision Tree', 'Random Forest', 'SVM', 'Neural Network'],
    'Accuracy': [dt_accuracy, rf_accuracy, svm_accuracy, nn_accuracy],
    # Other metrics can be added similarly
})

# Visualizing model performance for comparison
sns.barplot(x='Accuracy', y='Model', data=models_performance)
plt.title('Model Comparison based on Accuracy')
plt.show()
```

The selected model will then be the one we use for making predictions on new data, with the confidence that it has performed well across our evaluation metrics during the testing phase.