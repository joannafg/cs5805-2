<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Joanna Fang">
<meta name="dcterms.date" content="2023-11-29">

<title>Ziming’s Journey in Machine Learning - 3. Linear and Nonlinear Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Ziming’s Journey in Machine Learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/joannafg/cs5805-2" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/joanna-fang-6122ba182/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Linear and Nonlinear Regression</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i></button></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">ml</div>
                <div class="quarto-category">code</div>
                <div class="quarto-category">linear regression</div>
                <div class="quarto-category">nonlinear regression</div>
                <div class="quarto-category">pollution</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Joanna Fang </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 29, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#predicting-air-pollutant-concentrations-using-linear-and-random-forest-regression-a-jupyter-notebook-guide" id="toc-predicting-air-pollutant-concentrations-using-linear-and-random-forest-regression-a-jupyter-notebook-guide" class="nav-link active" data-scroll-target="#predicting-air-pollutant-concentrations-using-linear-and-random-forest-regression-a-jupyter-notebook-guide">Predicting Air Pollutant Concentrations Using Linear and Random Forest Regression: A Jupyter Notebook Guide</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link" data-scroll-target="#linear-regression">Linear Regression</a></li>
  <li><a href="#random-forest-regression" id="toc-random-forest-regression" class="nav-link" data-scroll-target="#random-forest-regression">Random Forest Regression</a></li>
  </ul></li>
  <li><a href="#understanding-the-dataset" id="toc-understanding-the-dataset" class="nav-link" data-scroll-target="#understanding-the-dataset">1. Understanding the Dataset</a>
  <ul class="collapse">
  <li><a href="#dataset-overview" id="toc-dataset-overview" class="nav-link" data-scroll-target="#dataset-overview">Dataset Overview</a></li>
  <li><a href="#column-descriptions" id="toc-column-descriptions" class="nav-link" data-scroll-target="#column-descriptions">Column Descriptions</a></li>
  <li><a href="#initial-insights-and-challenges" id="toc-initial-insights-and-challenges" class="nav-link" data-scroll-target="#initial-insights-and-challenges">Initial Insights and Challenges</a></li>
  </ul></li>
  <li><a href="#selecting-target-pollutants" id="toc-selecting-target-pollutants" class="nav-link" data-scroll-target="#selecting-target-pollutants">2. Selecting Target Pollutants</a>
  <ul class="collapse">
  <li><a href="#criteria-for-selecting-target-pollutants" id="toc-criteria-for-selecting-target-pollutants" class="nav-link" data-scroll-target="#criteria-for-selecting-target-pollutants">Criteria for Selecting Target Pollutants</a></li>
  <li><a href="#rationale-behind-the-selection" id="toc-rationale-behind-the-selection" class="nav-link" data-scroll-target="#rationale-behind-the-selection">Rationale Behind the Selection</a></li>
  </ul></li>
  <li><a href="#data-cleaning-and-transformation" id="toc-data-cleaning-and-transformation" class="nav-link" data-scroll-target="#data-cleaning-and-transformation">3. Data Cleaning and Transformation</a>
  <ul class="collapse">
  <li><a href="#identifying-and-handling-missing-or-inconsistent-data" id="toc-identifying-and-handling-missing-or-inconsistent-data" class="nav-link" data-scroll-target="#identifying-and-handling-missing-or-inconsistent-data">Identifying and Handling Missing or Inconsistent Data</a></li>
  <li><a href="#normalization-or-standardization-of-data" id="toc-normalization-or-standardization-of-data" class="nav-link" data-scroll-target="#normalization-or-standardization-of-data">Normalization or Standardization of Data</a></li>
  <li><a href="#transforming-categorical-data-into-a-usable-format" id="toc-transforming-categorical-data-into-a-usable-format" class="nav-link" data-scroll-target="#transforming-categorical-data-into-a-usable-format">Transforming Categorical Data into a Usable Format</a></li>
  <li><a href="#visuals-showing-before-and-after-data-transformation" id="toc-visuals-showing-before-and-after-data-transformation" class="nav-link" data-scroll-target="#visuals-showing-before-and-after-data-transformation">Visuals Showing Before and After Data Transformation</a></li>
  </ul></li>
  <li><a href="#correlation-analysis-and-multicollinearity-check" id="toc-correlation-analysis-and-multicollinearity-check" class="nav-link" data-scroll-target="#correlation-analysis-and-multicollinearity-check">4. Correlation Analysis and Multicollinearity Check</a>
  <ul class="collapse">
  <li><a href="#correlation-analysis-and-its-importance" id="toc-correlation-analysis-and-its-importance" class="nav-link" data-scroll-target="#correlation-analysis-and-its-importance">Correlation Analysis and Its Importance</a></li>
  <li><a href="#multicollinearity-check-and-its-implications" id="toc-multicollinearity-check-and-its-implications" class="nav-link" data-scroll-target="#multicollinearity-check-and-its-implications">Multicollinearity Check and Its Implications</a></li>
  <li><a href="#visual-representation-of-correlation-and-multicollinearity-findings" id="toc-visual-representation-of-correlation-and-multicollinearity-findings" class="nav-link" data-scroll-target="#visual-representation-of-correlation-and-multicollinearity-findings">Visual Representation of Correlation and Multicollinearity Findings</a></li>
  <li><a href="#feature-selection" id="toc-feature-selection" class="nav-link" data-scroll-target="#feature-selection">Feature Selection</a></li>
  </ul></li>
  <li><a href="#linear-regression-analysis" id="toc-linear-regression-analysis" class="nav-link" data-scroll-target="#linear-regression-analysis">5. Linear Regression Analysis</a>
  <ul class="collapse">
  <li><a href="#introduction-to-linear-regression-and-its-applicability" id="toc-introduction-to-linear-regression-and-its-applicability" class="nav-link" data-scroll-target="#introduction-to-linear-regression-and-its-applicability">Introduction to Linear Regression and Its Applicability</a></li>
  <li><a href="#step-by-step-linear-regression-analysis-using-jupyter-notebook" id="toc-step-by-step-linear-regression-analysis-using-jupyter-notebook" class="nav-link" data-scroll-target="#step-by-step-linear-regression-analysis-using-jupyter-notebook">Step-by-Step Linear Regression Analysis Using Jupyter Notebook</a></li>
  <li><a href="#visual-representation-of-linear-regression-results-and-plotting-the-best-fit-line" id="toc-visual-representation-of-linear-regression-results-and-plotting-the-best-fit-line" class="nav-link" data-scroll-target="#visual-representation-of-linear-regression-results-and-plotting-the-best-fit-line">Visual Representation of Linear Regression Results and Plotting the Best Fit Line</a></li>
  <li><a href="#evaluating-the-performance-of-the-linear-regression-model" id="toc-evaluating-the-performance-of-the-linear-regression-model" class="nav-link" data-scroll-target="#evaluating-the-performance-of-the-linear-regression-model">Evaluating the Performance of the Linear Regression Model</a></li>
  </ul></li>
  <li><a href="#random-forest-regression-analysis" id="toc-random-forest-regression-analysis" class="nav-link" data-scroll-target="#random-forest-regression-analysis">6. Random Forest Regression Analysis</a>
  <ul class="collapse">
  <li><a href="#introduction-to-random-forest-regression" id="toc-introduction-to-random-forest-regression" class="nav-link" data-scroll-target="#introduction-to-random-forest-regression">Introduction to Random Forest Regression</a></li>
  <li><a href="#implementing-random-forest-regression" id="toc-implementing-random-forest-regression" class="nav-link" data-scroll-target="#implementing-random-forest-regression">Implementing Random Forest Regression</a></li>
  <li><a href="#visualization-feature-importance-and-prediction-vs-actual" id="toc-visualization-feature-importance-and-prediction-vs-actual" class="nav-link" data-scroll-target="#visualization-feature-importance-and-prediction-vs-actual">Visualization: Feature Importance and Prediction vs Actual</a></li>
  </ul></li>
  <li><a href="#comparative-analysis-and-conclusion" id="toc-comparative-analysis-and-conclusion" class="nav-link" data-scroll-target="#comparative-analysis-and-conclusion">Comparative Analysis and Conclusion</a>
  <ul class="collapse">
  <li><a href="#visual-comparison-prediction-vs-actual-plot-for-both-models" id="toc-visual-comparison-prediction-vs-actual-plot-for-both-models" class="nav-link" data-scroll-target="#visual-comparison-prediction-vs-actual-plot-for-both-models">Visual Comparison: Prediction vs Actual Plot for Both Models</a></li>
  <li><a href="#limitation" id="toc-limitation" class="nav-link" data-scroll-target="#limitation">Limitation</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/joannafg/cs5805-2/edit/main/posts/linear/index.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/joannafg/cs5805-2/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="predicting-air-pollutant-concentrations-using-linear-and-random-forest-regression-a-jupyter-notebook-guide" class="level1">
<h1>Predicting Air Pollutant Concentrations Using Linear and Random Forest Regression: A Jupyter Notebook Guide</h1>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="thumbnail.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Air quality is a critical environmental factor impacting public health, ecosystem sustainability, and the global climate. Pollutants such as particulate matter (PM2.5 and PM10), sulfur dioxide (SO2), nitrogen dioxide (NO2), carbon monoxide (CO), and ozone (O3) can have severe health impacts, including respiratory and cardiovascular diseases. Understanding and predicting the concentrations of these pollutants is essential for creating effective environmental policies and public health interventions.</p>
<p>In this blog, we’ll delve into two powerful statistical methods used in predicting air pollutant concentrations: linear regression and Random Forest regression.</p>
<section id="linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="linear-regression">Linear Regression</h3>
<p>Linear regression is a fundamental statistical approach used to model the relationship between a dependent variable and one or more independent variables. In the context of air quality, it helps us understand how various environmental factors like temperature, humidity, and wind speed influence pollutant levels. The model assumes a linear relationship between the variables, which can be represented as:</p>
<p><span class="math display">\[
Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n + \epsilon
\]</span></p>
<p>Here, ( Y ) is the pollutant concentration we want to predict, ( X_1, X_2, …, X_n ) are the environmental factors, ( _0, _1, …, _n ) are the coefficients to be estimated, and ( ) is the error term.</p>
</section>
<section id="random-forest-regression" class="level3">
<h3 class="anchored" data-anchor-id="random-forest-regression">Random Forest Regression</h3>
<p>Random Forest, on the other hand, is a type of ensemble learning method, particularly useful for non-linear relationships. It operates by constructing multiple decision trees during training and outputting the mean prediction of the individual trees. This method is beneficial for handling complex interactions between variables and can provide more accurate predictions for complex datasets like those in air quality studies.</p>
<p>The purpose of this blog is to provide a step-by-step guide on how to use these methods, utilizing a Jupyter Notebook, to predict pollutant concentrations. We’ll start by exploring our dataset, <code>air_data_all.csv</code>, which includes a variety of environmental conditions and temporal factors, and then apply these regression techniques to gain insights into the factors affecting air quality.</p>
<p>By the end of this blog, you’ll have a clearer understanding of how to implement these techniques in Python and interpret their results, equipping you with the tools needed for insightful environmental data analysis.</p>
</section>
</section>
<section id="understanding-the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="understanding-the-dataset">1. Understanding the Dataset</h2>
<p>Before delving into regression models, it’s essential to familiarize ourselves with the dataset at hand—<code>air_data_all.csv</code>. This dataset contains hourly air quality measurements and meteorological data from Beijing, spanning from March 1st, 2013, to February 28th, 2017. The dataset is sourced from the Beijing Municipal Environmental Monitoring Center and is matched with meteorological data from the China Meteorological Administration. However, it’s important to note that missing data points are marked as NA. Link to dataset is https://archive.ics.uci.edu/dataset/501/beijing+multi+site+air+quality+data.</p>
<section id="dataset-overview" class="level3">
<h3 class="anchored" data-anchor-id="dataset-overview">Dataset Overview</h3>
<p>The dataset is a valuable resource, encompassing a wide range of environmental conditions and pollutant concentrations. It records temporal information, including the year, month, day, and hour, alongside readings of key air pollutants such as PM2.5, PM10, SO2, NO2, CO, and O3. Additionally, meteorological factors like temperature (TEMP), pressure (PRES), dew point temperature (DEWP), precipitation (RAIN), wind direction (wd), and wind speed (WSPM) are included. This comprehensive data is instrumental for studying air pollution dynamics and its correlation with various environmental and temporal factors.</p>
</section>
<section id="column-descriptions" class="level3">
<h3 class="anchored" data-anchor-id="column-descriptions">Column Descriptions</h3>
<p>Each column in the dataset serves a specific purpose:</p>
<ol type="1">
<li><strong>Temporal Data (year, month, day, hour)</strong>: These columns provide insights into pollutant variations across different timescales.</li>
<li><strong>Pollutant Concentrations (PM2.5, PM10, SO2, NO2, CO, O3)</strong>: These are primary pollutants, crucial for urban air quality analysis.</li>
<li><strong>Meteorological Data (TEMP, PRES, DEWP, RAIN, wd, WSPM)</strong>: Weather conditions significantly impact pollutant dispersion and concentration.</li>
<li><strong>Station</strong>: This column identifies the monitoring site, facilitating the study of geographical variations in air quality.</li>
</ol>
</section>
<section id="initial-insights-and-challenges" class="level3">
<h3 class="anchored" data-anchor-id="initial-insights-and-challenges">Initial Insights and Challenges</h3>
<p>Upon initial examination, it’s evident that the dataset offers a comprehensive foundation for in-depth analysis. However, several challenges may arise:</p>
<ul>
<li><strong>Missing Data</strong>: Handling missing values is imperative to prevent biases in the models.</li>
<li><strong>High Dimensionality</strong>: The dataset’s numerous variables may lead to multicollinearity issues, where variables are highly correlated.</li>
<li><strong>Non-linear Relationships</strong>: Linear models may not capture all pollutant-environmental interactions, requiring more complex approaches like Random Forest.</li>
</ul>
<p>In the upcoming sections, we will address these challenges while preparing the data for regression analysis. By the end of this process, we will be well-equipped to employ linear and Random Forest regression for accurate pollutant concentration predictions.</p>
</section>
</section>
<section id="selecting-target-pollutants" class="level2">
<h2 class="anchored" data-anchor-id="selecting-target-pollutants">2. Selecting Target Pollutants</h2>
<p>In our journey to understand and predict air quality, selecting the right target pollutants is crucial. For this analysis, we will focus on the following pollutants: PM2.5, PM10, SO2, NO2, CO, and O3. Let’s delve into the criteria and rationale behind choosing these specific pollutants.</p>
<section id="criteria-for-selecting-target-pollutants" class="level3">
<h3 class="anchored" data-anchor-id="criteria-for-selecting-target-pollutants">Criteria for Selecting Target Pollutants</h3>
<p>The selection of target pollutants is based on the following criteria:</p>
<ol type="1">
<li><strong>Health Impact</strong>: Pollutants known to have significant health effects are prioritized.</li>
<li><strong>Prevalence and Relevance</strong>: Common pollutants in urban and industrial areas are selected due to their higher relevance.</li>
<li><strong>Data Availability</strong>: Pollutants with consistent and reliable data within the dataset are chosen to ensure the accuracy of the analysis.</li>
</ol>
</section>
<section id="rationale-behind-the-selection" class="level3">
<h3 class="anchored" data-anchor-id="rationale-behind-the-selection">Rationale Behind the Selection</h3>
<p>Each selected pollutant has its unique importance in air quality analysis:</p>
<ul>
<li><p><strong>PM2.5 and PM10 (Particulate Matter)</strong>: These are tiny particles in the air that reduce visibility and cause the air to appear hazy when levels are elevated. PM2.5 and PM10 are known for their ability to penetrate deep into the lungs and even into the bloodstream, causing respiratory and cardiovascular issues.</p></li>
<li><p><strong>SO2 (Sulfur Dioxide)</strong>: A gas typically produced by burning fossil fuels containing sulfur. It’s associated with acid rain and has health implications, especially for individuals with asthma.</p></li>
<li><p><strong>NO2 (Nitrogen Dioxide)</strong>: Primarily gets into the air from the burning of fuel. NO2 forms from emissions from cars, trucks and buses, power plants, and off-road equipment. It’s linked to various respiratory problems.</p></li>
<li><p><strong>CO (Carbon Monoxide)</strong>: A colorless, odorless gas that is harmful when inhaled in large amounts. It’s released from vehicles and other combustion sources and can cause harmful health effects by reducing the amount of oxygen that can be transported in the bloodstream.</p></li>
<li><p><strong>O3 (Ozone)</strong>: At ground level, ozone is a harmful air pollutant and a significant component of smog. It’s not emitted directly into the air but is created by chemical reactions between oxides of nitrogen (NOx) and volatile organic compounds (VOC) in the presence of sunlight.</p></li>
</ul>
<p>By focusing on these pollutants, we can provide a comprehensive analysis of air quality and its health implications. Next, we will perform correlation analysis and multicollinearity checks to understand how these pollutants interact with each other and with different environmental factors.</p>
</section>
</section>
<section id="data-cleaning-and-transformation" class="level2">
<h2 class="anchored" data-anchor-id="data-cleaning-and-transformation">3. Data Cleaning and Transformation</h2>
<p>Before delving into sophisticated regression models, it’s imperative to prepare our dataset, “air_data_all.csv,” for analysis. This stage, known as data cleaning and transformation, involves several key steps to ensure the data’s integrity and usability.</p>
<section id="identifying-and-handling-missing-or-inconsistent-data" class="level3">
<h3 class="anchored" data-anchor-id="identifying-and-handling-missing-or-inconsistent-data">Identifying and Handling Missing or Inconsistent Data</h3>
<p>The initial step in data preprocessing is to identify and address any missing (NaN) or inconsistent data. This is crucial as such data can significantly skew our analysis.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install seaborn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install matplotlib</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install statsmodels</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install scikit<span class="op">-</span>learn</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install pandas</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Import necessary libraries</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the dataset</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>sample_data <span class="op">=</span> pd.read_csv(<span class="st">'air_data_all.csv'</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Identifying missing or infinite values</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>sample_data.replace([np.inf, <span class="op">-</span>np.inf], np.nan, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Checking for missing values</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>missing_values <span class="op">=</span> sample_data.isnull().<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: seaborn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.13.0)
Requirement already satisfied: matplotlib!=3.6.1,&gt;=3.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (3.8.2)
Requirement already satisfied: pandas&gt;=1.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (2.1.3)
Requirement already satisfied: numpy!=1.24.0,&gt;=1.20 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (1.26.2)
Requirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (4.45.1)
Requirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (3.1.1)
Requirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.2.0)
Requirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (23.2)
Requirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (2.8.2)
Requirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.4.5)
Requirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (10.1.0)
Requirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (6.1.1)
Requirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (0.12.1)
Requirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (3.17.0)
Requirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3.post1)
Requirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3)
Requirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.15.0)
WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.
You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: matplotlib in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (3.8.2)
Requirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.0)
Requirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)
Requirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)
Requirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.45.1)
Requirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.1)
Requirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)
Requirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (23.2)
Requirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.1.0)
Requirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.1.1)
Requirement already satisfied: numpy&lt;2,&gt;=1.21 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.2)
Requirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib) (3.17.0)
Requirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.15.0)
WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.
You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: statsmodels in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.14.0)
Requirement already satisfied: pandas&gt;=1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (2.1.3)
Requirement already satisfied: patsy&gt;=0.5.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (0.5.3)
Requirement already satisfied: scipy!=1.9.2,&gt;=1.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (1.11.4)
Requirement already satisfied: packaging&gt;=21.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (23.2)
Requirement already satisfied: numpy&gt;=1.18 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (1.26.2)
Requirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.0-&gt;statsmodels) (2023.3.post1)
Requirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.0-&gt;statsmodels) (2023.3)
Requirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.0-&gt;statsmodels) (2.8.2)
Requirement already satisfied: six in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from patsy&gt;=0.5.2-&gt;statsmodels) (1.15.0)
WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.
You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: scikit-learn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.3.2)
Requirement already satisfied: joblib&gt;=1.1.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.3.2)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.2.0)
Requirement already satisfied: scipy&gt;=1.5.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.11.4)
Requirement already satisfied: numpy&lt;2.0,&gt;=1.17.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.2)
WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.
You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: pandas in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (2.1.3)
Requirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)
Requirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)
Requirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)
Requirement already satisfied: numpy&lt;2,&gt;=1.22.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)
Requirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.15.0)
WARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.
You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.</code></pre>
</div>
</div>
<p>In this code block, we first replace any infinite values with NaNs. Then, we calculate the number of missing values in each column. Depending on the nature and volume of missing data, we can either fill these gaps using statistical methods (like mean, median) or consider removing the rows/columns entirely.</p>
</section>
<section id="normalization-or-standardization-of-data" class="level3">
<h3 class="anchored" data-anchor-id="normalization-or-standardization-of-data">Normalization or Standardization of Data</h3>
<p>Normalization (rescaling data to a range, like 0–1) and standardization (shifting the distribution to have a mean of zero and a standard deviation of one) are crucial for models sensitive to the scale of data, such as linear regression.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardizing the dataset</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>scaled_data <span class="op">=</span> scaler.fit_transform(sample_data[[<span class="st">'TEMP'</span>, <span class="st">'PRES'</span>, <span class="st">'DEWP'</span>, <span class="st">'RAIN'</span>, <span class="st">'WSPM'</span>]])</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Converting scaled data back to a DataFrame for further use</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>scaled_df <span class="op">=</span> pd.DataFrame(scaled_data, columns<span class="op">=</span>[<span class="st">'TEMP'</span>, <span class="st">'PRES'</span>, <span class="st">'DEWP'</span>, <span class="st">'RAIN'</span>, <span class="st">'WSPM'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here, we use <code>StandardScaler</code> from Scikit-learn to standardize the continuous variables such as temperature and pressure. This process aligns the data onto one scale, removing bias due to different units or scales.</p>
</section>
<section id="transforming-categorical-data-into-a-usable-format" class="level3">
<h3 class="anchored" data-anchor-id="transforming-categorical-data-into-a-usable-format">Transforming Categorical Data into a Usable Format</h3>
<p>Many regression models require numerical input, so transforming categorical data into a numerical format is essential.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating dummy variables for categorical data</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>wd_dummies <span class="op">=</span> pd.get_dummies(sample_data[<span class="st">'wd'</span>])</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>sample_data <span class="op">=</span> pd.concat([sample_data, wd_dummies], axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the above snippet, we create dummy variables for the <code>wd</code> column (wind direction), converting it into a format that can be efficiently processed by regression algorithms.</p>
</section>
<section id="visuals-showing-before-and-after-data-transformation" class="level3">
<h3 class="anchored" data-anchor-id="visuals-showing-before-and-after-data-transformation">Visuals Showing Before and After Data Transformation</h3>
<p>Visualizations are effective for demonstrating the impact of data transformation. For instance, before and after standardization, we can plot histograms of a variable to observe changes in its distribution.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting before and after standardization</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>plt.hist(sample_data[<span class="st">'TEMP'</span>], bins<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Original TEMP'</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>plt.hist(scaled_df[<span class="st">'TEMP'</span>], bins<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Standardized TEMP'</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-5-output-1.png" width="592" height="411"></p>
</div>
</div>
<p>This histogram allows us to compare the distribution of the temperature data before and after standardization, showcasing the effects of our data transformation steps.</p>
<p>By completing these data cleaning and transformation processes, we ensure that our dataset is primed for accurate and effective regression analysis, laying a solid foundation for our subsequent modeling steps.</p>
</section>
</section>
<section id="correlation-analysis-and-multicollinearity-check" class="level2">
<h2 class="anchored" data-anchor-id="correlation-analysis-and-multicollinearity-check">4. Correlation Analysis and Multicollinearity Check</h2>
<p>After preparing our dataset, the next step in our analysis involves understanding the relationships between variables using correlation analysis and checking for multicollinearity. These steps are critical for ensuring the reliability and interpretability of our regression models.</p>
<section id="correlation-analysis-and-its-importance" class="level3">
<h3 class="anchored" data-anchor-id="correlation-analysis-and-its-importance">Correlation Analysis and Its Importance</h3>
<p>Correlation analysis helps us understand the strength and direction of the relationship between two variables. In regression analysis, it’s important to identify how independent variables are related to the dependent variable and to each other.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Removing missing or infinite values from the scaled dataset</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>scaled_df.replace([np.inf, <span class="op">-</span>np.inf], np.nan, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>scaled_df.dropna(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculating the correlation matrix for key variables</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>corr_matrix <span class="op">=</span> sample_data[[<span class="st">'PM2.5'</span>, <span class="st">'PM10'</span>, <span class="st">'SO2'</span>, <span class="st">'NO2'</span>, <span class="st">'CO'</span>, <span class="st">'O3'</span>, <span class="st">'TEMP'</span>, <span class="st">'PRES'</span>, <span class="st">'DEWP'</span>, <span class="st">'RAIN'</span>, <span class="st">'WSPM'</span>]].corr()</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizing the correlation matrix using a heatmap</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">10</span>))</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>sns.heatmap(corr_matrix, annot<span class="op">=</span><span class="va">True</span>, cmap<span class="op">=</span><span class="st">'coolwarm'</span>, fmt<span class="op">=</span><span class="st">'.2f'</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Correlation Matrix of Environmental Factors and Pollutants"</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-6-output-1.png" width="888" height="801"></p>
</div>
</div>
<p>In this code, we calculate and visualize the correlation matrix of key pollutants and environmental factors. This heatmap provides a clear visual representation of the relationships, where the color intensity and the value in each cell indicate the strength and direction of the correlation.</p>
</section>
<section id="multicollinearity-check-and-its-implications" class="level3">
<h3 class="anchored" data-anchor-id="multicollinearity-check-and-its-implications">Multicollinearity Check and Its Implications</h3>
<p>Multicollinearity occurs when two or more independent variables in a regression model are highly correlated. This can lead to unreliable coefficient estimates, making it difficult to determine the effect of each independent variable.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.outliers_influence <span class="im">import</span> variance_inflation_factor</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Preparing data for multicollinearity check</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> scaled_df[[<span class="st">'TEMP'</span>, <span class="st">'PRES'</span>, <span class="st">'DEWP'</span>, <span class="st">'RAIN'</span>, <span class="st">'WSPM'</span>]]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculating VIF for each feature</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>vif_data <span class="op">=</span> pd.DataFrame()</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>vif_data[<span class="st">'Feature'</span>] <span class="op">=</span> features.columns</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>vif_data[<span class="st">'VIF'</span>] <span class="op">=</span> [variance_inflation_factor(features.values, i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(features.shape[<span class="dv">1</span>])]</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>vif_data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Feature</th>
<th data-quarto-table-cell-role="th">VIF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>TEMP</td>
<td>5.355958</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>PRES</td>
<td>3.155330</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>DEWP</td>
<td>4.747345</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>RAIN</td>
<td>1.020343</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>WSPM</td>
<td>1.486136</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Here, we calculate the Variance Inflation Factor (VIF) for each feature. A VIF value greater than 5 or 10 indicates high multicollinearity, suggesting that the variable could be linearly predicted from the others with a substantial degree of accuracy.</p>
</section>
<section id="visual-representation-of-correlation-and-multicollinearity-findings" class="level3">
<h3 class="anchored" data-anchor-id="visual-representation-of-correlation-and-multicollinearity-findings">Visual Representation of Correlation and Multicollinearity Findings</h3>
<p>Visualizing these statistics can help in better understanding and communicating the findings.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizing VIF values</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>plt.bar(vif_data[<span class="st">'Feature'</span>], vif_data[<span class="st">'VIF'</span>])</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Features'</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Variance Inflation Factor (VIF)'</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Multicollinearity Check - VIF Values'</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-8-output-1.png" width="576" height="449"></p>
</div>
</div>
<p>This bar chart provides a clear representation of the VIF values for each feature, helping us identify which variables might be contributing to multicollinearity in the model.</p>
<p>By conducting both correlation analysis and a multicollinearity check, we ensure the integrity and effectiveness of our regression models, setting a strong foundation for accurate and insightful analysis of the factors influencing air quality.</p>
</section>
<section id="feature-selection" class="level3">
<h3 class="anchored" data-anchor-id="feature-selection">Feature Selection</h3>
<p>Based on the results of Correlation Analysis and Multicollinearity Check. I decided to predict SO2 with ‘TEMP’, ‘PRES’, ‘DEWP’.</p>
</section>
</section>
<section id="linear-regression-analysis" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression-analysis">5. Linear Regression Analysis</h2>
<p>In this section, we will apply linear regression analysis to predict the concentration of sulfur dioxide (SO2) based on three key environmental factors: ‘TEMP’, ‘PRES’, and ‘DEWP’. Linear regression is a fundamental statistical method used to understand the relationship between a dependent variable and one or more independent variables.</p>
<section id="introduction-to-linear-regression-and-its-applicability" class="level3">
<h3 class="anchored" data-anchor-id="introduction-to-linear-regression-and-its-applicability">Introduction to Linear Regression and Its Applicability</h3>
<p>Linear regression is a widely used statistical technique for modeling and analyzing the relationship between a scalar response (dependent variable) and one or more explanatory variables (independent variables). The method assumes a linear relationship between the variables. In our context, we will use linear regression to understand how temperature (‘TEMP’), pressure (‘PRES’), and dew point (‘DEWP’) affect the concentration of SO2 in the air.</p>
</section>
<section id="step-by-step-linear-regression-analysis-using-jupyter-notebook" class="level3">
<h3 class="anchored" data-anchor-id="step-by-step-linear-regression-analysis-using-jupyter-notebook">Step-by-Step Linear Regression Analysis Using Jupyter Notebook</h3>
<p>Now, let’s conduct a linear regression analysis using Python in a Jupyter Notebook environment.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter out rows where any of the feature columns or 'SO2' is NaN</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>filtered_data <span class="op">=</span> sample_data.dropna(subset<span class="op">=</span>[<span class="st">'TEMP'</span>, <span class="st">'PRES'</span>, <span class="st">'DEWP'</span>, <span class="st">'SO2'</span>])</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardizing the relevant columns of the filtered data</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>scaled_columns <span class="op">=</span> scaler.fit_transform(filtered_data[[<span class="st">'TEMP'</span>, <span class="st">'PRES'</span>, <span class="st">'DEWP'</span>]])</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Converting scaled data back to a DataFrame</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>scaled_df <span class="op">=</span> pd.DataFrame(scaled_columns, columns<span class="op">=</span>[<span class="st">'TEMP'</span>, <span class="st">'PRES'</span>, <span class="st">'DEWP'</span>])</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Defining features (X) and target variable (y)</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> scaled_df</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> filtered_data[<span class="st">'SO2'</span>]</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Splitting the dataset into training and testing sets</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating and fitting the linear regression model</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression()</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression()</pre></div></div></div></div></div>
</div>
</div>
<p>In this code, we first select our features and target variable, split the data into training and test sets, create a Linear Regression model, and then fit it to our training data.</p>
</section>
<section id="visual-representation-of-linear-regression-results-and-plotting-the-best-fit-line" class="level3">
<h3 class="anchored" data-anchor-id="visual-representation-of-linear-regression-results-and-plotting-the-best-fit-line">Visual Representation of Linear Regression Results and Plotting the Best Fit Line</h3>
<p>Visualizing the model’s predictions in comparison with the actual values is crucial for assessing its performance. We’ll also plot the best-fit line to better understand the linear relationship.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicting SO2 values for the test set</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>lr_y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizing the actual vs predicted values and the best-fit line</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_test, lr_y_pred, alpha<span class="op">=</span><span class="fl">0.6</span>, color<span class="op">=</span><span class="st">'blue'</span>)  <span class="co"># Actual vs Predicted scatter plot</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Actual SO2'</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Predicted SO2'</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Actual vs Predicted SO2 Concentrations'</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the best-fit line</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>plt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, lr_y_pred, <span class="dv">1</span>))(np.unique(y_test)), color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Zoom in </span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>, <span class="dv">200</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">40</span>)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizing the actual vs predicted values and the best-fit line</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_test, lr_y_pred, alpha<span class="op">=</span><span class="fl">0.6</span>, color<span class="op">=</span><span class="st">'blue'</span>)  <span class="co"># Actual vs Predicted scatter plot</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Actual SO2'</span>)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Predicted SO2'</span>)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Actual vs Predicted SO2 Concentrations'</span>)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the best-fit line</span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>plt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, lr_y_pred, <span class="dv">1</span>))(np.unique(y_test)), color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-10-output-1.png" width="585" height="449"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-10-output-2.png" width="597" height="449"></p>
</div>
</div>
<p>The scatter plot shows the actual vs.&nbsp;predicted SO2 values, and the red line represents the linear fit, providing a visual indication of how well the model predicts SO2 concentration.</p>
</section>
<section id="evaluating-the-performance-of-the-linear-regression-model" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-the-performance-of-the-linear-regression-model">Evaluating the Performance of the Linear Regression Model</h3>
<p>Finally, we evaluate the performance of our model using common statistical metrics.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Computing performance metrics</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>lr_mse <span class="op">=</span> mean_squared_error(y_test, lr_y_pred)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>lr_r2 <span class="op">=</span> r2_score(y_test, lr_y_pred)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean Squared Error: </span><span class="sc">{</span>lr_mse<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"R² Score: </span><span class="sc">{</span>lr_r2<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mean Squared Error: 411.5799313674985
R² Score: 0.10938551133078755</code></pre>
</div>
</div>
<p>The Mean Squared Error (MSE) provides an average of the squares of the errors, essentially quantifying the difference between predicted and actual values. The R² Score measures the proportion of the variance in the dependent variable that is predictable from the independent variables.</p>
<p>By following these steps, we can use linear regression to effectively predict environmental factors’ impact on air quality, specifically sulfur dioxide concentrations, and evaluate the accuracy of our predictions.</p>
<p>Sure, I’ll help you generate text for the “Random Forest Regression Analysis” section of your blog. Here’s the content for that section:</p>
</section>
</section>
<section id="random-forest-regression-analysis" class="level2">
<h2 class="anchored" data-anchor-id="random-forest-regression-analysis">6. Random Forest Regression Analysis</h2>
<section id="introduction-to-random-forest-regression" class="level3">
<h3 class="anchored" data-anchor-id="introduction-to-random-forest-regression">Introduction to Random Forest Regression</h3>
<p>Random Forest is an ensemble learning method predominantly used for classification and regression tasks. It operates by constructing a multitude of decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Compared to linear regression, Random Forest offers several advantages:</p>
<ul>
<li><strong>Handling Non-linear Data</strong>: It can model complex relationships between features and the target variable, which linear regression may fail to capture.</li>
<li><strong>Reducing Overfitting</strong>: By averaging multiple decision trees, it reduces the risk of overfitting to the training data.</li>
<li><strong>Importance of Features</strong>: Random Forest can provide insights into the relative importance of each feature in prediction.</li>
</ul>
</section>
<section id="implementing-random-forest-regression" class="level3">
<h3 class="anchored" data-anchor-id="implementing-random-forest-regression">Implementing Random Forest Regression</h3>
<p>Let’s implement Random Forest regression to predict the concentration of sulfur dioxide (SO2) using ‘TEMP’ (temperature), ‘PRES’ (pressure), and ‘DEWP’ (dew point). We have already preprocessed and scaled our dataset. Now, we’ll apply Random Forest regression:</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a Random Forest model</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>rf_model <span class="op">=</span> RandomForestRegressor(random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model to the training data</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>rf_model.fit(X_train, y_train)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicting the SO2 values using the test set</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>rf_y_pred <span class="op">=</span> rf_model.predict(X_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="visualization-feature-importance-and-prediction-vs-actual" class="level3">
<h3 class="anchored" data-anchor-id="visualization-feature-importance-and-prediction-vs-actual">Visualization: Feature Importance and Prediction vs Actual</h3>
<ol type="1">
<li><strong>Feature Importance Plot</strong>: This graph illustrates the relative importance of each feature in predicting the SO2 levels.</li>
</ol>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>feature_importances <span class="op">=</span> rf_model.feature_importances_</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>plt.barh([<span class="st">'TEMP'</span>, <span class="st">'PRES'</span>, <span class="st">'DEWP'</span>], feature_importances)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Feature Importance'</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Feature'</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Feature Importance in Random Forest Model'</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-13-output-1.png" width="608" height="449"></p>
</div>
</div>
<p>The feature importance plot shows ‘TEMP’ with the highest score, indicating it has the most significant impact on predicting SO2 levels, followed by ‘PRES’ and ‘DEWP’. This suggests that temperature changes are potentially a more dominant factor in influencing SO2 concentrations in the atmosphere.</p>
<ol start="2" type="1">
<li><strong>Prediction vs Actual Plot</strong>: This plot compares the actual vs.&nbsp;predicted SO2 levels using the Random Forest model.</li>
</ol>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_test, rf_y_pred)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Actual SO2 Levels'</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Predicted SO2 Levels'</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Random Forest: Actual vs Predicted SO2 Levels'</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>plt.plot([y_test.<span class="bu">min</span>(), y_test.<span class="bu">max</span>()], [y_test.<span class="bu">min</span>(), y_test.<span class="bu">max</span>()], <span class="st">'k--'</span>, lw<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-14-output-1.png" width="593" height="449"></p>
</div>
</div>
<p>The Prediction vs Actual Plot for the Random Forest model reveals a tighter clustering of data points along the line of perfect prediction compared to the Linear Regression model. This clustering indicates a higher accuracy in predictions made by the Random Forest model.</p>
</section>
</section>
<section id="comparative-analysis-and-conclusion" class="level2">
<h2 class="anchored" data-anchor-id="comparative-analysis-and-conclusion">Comparative Analysis and Conclusion</h2>
<p>We compare the performance metrics of Random Forest and Linear Regression:</p>
<ul>
<li><strong>Random Forest</strong>
<ul>
<li>MSE: 204.29218141691157</li>
<li>R²: 0.5579337989410323</li>
</ul></li>
<li><strong>Linear Regression</strong>
<ul>
<li>MSE: 411.5799313674985</li>
<li>R²: 0.10938551133078755</li>
</ul></li>
</ul>
<section id="interpretation" class="level4">
<h4 class="anchored" data-anchor-id="interpretation">Interpretation</h4>
<p>The Random Forest model shows a significantly lower Mean Squared Error (MSE) and higher R² value compared to Linear Regression. This indicates that the Random Forest model fits the data better and has a greater predictive accuracy. The reduced MSE suggests that the Random Forest model’s predictions are closer to the actual data. The higher R² value indicates that a larger proportion of the variance in the SO2 concentration is being explained by the model.</p>
</section>
<section id="visual-comparison-prediction-vs-actual-plot-for-both-models" class="level3">
<h3 class="anchored" data-anchor-id="visual-comparison-prediction-vs-actual-plot-for-both-models">Visual Comparison: Prediction vs Actual Plot for Both Models</h3>
<p>This plot will compare the predictions of both models against the actual SO2 levels. Here, ‘lr_y_pred’ represents the predicted values from the Linear Regression model.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_test, lr_y_pred, label<span class="op">=</span><span class="st">'Linear Regression'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'b'</span>, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_test, rf_y_pred, label<span class="op">=</span><span class="st">'Random Forest'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'r'</span>, marker<span class="op">=</span><span class="st">'+'</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Actual SO2 Levels'</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Predicted SO2 Levels'</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Comparison of Predictions: Linear Regression vs Random Forest'</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>plt.plot([y_test.<span class="bu">min</span>(), y_test.<span class="bu">max</span>()], [y_test.<span class="bu">min</span>(), y_test.<span class="bu">max</span>()], <span class="st">'k--'</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-15-output-1.png" width="593" height="449"></p>
</div>
</div>
<p>The combined Prediction vs Actual Plot demonstrates a stark contrast between the two models. The Random Forest predictions are more concentrated around the line of perfect fit, while the Linear Regression predictions are more dispersed, indicating more errors in prediction. This visual reaffirms the quantitative metrics, illustrating that Random Forest provides a more accurate model for predicting SO2 levels based on ‘TEMP’, ‘PRES’, and ‘DEWP’.</p>
</section>
<section id="limitation" class="level3">
<h3 class="anchored" data-anchor-id="limitation">Limitation</h3>
<p>As depicted in the visualizations, there appear to be a few outliers in the graph. Conducting an outlier analysis before proceeding with modeling could potentially enhance the accuracy of our predictions.</p>


<!-- -->

</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb17" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "3\\. Linear and Nonlinear Regression"</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Joanna Fang"</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2023-11-29"</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [ml, code, linear regression, nonlinear regression, pollution]</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">  html: </span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">    code-block-bg: "#FFFFFF"</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">    code-block-border-left: "#E83283"</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools:</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co">      source: true</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co">      toggle: false</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co">      caption: none</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="fu"># Predicting Air Pollutant Concentrations Using Linear and Random Forest Regression: A Jupyter Notebook Guide</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="al">![](thumbnail.jpg)</span>{height="50%" fig-align="center"}</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>Air quality is a critical environmental factor impacting public health, ecosystem sustainability, and the global climate. Pollutants such as particulate matter (PM2.5 and PM10), sulfur dioxide (SO2), nitrogen dioxide (NO2), carbon monoxide (CO), and ozone (O3) can have severe health impacts, including respiratory and cardiovascular diseases. Understanding and predicting the concentrations of these pollutants is essential for creating effective environmental policies and public health interventions.</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>In this blog, we'll delve into two powerful statistical methods used in predicting air pollutant concentrations: linear regression and Random Forest regression.</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a><span class="fu">### Linear Regression</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>Linear regression is a fundamental statistical approach used to model the relationship between a dependent variable and one or more independent variables. In the context of air quality, it helps us understand how various environmental factors like temperature, humidity, and wind speed influence pollutant levels. The model assumes a linear relationship between the variables, which can be represented as:</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n + \epsilon</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>Here, <span class="sc">\(</span> Y <span class="sc">\)</span> is the pollutant concentration we want to predict, <span class="sc">\(</span> X_1, X_2, ..., X_n <span class="sc">\)</span> are the environmental factors, <span class="sc">\(</span> \beta_0, \beta_1, ..., \beta_n <span class="sc">\)</span> are the coefficients to be estimated, and <span class="sc">\(</span> \epsilon <span class="sc">\)</span> is the error term.</span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a><span class="fu">### Random Forest Regression</span></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>Random Forest, on the other hand, is a type of ensemble learning method, particularly useful for non-linear relationships. It operates by constructing multiple decision trees during training and outputting the mean prediction of the individual trees. This method is beneficial for handling complex interactions between variables and can provide more accurate predictions for complex datasets like those in air quality studies.</span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>The purpose of this blog is to provide a step-by-step guide on how to use these methods, utilizing a Jupyter Notebook, to predict pollutant concentrations. We'll start by exploring our dataset, <span class="in">`air_data_all.csv`</span>, which includes a variety of environmental conditions and temporal factors, and then apply these regression techniques to gain insights into the factors affecting air quality.</span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>By the end of this blog, you'll have a clearer understanding of how to implement these techniques in Python and interpret their results, equipping you with the tools needed for insightful environmental data analysis.</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a><span class="fu">## 1. Understanding the Dataset</span></span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>Before delving into regression models, it's essential to familiarize ourselves with the dataset at hand—<span class="in">`air_data_all.csv`</span>. This dataset contains hourly air quality measurements and meteorological data from Beijing, spanning from March 1st, 2013, to February 28th, 2017. The dataset is sourced from the Beijing Municipal Environmental Monitoring Center and is matched with meteorological data from the China Meteorological Administration. However, it's important to note that missing data points are marked as NA. Link to dataset is https://archive.ics.uci.edu/dataset/501/beijing+multi+site+air+quality+data. </span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a><span class="fu">### Dataset Overview</span></span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>The dataset is a valuable resource, encompassing a wide range of environmental conditions and pollutant concentrations. It records temporal information, including the year, month, day, and hour, alongside readings of key air pollutants such as PM2.5, PM10, SO2, NO2, CO, and O3. Additionally, meteorological factors like temperature (TEMP), pressure (PRES), dew point temperature (DEWP), precipitation (RAIN), wind direction (wd), and wind speed (WSPM) are included. This comprehensive data is instrumental for studying air pollution dynamics and its correlation with various environmental and temporal factors.</span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a><span class="fu">### Column Descriptions</span></span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a>Each column in the dataset serves a specific purpose:</span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Temporal Data (year, month, day, hour)**: These columns provide insights into pollutant variations across different timescales.</span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Pollutant Concentrations (PM2.5, PM10, SO2, NO2, CO, O3)**: These are primary pollutants, crucial for urban air quality analysis.</span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Meteorological Data (TEMP, PRES, DEWP, RAIN, wd, WSPM)**: Weather conditions significantly impact pollutant dispersion and concentration.</span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Station**: This column identifies the monitoring site, facilitating the study of geographical variations in air quality.</span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a><span class="fu">### Initial Insights and Challenges</span></span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a>Upon initial examination, it's evident that the dataset offers a comprehensive foundation for in-depth analysis. However, several challenges may arise:</span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Missing Data**: Handling missing values is imperative to prevent biases in the models.</span>
<span id="cb17-64"><a href="#cb17-64" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**High Dimensionality**: The dataset's numerous variables may lead to multicollinearity issues, where variables are highly correlated.</span>
<span id="cb17-65"><a href="#cb17-65" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Non-linear Relationships**: Linear models may not capture all pollutant-environmental interactions, requiring more complex approaches like Random Forest.</span>
<span id="cb17-66"><a href="#cb17-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-67"><a href="#cb17-67" aria-hidden="true" tabindex="-1"></a>In the upcoming sections, we will address these challenges while preparing the data for regression analysis. By the end of this process, we will be well-equipped to employ linear and Random Forest regression for accurate pollutant concentration predictions.</span>
<span id="cb17-68"><a href="#cb17-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-69"><a href="#cb17-69" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2. Selecting Target Pollutants</span></span>
<span id="cb17-70"><a href="#cb17-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-71"><a href="#cb17-71" aria-hidden="true" tabindex="-1"></a>In our journey to understand and predict air quality, selecting the right target pollutants is crucial. For this analysis, we will focus on the following pollutants: PM2.5, PM10, SO2, NO2, CO, and O3. Let's delve into the criteria and rationale behind choosing these specific pollutants.</span>
<span id="cb17-72"><a href="#cb17-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-73"><a href="#cb17-73" aria-hidden="true" tabindex="-1"></a><span class="fu">### Criteria for Selecting Target Pollutants</span></span>
<span id="cb17-74"><a href="#cb17-74" aria-hidden="true" tabindex="-1"></a>The selection of target pollutants is based on the following criteria:</span>
<span id="cb17-75"><a href="#cb17-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-76"><a href="#cb17-76" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Health Impact**: Pollutants known to have significant health effects are prioritized.</span>
<span id="cb17-77"><a href="#cb17-77" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Prevalence and Relevance**: Common pollutants in urban and industrial areas are selected due to their higher relevance.</span>
<span id="cb17-78"><a href="#cb17-78" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Data Availability**: Pollutants with consistent and reliable data within the dataset are chosen to ensure the accuracy of the analysis.</span>
<span id="cb17-79"><a href="#cb17-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-80"><a href="#cb17-80" aria-hidden="true" tabindex="-1"></a><span class="fu">### Rationale Behind the Selection</span></span>
<span id="cb17-81"><a href="#cb17-81" aria-hidden="true" tabindex="-1"></a>Each selected pollutant has its unique importance in air quality analysis:</span>
<span id="cb17-82"><a href="#cb17-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-83"><a href="#cb17-83" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**PM2.5 and PM10 (Particulate Matter)**: These are tiny particles in the air that reduce visibility and cause the air to appear hazy when levels are elevated. PM2.5 and PM10 are known for their ability to penetrate deep into the lungs and even into the bloodstream, causing respiratory and cardiovascular issues.</span>
<span id="cb17-84"><a href="#cb17-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-85"><a href="#cb17-85" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SO2 (Sulfur Dioxide)**: A gas typically produced by burning fossil fuels containing sulfur. It's associated with acid rain and has health implications, especially for individuals with asthma.</span>
<span id="cb17-86"><a href="#cb17-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-87"><a href="#cb17-87" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**NO2 (Nitrogen Dioxide)**: Primarily gets into the air from the burning of fuel. NO2 forms from emissions from cars, trucks and buses, power plants, and off-road equipment. It's linked to various respiratory problems.</span>
<span id="cb17-88"><a href="#cb17-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-89"><a href="#cb17-89" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**CO (Carbon Monoxide)**: A colorless, odorless gas that is harmful when inhaled in large amounts. It's released from vehicles and other combustion sources and can cause harmful health effects by reducing the amount of oxygen that can be transported in the bloodstream.</span>
<span id="cb17-90"><a href="#cb17-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-91"><a href="#cb17-91" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**O3 (Ozone)**: At ground level, ozone is a harmful air pollutant and a significant component of smog. It's not emitted directly into the air but is created by chemical reactions between oxides of nitrogen (NOx) and volatile organic compounds (VOC) in the presence of sunlight.</span>
<span id="cb17-92"><a href="#cb17-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-93"><a href="#cb17-93" aria-hidden="true" tabindex="-1"></a>By focusing on these pollutants, we can provide a comprehensive analysis of air quality and its health implications. Next, we will perform correlation analysis and multicollinearity checks to understand how these pollutants interact with each other and with different environmental factors.</span>
<span id="cb17-94"><a href="#cb17-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-95"><a href="#cb17-95" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3. Data Cleaning and Transformation</span></span>
<span id="cb17-96"><a href="#cb17-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-97"><a href="#cb17-97" aria-hidden="true" tabindex="-1"></a>Before delving into sophisticated regression models, it's imperative to prepare our dataset, "air_data_all.csv," for analysis. This stage, known as data cleaning and transformation, involves several key steps to ensure the data's integrity and usability.</span>
<span id="cb17-98"><a href="#cb17-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-99"><a href="#cb17-99" aria-hidden="true" tabindex="-1"></a><span class="fu">### Identifying and Handling Missing or Inconsistent Data</span></span>
<span id="cb17-100"><a href="#cb17-100" aria-hidden="true" tabindex="-1"></a>The initial step in data preprocessing is to identify and address any missing (NaN) or inconsistent data. This is crucial as such data can significantly skew our analysis.</span>
<span id="cb17-101"><a href="#cb17-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-104"><a href="#cb17-104" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-105"><a href="#cb17-105" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb17-106"><a href="#cb17-106" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install seaborn</span>
<span id="cb17-107"><a href="#cb17-107" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install matplotlib</span>
<span id="cb17-108"><a href="#cb17-108" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install statsmodels</span>
<span id="cb17-109"><a href="#cb17-109" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install scikit<span class="op">-</span>learn</span>
<span id="cb17-110"><a href="#cb17-110" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install pandas</span>
<span id="cb17-111"><a href="#cb17-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-112"><a href="#cb17-112" aria-hidden="true" tabindex="-1"></a><span class="co"># Import necessary libraries</span></span>
<span id="cb17-113"><a href="#cb17-113" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb17-114"><a href="#cb17-114" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-115"><a href="#cb17-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-116"><a href="#cb17-116" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the dataset</span></span>
<span id="cb17-117"><a href="#cb17-117" aria-hidden="true" tabindex="-1"></a>sample_data <span class="op">=</span> pd.read_csv(<span class="st">'air_data_all.csv'</span>)</span>
<span id="cb17-118"><a href="#cb17-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-119"><a href="#cb17-119" aria-hidden="true" tabindex="-1"></a><span class="co"># Identifying missing or infinite values</span></span>
<span id="cb17-120"><a href="#cb17-120" aria-hidden="true" tabindex="-1"></a>sample_data.replace([np.inf, <span class="op">-</span>np.inf], np.nan, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-121"><a href="#cb17-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-122"><a href="#cb17-122" aria-hidden="true" tabindex="-1"></a><span class="co"># Checking for missing values</span></span>
<span id="cb17-123"><a href="#cb17-123" aria-hidden="true" tabindex="-1"></a>missing_values <span class="op">=</span> sample_data.isnull().<span class="bu">sum</span>()</span>
<span id="cb17-124"><a href="#cb17-124" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-125"><a href="#cb17-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-126"><a href="#cb17-126" aria-hidden="true" tabindex="-1"></a>In this code block, we first replace any infinite values with NaNs. Then, we calculate the number of missing values in each column. Depending on the nature and volume of missing data, we can either fill these gaps using statistical methods (like mean, median) or consider removing the rows/columns entirely.</span>
<span id="cb17-127"><a href="#cb17-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-128"><a href="#cb17-128" aria-hidden="true" tabindex="-1"></a><span class="fu">### Normalization or Standardization of Data</span></span>
<span id="cb17-129"><a href="#cb17-129" aria-hidden="true" tabindex="-1"></a>Normalization (rescaling data to a range, like 0–1) and standardization (shifting the distribution to have a mean of zero and a standard deviation of one) are crucial for models sensitive to the scale of data, such as linear regression.</span>
<span id="cb17-130"><a href="#cb17-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-133"><a href="#cb17-133" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-134"><a href="#cb17-134" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb17-135"><a href="#cb17-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-136"><a href="#cb17-136" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardizing the dataset</span></span>
<span id="cb17-137"><a href="#cb17-137" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb17-138"><a href="#cb17-138" aria-hidden="true" tabindex="-1"></a>scaled_data <span class="op">=</span> scaler.fit_transform(sample_data[[<span class="st">'TEMP'</span>, <span class="st">'PRES'</span>, <span class="st">'DEWP'</span>, <span class="st">'RAIN'</span>, <span class="st">'WSPM'</span>]])</span>
<span id="cb17-139"><a href="#cb17-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-140"><a href="#cb17-140" aria-hidden="true" tabindex="-1"></a><span class="co"># Converting scaled data back to a DataFrame for further use</span></span>
<span id="cb17-141"><a href="#cb17-141" aria-hidden="true" tabindex="-1"></a>scaled_df <span class="op">=</span> pd.DataFrame(scaled_data, columns<span class="op">=</span>[<span class="st">'TEMP'</span>, <span class="st">'PRES'</span>, <span class="st">'DEWP'</span>, <span class="st">'RAIN'</span>, <span class="st">'WSPM'</span>])</span>
<span id="cb17-142"><a href="#cb17-142" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-143"><a href="#cb17-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-144"><a href="#cb17-144" aria-hidden="true" tabindex="-1"></a>Here, we use <span class="in">`StandardScaler`</span> from Scikit-learn to standardize the continuous variables such as temperature and pressure. This process aligns the data onto one scale, removing bias due to different units or scales.</span>
<span id="cb17-145"><a href="#cb17-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-146"><a href="#cb17-146" aria-hidden="true" tabindex="-1"></a><span class="fu">### Transforming Categorical Data into a Usable Format</span></span>
<span id="cb17-147"><a href="#cb17-147" aria-hidden="true" tabindex="-1"></a>Many regression models require numerical input, so transforming categorical data into a numerical format is essential.</span>
<span id="cb17-148"><a href="#cb17-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-151"><a href="#cb17-151" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-152"><a href="#cb17-152" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating dummy variables for categorical data</span></span>
<span id="cb17-153"><a href="#cb17-153" aria-hidden="true" tabindex="-1"></a>wd_dummies <span class="op">=</span> pd.get_dummies(sample_data[<span class="st">'wd'</span>])</span>
<span id="cb17-154"><a href="#cb17-154" aria-hidden="true" tabindex="-1"></a>sample_data <span class="op">=</span> pd.concat([sample_data, wd_dummies], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb17-155"><a href="#cb17-155" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-156"><a href="#cb17-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-157"><a href="#cb17-157" aria-hidden="true" tabindex="-1"></a>In the above snippet, we create dummy variables for the <span class="in">`wd`</span> column (wind direction), converting it into a format that can be efficiently processed by regression algorithms.</span>
<span id="cb17-158"><a href="#cb17-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-159"><a href="#cb17-159" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visuals Showing Before and After Data Transformation</span></span>
<span id="cb17-160"><a href="#cb17-160" aria-hidden="true" tabindex="-1"></a>Visualizations are effective for demonstrating the impact of data transformation. For instance, before and after standardization, we can plot histograms of a variable to observe changes in its distribution.</span>
<span id="cb17-161"><a href="#cb17-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-164"><a href="#cb17-164" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-165"><a href="#cb17-165" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-166"><a href="#cb17-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-167"><a href="#cb17-167" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting before and after standardization</span></span>
<span id="cb17-168"><a href="#cb17-168" aria-hidden="true" tabindex="-1"></a>plt.hist(sample_data[<span class="st">'TEMP'</span>], bins<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Original TEMP'</span>)</span>
<span id="cb17-169"><a href="#cb17-169" aria-hidden="true" tabindex="-1"></a>plt.hist(scaled_df[<span class="st">'TEMP'</span>], bins<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Standardized TEMP'</span>)</span>
<span id="cb17-170"><a href="#cb17-170" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb17-171"><a href="#cb17-171" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-172"><a href="#cb17-172" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-173"><a href="#cb17-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-174"><a href="#cb17-174" aria-hidden="true" tabindex="-1"></a>This histogram allows us to compare the distribution of the temperature data before and after standardization, showcasing the effects of our data transformation steps.</span>
<span id="cb17-175"><a href="#cb17-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-176"><a href="#cb17-176" aria-hidden="true" tabindex="-1"></a>By completing these data cleaning and transformation processes, we ensure that our dataset is primed for accurate and effective regression analysis, laying a solid foundation for our subsequent modeling steps.</span>
<span id="cb17-177"><a href="#cb17-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-178"><a href="#cb17-178" aria-hidden="true" tabindex="-1"></a><span class="fu">## 4. Correlation Analysis and Multicollinearity Check</span></span>
<span id="cb17-179"><a href="#cb17-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-180"><a href="#cb17-180" aria-hidden="true" tabindex="-1"></a>After preparing our dataset, the next step in our analysis involves understanding the relationships between variables using correlation analysis and checking for multicollinearity. These steps are critical for ensuring the reliability and interpretability of our regression models.</span>
<span id="cb17-181"><a href="#cb17-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-182"><a href="#cb17-182" aria-hidden="true" tabindex="-1"></a><span class="fu">### Correlation Analysis and Its Importance</span></span>
<span id="cb17-183"><a href="#cb17-183" aria-hidden="true" tabindex="-1"></a>Correlation analysis helps us understand the strength and direction of the relationship between two variables. In regression analysis, it's important to identify how independent variables are related to the dependent variable and to each other.</span>
<span id="cb17-184"><a href="#cb17-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-187"><a href="#cb17-187" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-188"><a href="#cb17-188" aria-hidden="true" tabindex="-1"></a><span class="co"># Removing missing or infinite values from the scaled dataset</span></span>
<span id="cb17-189"><a href="#cb17-189" aria-hidden="true" tabindex="-1"></a>scaled_df.replace([np.inf, <span class="op">-</span>np.inf], np.nan, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-190"><a href="#cb17-190" aria-hidden="true" tabindex="-1"></a>scaled_df.dropna(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-191"><a href="#cb17-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-192"><a href="#cb17-192" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb17-193"><a href="#cb17-193" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-194"><a href="#cb17-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-195"><a href="#cb17-195" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculating the correlation matrix for key variables</span></span>
<span id="cb17-196"><a href="#cb17-196" aria-hidden="true" tabindex="-1"></a>corr_matrix <span class="op">=</span> sample_data[[<span class="st">'PM2.5'</span>, <span class="st">'PM10'</span>, <span class="st">'SO2'</span>, <span class="st">'NO2'</span>, <span class="st">'CO'</span>, <span class="st">'O3'</span>, <span class="st">'TEMP'</span>, <span class="st">'PRES'</span>, <span class="st">'DEWP'</span>, <span class="st">'RAIN'</span>, <span class="st">'WSPM'</span>]].corr()</span>
<span id="cb17-197"><a href="#cb17-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-198"><a href="#cb17-198" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizing the correlation matrix using a heatmap</span></span>
<span id="cb17-199"><a href="#cb17-199" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">10</span>))</span>
<span id="cb17-200"><a href="#cb17-200" aria-hidden="true" tabindex="-1"></a>sns.heatmap(corr_matrix, annot<span class="op">=</span><span class="va">True</span>, cmap<span class="op">=</span><span class="st">'coolwarm'</span>, fmt<span class="op">=</span><span class="st">'.2f'</span>)</span>
<span id="cb17-201"><a href="#cb17-201" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Correlation Matrix of Environmental Factors and Pollutants"</span>)</span>
<span id="cb17-202"><a href="#cb17-202" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-203"><a href="#cb17-203" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-204"><a href="#cb17-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-205"><a href="#cb17-205" aria-hidden="true" tabindex="-1"></a>In this code, we calculate and visualize the correlation matrix of key pollutants and environmental factors. This heatmap provides a clear visual representation of the relationships, where the color intensity and the value in each cell indicate the strength and direction of the correlation.</span>
<span id="cb17-206"><a href="#cb17-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-207"><a href="#cb17-207" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multicollinearity Check and Its Implications</span></span>
<span id="cb17-208"><a href="#cb17-208" aria-hidden="true" tabindex="-1"></a>Multicollinearity occurs when two or more independent variables in a regression model are highly correlated. This can lead to unreliable coefficient estimates, making it difficult to determine the effect of each independent variable.</span>
<span id="cb17-209"><a href="#cb17-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-212"><a href="#cb17-212" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-213"><a href="#cb17-213" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.outliers_influence <span class="im">import</span> variance_inflation_factor</span>
<span id="cb17-214"><a href="#cb17-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-215"><a href="#cb17-215" aria-hidden="true" tabindex="-1"></a><span class="co"># Preparing data for multicollinearity check</span></span>
<span id="cb17-216"><a href="#cb17-216" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> scaled_df[[<span class="st">'TEMP'</span>, <span class="st">'PRES'</span>, <span class="st">'DEWP'</span>, <span class="st">'RAIN'</span>, <span class="st">'WSPM'</span>]]</span>
<span id="cb17-217"><a href="#cb17-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-218"><a href="#cb17-218" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculating VIF for each feature</span></span>
<span id="cb17-219"><a href="#cb17-219" aria-hidden="true" tabindex="-1"></a>vif_data <span class="op">=</span> pd.DataFrame()</span>
<span id="cb17-220"><a href="#cb17-220" aria-hidden="true" tabindex="-1"></a>vif_data[<span class="st">'Feature'</span>] <span class="op">=</span> features.columns</span>
<span id="cb17-221"><a href="#cb17-221" aria-hidden="true" tabindex="-1"></a>vif_data[<span class="st">'VIF'</span>] <span class="op">=</span> [variance_inflation_factor(features.values, i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(features.shape[<span class="dv">1</span>])]</span>
<span id="cb17-222"><a href="#cb17-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-223"><a href="#cb17-223" aria-hidden="true" tabindex="-1"></a>vif_data</span>
<span id="cb17-224"><a href="#cb17-224" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-225"><a href="#cb17-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-226"><a href="#cb17-226" aria-hidden="true" tabindex="-1"></a>Here, we calculate the Variance Inflation Factor (VIF) for each feature. A VIF value greater than 5 or 10 indicates high multicollinearity, suggesting that the variable could be linearly predicted from the others with a substantial degree of accuracy.</span>
<span id="cb17-227"><a href="#cb17-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-228"><a href="#cb17-228" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visual Representation of Correlation and Multicollinearity Findings</span></span>
<span id="cb17-229"><a href="#cb17-229" aria-hidden="true" tabindex="-1"></a>Visualizing these statistics can help in better understanding and communicating the findings.</span>
<span id="cb17-230"><a href="#cb17-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-233"><a href="#cb17-233" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-234"><a href="#cb17-234" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizing VIF values</span></span>
<span id="cb17-235"><a href="#cb17-235" aria-hidden="true" tabindex="-1"></a>plt.bar(vif_data[<span class="st">'Feature'</span>], vif_data[<span class="st">'VIF'</span>])</span>
<span id="cb17-236"><a href="#cb17-236" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Features'</span>)</span>
<span id="cb17-237"><a href="#cb17-237" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Variance Inflation Factor (VIF)'</span>)</span>
<span id="cb17-238"><a href="#cb17-238" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Multicollinearity Check - VIF Values'</span>)</span>
<span id="cb17-239"><a href="#cb17-239" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-240"><a href="#cb17-240" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-241"><a href="#cb17-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-242"><a href="#cb17-242" aria-hidden="true" tabindex="-1"></a>This bar chart provides a clear representation of the VIF values for each feature, helping us identify which variables might be contributing to multicollinearity in the model.</span>
<span id="cb17-243"><a href="#cb17-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-244"><a href="#cb17-244" aria-hidden="true" tabindex="-1"></a>By conducting both correlation analysis and a multicollinearity check, we ensure the integrity and effectiveness of our regression models, setting a strong foundation for accurate and insightful analysis of the factors influencing air quality.</span>
<span id="cb17-245"><a href="#cb17-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-246"><a href="#cb17-246" aria-hidden="true" tabindex="-1"></a><span class="fu">### Feature Selection</span></span>
<span id="cb17-247"><a href="#cb17-247" aria-hidden="true" tabindex="-1"></a>Based on the results of Correlation Analysis and Multicollinearity Check. I decided to predict SO2 with 'TEMP', 'PRES', 'DEWP'. </span>
<span id="cb17-248"><a href="#cb17-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-249"><a href="#cb17-249" aria-hidden="true" tabindex="-1"></a><span class="fu">## 5. Linear Regression Analysis</span></span>
<span id="cb17-250"><a href="#cb17-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-251"><a href="#cb17-251" aria-hidden="true" tabindex="-1"></a>In this section, we will apply linear regression analysis to predict the concentration of sulfur dioxide (SO2) based on three key environmental factors: 'TEMP', 'PRES', and 'DEWP'. Linear regression is a fundamental statistical method used to understand the relationship between a dependent variable and one or more independent variables.</span>
<span id="cb17-252"><a href="#cb17-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-253"><a href="#cb17-253" aria-hidden="true" tabindex="-1"></a><span class="fu">### Introduction to Linear Regression and Its Applicability</span></span>
<span id="cb17-254"><a href="#cb17-254" aria-hidden="true" tabindex="-1"></a>Linear regression is a widely used statistical technique for modeling and analyzing the relationship between a scalar response (dependent variable) and one or more explanatory variables (independent variables). The method assumes a linear relationship between the variables. In our context, we will use linear regression to understand how temperature ('TEMP'), pressure ('PRES'), and dew point ('DEWP') affect the concentration of SO2 in the air.</span>
<span id="cb17-255"><a href="#cb17-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-256"><a href="#cb17-256" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step-by-Step Linear Regression Analysis Using Jupyter Notebook</span></span>
<span id="cb17-257"><a href="#cb17-257" aria-hidden="true" tabindex="-1"></a>Now, let's conduct a linear regression analysis using Python in a Jupyter Notebook environment.</span>
<span id="cb17-258"><a href="#cb17-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-261"><a href="#cb17-261" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-262"><a href="#cb17-262" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb17-263"><a href="#cb17-263" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb17-264"><a href="#cb17-264" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score</span>
<span id="cb17-265"><a href="#cb17-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-266"><a href="#cb17-266" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter out rows where any of the feature columns or 'SO2' is NaN</span></span>
<span id="cb17-267"><a href="#cb17-267" aria-hidden="true" tabindex="-1"></a>filtered_data <span class="op">=</span> sample_data.dropna(subset<span class="op">=</span>[<span class="st">'TEMP'</span>, <span class="st">'PRES'</span>, <span class="st">'DEWP'</span>, <span class="st">'SO2'</span>])</span>
<span id="cb17-268"><a href="#cb17-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-269"><a href="#cb17-269" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardizing the relevant columns of the filtered data</span></span>
<span id="cb17-270"><a href="#cb17-270" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb17-271"><a href="#cb17-271" aria-hidden="true" tabindex="-1"></a>scaled_columns <span class="op">=</span> scaler.fit_transform(filtered_data[[<span class="st">'TEMP'</span>, <span class="st">'PRES'</span>, <span class="st">'DEWP'</span>]])</span>
<span id="cb17-272"><a href="#cb17-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-273"><a href="#cb17-273" aria-hidden="true" tabindex="-1"></a><span class="co"># Converting scaled data back to a DataFrame</span></span>
<span id="cb17-274"><a href="#cb17-274" aria-hidden="true" tabindex="-1"></a>scaled_df <span class="op">=</span> pd.DataFrame(scaled_columns, columns<span class="op">=</span>[<span class="st">'TEMP'</span>, <span class="st">'PRES'</span>, <span class="st">'DEWP'</span>])</span>
<span id="cb17-275"><a href="#cb17-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-276"><a href="#cb17-276" aria-hidden="true" tabindex="-1"></a><span class="co"># Defining features (X) and target variable (y)</span></span>
<span id="cb17-277"><a href="#cb17-277" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> scaled_df</span>
<span id="cb17-278"><a href="#cb17-278" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> filtered_data[<span class="st">'SO2'</span>]</span>
<span id="cb17-279"><a href="#cb17-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-280"><a href="#cb17-280" aria-hidden="true" tabindex="-1"></a><span class="co"># Splitting the dataset into training and testing sets</span></span>
<span id="cb17-281"><a href="#cb17-281" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb17-282"><a href="#cb17-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-283"><a href="#cb17-283" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating and fitting the linear regression model</span></span>
<span id="cb17-284"><a href="#cb17-284" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression()</span>
<span id="cb17-285"><a href="#cb17-285" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb17-286"><a href="#cb17-286" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-287"><a href="#cb17-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-288"><a href="#cb17-288" aria-hidden="true" tabindex="-1"></a>In this code, we first select our features and target variable, split the data into training and test sets, create a Linear Regression model, and then fit it to our training data.</span>
<span id="cb17-289"><a href="#cb17-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-290"><a href="#cb17-290" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visual Representation of Linear Regression Results and Plotting the Best Fit Line</span></span>
<span id="cb17-291"><a href="#cb17-291" aria-hidden="true" tabindex="-1"></a>Visualizing the model's predictions in comparison with the actual values is crucial for assessing its performance. We'll also plot the best-fit line to better understand the linear relationship.</span>
<span id="cb17-292"><a href="#cb17-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-295"><a href="#cb17-295" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-296"><a href="#cb17-296" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicting SO2 values for the test set</span></span>
<span id="cb17-297"><a href="#cb17-297" aria-hidden="true" tabindex="-1"></a>lr_y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb17-298"><a href="#cb17-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-299"><a href="#cb17-299" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizing the actual vs predicted values and the best-fit line</span></span>
<span id="cb17-300"><a href="#cb17-300" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_test, lr_y_pred, alpha<span class="op">=</span><span class="fl">0.6</span>, color<span class="op">=</span><span class="st">'blue'</span>)  <span class="co"># Actual vs Predicted scatter plot</span></span>
<span id="cb17-301"><a href="#cb17-301" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Actual SO2'</span>)</span>
<span id="cb17-302"><a href="#cb17-302" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Predicted SO2'</span>)</span>
<span id="cb17-303"><a href="#cb17-303" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Actual vs Predicted SO2 Concentrations'</span>)</span>
<span id="cb17-304"><a href="#cb17-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-305"><a href="#cb17-305" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the best-fit line</span></span>
<span id="cb17-306"><a href="#cb17-306" aria-hidden="true" tabindex="-1"></a>plt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, lr_y_pred, <span class="dv">1</span>))(np.unique(y_test)), color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb17-307"><a href="#cb17-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-308"><a href="#cb17-308" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-309"><a href="#cb17-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-310"><a href="#cb17-310" aria-hidden="true" tabindex="-1"></a><span class="co"># Zoom in </span></span>
<span id="cb17-311"><a href="#cb17-311" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>, <span class="dv">200</span>)</span>
<span id="cb17-312"><a href="#cb17-312" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">40</span>)</span>
<span id="cb17-313"><a href="#cb17-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-314"><a href="#cb17-314" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizing the actual vs predicted values and the best-fit line</span></span>
<span id="cb17-315"><a href="#cb17-315" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_test, lr_y_pred, alpha<span class="op">=</span><span class="fl">0.6</span>, color<span class="op">=</span><span class="st">'blue'</span>)  <span class="co"># Actual vs Predicted scatter plot</span></span>
<span id="cb17-316"><a href="#cb17-316" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Actual SO2'</span>)</span>
<span id="cb17-317"><a href="#cb17-317" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Predicted SO2'</span>)</span>
<span id="cb17-318"><a href="#cb17-318" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Actual vs Predicted SO2 Concentrations'</span>)</span>
<span id="cb17-319"><a href="#cb17-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-320"><a href="#cb17-320" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the best-fit line</span></span>
<span id="cb17-321"><a href="#cb17-321" aria-hidden="true" tabindex="-1"></a>plt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, lr_y_pred, <span class="dv">1</span>))(np.unique(y_test)), color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb17-322"><a href="#cb17-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-323"><a href="#cb17-323" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-324"><a href="#cb17-324" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-325"><a href="#cb17-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-326"><a href="#cb17-326" aria-hidden="true" tabindex="-1"></a>The scatter plot shows the actual vs. predicted SO2 values, and the red line represents the linear fit, providing a visual indication of how well the model predicts SO2 concentration.</span>
<span id="cb17-327"><a href="#cb17-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-328"><a href="#cb17-328" aria-hidden="true" tabindex="-1"></a><span class="fu">### Evaluating the Performance of the Linear Regression Model</span></span>
<span id="cb17-329"><a href="#cb17-329" aria-hidden="true" tabindex="-1"></a>Finally, we evaluate the performance of our model using common statistical metrics.</span>
<span id="cb17-330"><a href="#cb17-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-333"><a href="#cb17-333" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-334"><a href="#cb17-334" aria-hidden="true" tabindex="-1"></a><span class="co"># Computing performance metrics</span></span>
<span id="cb17-335"><a href="#cb17-335" aria-hidden="true" tabindex="-1"></a>lr_mse <span class="op">=</span> mean_squared_error(y_test, lr_y_pred)</span>
<span id="cb17-336"><a href="#cb17-336" aria-hidden="true" tabindex="-1"></a>lr_r2 <span class="op">=</span> r2_score(y_test, lr_y_pred)</span>
<span id="cb17-337"><a href="#cb17-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-338"><a href="#cb17-338" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean Squared Error: </span><span class="sc">{</span>lr_mse<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-339"><a href="#cb17-339" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"R² Score: </span><span class="sc">{</span>lr_r2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-340"><a href="#cb17-340" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-341"><a href="#cb17-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-342"><a href="#cb17-342" aria-hidden="true" tabindex="-1"></a>The Mean Squared Error (MSE) provides an average of the squares of the errors, essentially quantifying the difference between predicted and actual values. The R² Score measures the proportion of the variance in the dependent variable that is predictable from the independent variables.</span>
<span id="cb17-343"><a href="#cb17-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-344"><a href="#cb17-344" aria-hidden="true" tabindex="-1"></a>By following these steps, we can use linear regression to effectively predict environmental factors' impact on air quality, specifically sulfur dioxide concentrations, and evaluate the accuracy of our predictions.</span>
<span id="cb17-345"><a href="#cb17-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-346"><a href="#cb17-346" aria-hidden="true" tabindex="-1"></a>Sure, I'll help you generate text for the "Random Forest Regression Analysis" section of your blog. Here's the content for that section:</span>
<span id="cb17-347"><a href="#cb17-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-348"><a href="#cb17-348" aria-hidden="true" tabindex="-1"></a><span class="fu">## 6. Random Forest Regression Analysis</span></span>
<span id="cb17-349"><a href="#cb17-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-350"><a href="#cb17-350" aria-hidden="true" tabindex="-1"></a><span class="fu">### Introduction to Random Forest Regression</span></span>
<span id="cb17-351"><a href="#cb17-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-352"><a href="#cb17-352" aria-hidden="true" tabindex="-1"></a>Random Forest is an ensemble learning method predominantly used for classification and regression tasks. It operates by constructing a multitude of decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Compared to linear regression, Random Forest offers several advantages:</span>
<span id="cb17-353"><a href="#cb17-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-354"><a href="#cb17-354" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Handling Non-linear Data**: It can model complex relationships between features and the target variable, which linear regression may fail to capture.</span>
<span id="cb17-355"><a href="#cb17-355" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Reducing Overfitting**: By averaging multiple decision trees, it reduces the risk of overfitting to the training data.</span>
<span id="cb17-356"><a href="#cb17-356" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Importance of Features**: Random Forest can provide insights into the relative importance of each feature in prediction.</span>
<span id="cb17-357"><a href="#cb17-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-358"><a href="#cb17-358" aria-hidden="true" tabindex="-1"></a><span class="fu">### Implementing Random Forest Regression</span></span>
<span id="cb17-359"><a href="#cb17-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-360"><a href="#cb17-360" aria-hidden="true" tabindex="-1"></a>Let's implement Random Forest regression to predict the concentration of sulfur dioxide (SO2) using 'TEMP' (temperature), 'PRES' (pressure), and 'DEWP' (dew point). We have already preprocessed and scaled our dataset. Now, we'll apply Random Forest regression:</span>
<span id="cb17-361"><a href="#cb17-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-364"><a href="#cb17-364" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-365"><a href="#cb17-365" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb17-366"><a href="#cb17-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-367"><a href="#cb17-367" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a Random Forest model</span></span>
<span id="cb17-368"><a href="#cb17-368" aria-hidden="true" tabindex="-1"></a>rf_model <span class="op">=</span> RandomForestRegressor(random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb17-369"><a href="#cb17-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-370"><a href="#cb17-370" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model to the training data</span></span>
<span id="cb17-371"><a href="#cb17-371" aria-hidden="true" tabindex="-1"></a>rf_model.fit(X_train, y_train)</span>
<span id="cb17-372"><a href="#cb17-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-373"><a href="#cb17-373" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicting the SO2 values using the test set</span></span>
<span id="cb17-374"><a href="#cb17-374" aria-hidden="true" tabindex="-1"></a>rf_y_pred <span class="op">=</span> rf_model.predict(X_test)</span>
<span id="cb17-375"><a href="#cb17-375" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-376"><a href="#cb17-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-377"><a href="#cb17-377" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visualization: Feature Importance and Prediction vs Actual</span></span>
<span id="cb17-378"><a href="#cb17-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-379"><a href="#cb17-379" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Feature Importance Plot**: This graph illustrates the relative importance of each feature in predicting the SO2 levels.</span>
<span id="cb17-380"><a href="#cb17-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-383"><a href="#cb17-383" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-384"><a href="#cb17-384" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-385"><a href="#cb17-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-386"><a href="#cb17-386" aria-hidden="true" tabindex="-1"></a>feature_importances <span class="op">=</span> rf_model.feature_importances_</span>
<span id="cb17-387"><a href="#cb17-387" aria-hidden="true" tabindex="-1"></a>plt.barh([<span class="st">'TEMP'</span>, <span class="st">'PRES'</span>, <span class="st">'DEWP'</span>], feature_importances)</span>
<span id="cb17-388"><a href="#cb17-388" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Feature Importance'</span>)</span>
<span id="cb17-389"><a href="#cb17-389" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Feature'</span>)</span>
<span id="cb17-390"><a href="#cb17-390" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Feature Importance in Random Forest Model'</span>)</span>
<span id="cb17-391"><a href="#cb17-391" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-392"><a href="#cb17-392" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-393"><a href="#cb17-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-394"><a href="#cb17-394" aria-hidden="true" tabindex="-1"></a>The feature importance plot shows 'TEMP' with the highest score, indicating it has the most significant impact on predicting SO2 levels, followed by 'PRES' and 'DEWP'. This suggests that temperature changes are potentially a more dominant factor in influencing SO2 concentrations in the atmosphere.</span>
<span id="cb17-395"><a href="#cb17-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-396"><a href="#cb17-396" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Prediction vs Actual Plot**: This plot compares the actual vs. predicted SO2 levels using the Random Forest model.</span>
<span id="cb17-397"><a href="#cb17-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-400"><a href="#cb17-400" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-401"><a href="#cb17-401" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_test, rf_y_pred)</span>
<span id="cb17-402"><a href="#cb17-402" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Actual SO2 Levels'</span>)</span>
<span id="cb17-403"><a href="#cb17-403" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Predicted SO2 Levels'</span>)</span>
<span id="cb17-404"><a href="#cb17-404" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Random Forest: Actual vs Predicted SO2 Levels'</span>)</span>
<span id="cb17-405"><a href="#cb17-405" aria-hidden="true" tabindex="-1"></a>plt.plot([y_test.<span class="bu">min</span>(), y_test.<span class="bu">max</span>()], [y_test.<span class="bu">min</span>(), y_test.<span class="bu">max</span>()], <span class="st">'k--'</span>, lw<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb17-406"><a href="#cb17-406" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-407"><a href="#cb17-407" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-408"><a href="#cb17-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-409"><a href="#cb17-409" aria-hidden="true" tabindex="-1"></a>The Prediction vs Actual Plot for the Random Forest model reveals a tighter clustering of data points along the line of perfect prediction compared to the Linear Regression model. This clustering indicates a higher accuracy in predictions made by the Random Forest model.</span>
<span id="cb17-410"><a href="#cb17-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-411"><a href="#cb17-411" aria-hidden="true" tabindex="-1"></a><span class="fu">## Comparative Analysis and Conclusion</span></span>
<span id="cb17-412"><a href="#cb17-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-413"><a href="#cb17-413" aria-hidden="true" tabindex="-1"></a>We compare the performance metrics of Random Forest and Linear Regression:</span>
<span id="cb17-414"><a href="#cb17-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-415"><a href="#cb17-415" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Random Forest**</span>
<span id="cb17-416"><a href="#cb17-416" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>MSE: 204.29218141691157</span>
<span id="cb17-417"><a href="#cb17-417" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>R²: 0.5579337989410323</span>
<span id="cb17-418"><a href="#cb17-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-419"><a href="#cb17-419" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Linear Regression**</span>
<span id="cb17-420"><a href="#cb17-420" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>MSE: 411.5799313674985</span>
<span id="cb17-421"><a href="#cb17-421" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>R²: 0.10938551133078755</span>
<span id="cb17-422"><a href="#cb17-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-423"><a href="#cb17-423" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Interpretation</span></span>
<span id="cb17-424"><a href="#cb17-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-425"><a href="#cb17-425" aria-hidden="true" tabindex="-1"></a>The Random Forest model shows a significantly lower Mean Squared Error (MSE) and higher R² value compared to Linear Regression. This indicates that the Random Forest model fits the data better and has a greater predictive accuracy. The reduced MSE suggests that the Random Forest model's predictions are closer to the actual data. The higher R² value indicates that a larger proportion of the variance in the SO2 concentration is being explained by the model.</span>
<span id="cb17-426"><a href="#cb17-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-427"><a href="#cb17-427" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visual Comparison: Prediction vs Actual Plot for Both Models</span></span>
<span id="cb17-428"><a href="#cb17-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-429"><a href="#cb17-429" aria-hidden="true" tabindex="-1"></a>This plot will compare the predictions of both models against the actual SO2 levels. Here, 'lr_y_pred' represents the predicted values from the Linear Regression model.</span>
<span id="cb17-430"><a href="#cb17-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-433"><a href="#cb17-433" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-434"><a href="#cb17-434" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_test, lr_y_pred, label<span class="op">=</span><span class="st">'Linear Regression'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'b'</span>, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb17-435"><a href="#cb17-435" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_test, rf_y_pred, label<span class="op">=</span><span class="st">'Random Forest'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'r'</span>, marker<span class="op">=</span><span class="st">'+'</span>)</span>
<span id="cb17-436"><a href="#cb17-436" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Actual SO2 Levels'</span>)</span>
<span id="cb17-437"><a href="#cb17-437" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Predicted SO2 Levels'</span>)</span>
<span id="cb17-438"><a href="#cb17-438" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Comparison of Predictions: Linear Regression vs Random Forest'</span>)</span>
<span id="cb17-439"><a href="#cb17-439" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb17-440"><a href="#cb17-440" aria-hidden="true" tabindex="-1"></a>plt.plot([y_test.<span class="bu">min</span>(), y_test.<span class="bu">max</span>()], [y_test.<span class="bu">min</span>(), y_test.<span class="bu">max</span>()], <span class="st">'k--'</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb17-441"><a href="#cb17-441" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-442"><a href="#cb17-442" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-443"><a href="#cb17-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-444"><a href="#cb17-444" aria-hidden="true" tabindex="-1"></a>The combined Prediction vs Actual Plot demonstrates a stark contrast between the two models. The Random Forest predictions are more concentrated around the line of perfect fit, while the Linear Regression predictions are more dispersed, indicating more errors in prediction. This visual reaffirms the quantitative metrics, illustrating that Random Forest provides a more accurate model for predicting SO2 levels based on 'TEMP', 'PRES', and 'DEWP'.</span>
<span id="cb17-445"><a href="#cb17-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-446"><a href="#cb17-446" aria-hidden="true" tabindex="-1"></a><span class="fu">### Limitation</span></span>
<span id="cb17-447"><a href="#cb17-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-448"><a href="#cb17-448" aria-hidden="true" tabindex="-1"></a>As depicted in the visualizations, there appear to be a few outliers in the graph. Conducting an outlier analysis before proceeding with modeling could potentially enhance the accuracy of our predictions.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>