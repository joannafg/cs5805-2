---
title: "3\\. Linear and Nonlinear Regression"
author: "Joanna Fang"
date: "2023-11-28"
categories: [ml, code, linear regression, nonlinear regression, pollution]
jupyter: python3
format:
  html: 
    code-block-bg: "#FFFFFF"
    code-block-border-left: "#E83283"
    toc: true
    code-tools:
      source: true
      toggle: false
      caption: none

---

# Predicting Air Pollutant Concentrations Using Linear and Random Forest Regression: A Jupyter Notebook Guide

![](thumbnail.jpg){width="20%" fig-align="center"}

## Introduction

Air quality is a critical environmental factor impacting public health, ecosystem sustainability, and the global climate. Pollutants such as particulate matter (PM2.5 and PM10), sulfur dioxide (SO2), nitrogen dioxide (NO2), carbon monoxide (CO), and ozone (O3) can have severe health impacts, including respiratory and cardiovascular diseases. Understanding and predicting the concentrations of these pollutants is essential for creating effective environmental policies and public health interventions.

In this blog, we'll delve into two powerful statistical methods used in predicting air pollutant concentrations: linear regression and Random Forest regression.

### Linear Regression
Linear regression is a fundamental statistical approach used to model the relationship between a dependent variable and one or more independent variables. In the context of air quality, it helps us understand how various environmental factors like temperature, humidity, and wind speed influence pollutant levels. The model assumes a linear relationship between the variables, which can be represented as:

$$
Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n + \epsilon
$$

Here, \( Y \) is the pollutant concentration we want to predict, \( X_1, X_2, ..., X_n \) are the environmental factors, \( \beta_0, \beta_1, ..., \beta_n \) are the coefficients to be estimated, and \( \epsilon \) is the error term.

### Random Forest Regression
In this blog, we'll harness the power of Random Forest—an ensemble learning method ideal for complex datasets with non-linear relationships—to predict air quality. Starting with an exploration of the `air_data_all.csv` dataset, we'll guide you through using Random Forest and other regression techniques in a Jupyter Notebook to analyze environmental conditions and temporal factors. By the end, you'll be well-equipped to apply these methods in Python for comprehensive environmental data analysis.

## Understanding the Dataset

Before delving into regression models, it's essential to familiarize ourselves with the dataset at hand—`air_data_all.csv`. This dataset contains hourly air quality measurements and meteorological data from Beijing, spanning from March 1st, 2013, to February 28th, 2017. The dataset is sourced from the Beijing Municipal Environmental Monitoring Center and is matched with meteorological data from the China Meteorological Administration. However, it's important to note that missing data points are marked as NA. Link to dataset is https://archive.ics.uci.edu/dataset/501/beijing+multi+site+air+quality+data. 

### Dataset Overview
The dataset is a valuable resource, encompassing a wide range of environmental conditions and pollutant concentrations. It records temporal information, including the year, month, day, and hour, alongside readings of key air pollutants such as PM2.5, PM10, SO2, NO2, CO, and O3. Additionally, meteorological factors like temperature (TEMP), pressure (PRES), dew point temperature (DEWP), precipitation (RAIN), wind direction (wd), and wind speed (WSPM) are included. This comprehensive data is instrumental for studying air pollution dynamics and its correlation with various environmental and temporal factors.

### Column Descriptions
Each column in the dataset serves a specific purpose:

1. **Temporal Data (year, month, day, hour)**: These columns provide insights into pollutant variations across different timescales.
2. **Pollutant Concentrations (PM2.5, PM10, SO2, NO2, CO, O3)**: These are primary pollutants, crucial for urban air quality analysis.
3. **Meteorological Data (TEMP, PRES, DEWP, RAIN, wd, WSPM)**: Weather conditions significantly impact pollutant dispersion and concentration.
4. **Station**: This column identifies the monitoring site, facilitating the study of geographical variations in air quality.

## Data Cleaning and Transformation

Before delving into sophisticated regression models, it's imperative to prepare our dataset, "air_data_all.csv," for analysis. This stage, known as data cleaning and transformation, involves several key steps to ensure the data's integrity and usability.

### Identifying and Handling Missing or Inconsistent Data
The initial step in data preprocessing is to identify and address any missing (NaN) or inconsistent data. This is crucial as such data can significantly skew our analysis.

```{python}
import sys
!{sys.executable} -m pip install seaborn
!{sys.executable} -m pip install matplotlib
!{sys.executable} -m pip install statsmodels
!{sys.executable} -m pip install scikit-learn
!{sys.executable} -m pip install pandas

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
```

```{python}
# Load the dataset
sample_data = pd.read_csv('air_data_all.csv')

# Identifying missing or infinite values
sample_data.replace([np.inf, -np.inf], np.nan, inplace=True)

# Checking for missing values
missing_values = sample_data.isnull().sum()
```

In this code block, we first attempt to replace any infinite values with NaNs. Then, we calculate the number of missing values in each column. Depending on the nature and volume of missing data, we can either fill these gaps using statistical methods (like mean, median) or consider removing the rows/columns entirely.

### Normalization or Standardization of Data
Normalization (rescaling data to a range, like 0–1) and standardization (shifting the distribution to have a mean of zero and a standard deviation of one) are crucial for models sensitive to the scale of data, such as linear regression.

```{python}
# Standardizing the dataset
scaler = StandardScaler()
scaled_data = scaler.fit_transform(sample_data[['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']])

# Converting scaled data back to a DataFrame for further use
scaled_df = pd.DataFrame(scaled_data, columns=['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM'])
```

Here, we use `StandardScaler` from Scikit-learn to standardize the continuous variables such as temperature and pressure. This process aligns the data onto one scale, removing bias due to different units or scales.

### Transforming Categorical Data into a Usable Format
Many regression models require numerical input, so transforming categorical data into a numerical format is essential.

```{python}
# Creating dummy variables for categorical data
wd_dummies = pd.get_dummies(sample_data['wd'])
sample_data = pd.concat([sample_data, wd_dummies], axis=1)
```

In the above snippet, we create dummy variables for the `wd` column (wind direction), converting it into a format that can be efficiently processed by regression algorithms.

### Visuals Showing Before and After Data Transformation
Visualizations are effective for demonstrating the impact of data transformation. For instance, before and after standardization, we can plot histograms of a variable to observe changes in its distribution.

```{python}
# Plotting before and after standardization
plt.hist(sample_data['TEMP'], bins=30, alpha=0.5, label='Original TEMP')
plt.hist(scaled_df['TEMP'], bins=30, alpha=0.5, label='Standardized TEMP')
plt.legend()
plt.show()
```

This histogram allows us to compare the distribution of the temperature data before and after standardization, showcasing the effects of our data transformation steps.

By completing these data cleaning and transformation processes, we ensure that our dataset is primed for accurate and effective regression analysis, laying a solid foundation for our subsequent modeling steps.

## Correlation Analysis and Multicollinearity Check

After preparing our dataset, the next step in our analysis involves understanding the relationships between variables using correlation analysis and checking for multicollinearity. These steps are critical for ensuring the reliability and interpretability of our regression models.

### Correlation Analysis and Its Importance
Correlation analysis helps us understand the strength and direction of the relationship between two variables. In regression analysis, it's important to identify how independent variables are related to the dependent variable and to each other.

```{python}
# Removing missing or infinite values from the scaled dataset
scaled_df.replace([np.inf, -np.inf], np.nan, inplace=True)
scaled_df.dropna(inplace=True)

# Calculating the correlation matrix for key variables
corr_matrix = sample_data[['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']].corr()

# Visualizing the correlation matrix using a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Correlation Matrix of Environmental Factors and Pollutants")
plt.show()
```

In this code, we calculate and visualize the correlation matrix of key pollutants and environmental factors. This heatmap provides a clear visual representation of the relationships, where the color intensity and the value in each cell indicate the strength and direction of the correlation.

### Multicollinearity Check and Its Implications
Multicollinearity occurs when two or more independent variables in a regression model are highly correlated. This can lead to unreliable coefficient estimates, making it difficult to determine the effect of each independent variable.

```{python}
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Preparing data for multicollinearity check
features = scaled_df[['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']]

# Calculating VIF for each feature
vif_data = pd.DataFrame()
vif_data['Feature'] = features.columns
vif_data['VIF'] = [variance_inflation_factor(features.values, i) for i in range(features.shape[1])]

vif_data
```

Here, we calculate the Variance Inflation Factor (VIF) for each feature. A VIF value greater than 5 or 10 indicates high multicollinearity, suggesting that the variable could be linearly predicted from the others with a substantial degree of accuracy.

### Visual Representation of Correlation and Multicollinearity Findings
Visualizing these statistics can help in better understanding and communicating the findings.

```{python}
# Visualizing VIF values
plt.bar(vif_data['Feature'], vif_data['VIF'])
plt.xlabel('Features')
plt.ylabel('Variance Inflation Factor (VIF)')
plt.title('Multicollinearity Check - VIF Values')
plt.show()
```

This bar chart provides a clear representation of the VIF values for each feature, helping us identify which variables might be contributing to multicollinearity in the model.

By conducting both correlation analysis and a multicollinearity check, we ensure the integrity and effectiveness of our regression models, setting a strong foundation for accurate and insightful analysis of the factors influencing air quality.

### Feature Selection
Based on the results of Correlation Analysis and Multicollinearity Check. I decided to predict SO2 with 'TEMP', 'PRES', 'DEWP'. 

## Linear Regression Analysis

In this section, we will apply linear regression analysis to predict the concentration of sulfur dioxide (SO2) based on three key environmental factors: 'TEMP', 'PRES', and 'DEWP'. Linear regression is a fundamental statistical method used to understand the relationship between a dependent variable and one or more independent variables.

### Introduction to Linear Regression and Its Applicability
Linear regression is a widely used statistical technique for modeling and analyzing the relationship between a scalar response (dependent variable) and one or more explanatory variables (independent variables). The method assumes a linear relationship between the variables. In our context, we will use linear regression to understand how temperature ('TEMP'), pressure ('PRES'), and dew point ('DEWP') affect the concentration of SO2 in the air.

### Step-by-Step Linear Regression Analysis Using Jupyter Notebook
Now, let's conduct a linear regression analysis using Python in a Jupyter Notebook environment.

```{python}
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Filter out rows where any of the feature columns or 'SO2' is NaN
filtered_data = sample_data.dropna(subset=['TEMP', 'PRES', 'DEWP', 'SO2'])

# Standardizing the relevant columns of the filtered data
scaler = StandardScaler()
scaled_columns = scaler.fit_transform(filtered_data[['TEMP', 'PRES', 'DEWP']])

# Converting scaled data back to a DataFrame
scaled_df = pd.DataFrame(scaled_columns, columns=['TEMP', 'PRES', 'DEWP'])

# Defining features (X) and target variable (y)
X = scaled_df
y = filtered_data['SO2']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Creating and fitting the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)
```

In this code, we first select our features and target variable, split the data into training and test sets, create a Linear Regression model, and then fit it to our training data.

### Visual Representation of Linear Regression Results and Plotting the Best Fit Line
Visualizing the model's predictions in comparison with the actual values is crucial for assessing its performance. We'll also plot the best-fit line to better understand the linear relationship.

```{python}
# Predicting SO2 values for the test set
lr_y_pred = model.predict(X_test)

# Visualizing the actual vs predicted values and the best-fit line
plt.scatter(y_test, lr_y_pred, alpha=0.6, color='blue')  # Actual vs Predicted scatter plot
plt.xlabel('Actual SO2')
plt.ylabel('Predicted SO2')
plt.title('Actual vs Predicted SO2 Concentrations')

# Plotting the best-fit line
plt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, lr_y_pred, 1))(np.unique(y_test)), color='red')

plt.show()

# Zoom in 
plt.xlim(0, 200)
plt.ylim(0, 40)

# Visualizing the actual vs predicted values and the best-fit line
plt.scatter(y_test, lr_y_pred, alpha=0.6, color='blue')  # Actual vs Predicted scatter plot
plt.xlabel('Actual SO2')
plt.ylabel('Predicted SO2')
plt.title('Actual vs Predicted SO2 Concentrations')

# Plotting the best-fit line
plt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, lr_y_pred, 1))(np.unique(y_test)), color='red')

plt.show()
```

The scatter plot shows the actual vs. predicted SO2 values, and the red line represents the linear fit, providing a visual indication of how well the model predicts SO2 concentration.

### Evaluating the Performance of the Linear Regression Model
Finally, we evaluate the performance of our model using common statistical metrics.

```{python}
# Computing performance metrics
lr_mse = mean_squared_error(y_test, lr_y_pred)
lr_r2 = r2_score(y_test, lr_y_pred)

print(f"Mean Squared Error: {lr_mse}")
print(f"R² Score: {lr_r2}")
```

The Mean Squared Error (MSE) provides an average of the squares of the errors, essentially quantifying the difference between predicted and actual values. The R² Score measures the proportion of the variance in the dependent variable that is predictable from the independent variables.

By following these steps, we can use linear regression to effectively predict environmental factors' impact on air quality, specifically sulfur dioxide concentrations, and evaluate the accuracy of our predictions.

Sure, I'll help you generate text for the "Random Forest Regression Analysis" section of your blog. Here's the content for that section:

## Random Forest Regression Analysis

### Introduction to Random Forest Regression

Random Forest is an ensemble learning method predominantly used for classification and regression tasks. It operates by constructing a multitude of decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Compared to linear regression, Random Forest offers several advantages:

- **Handling Non-linear Data**: It can model complex relationships between features and the target variable, which linear regression may fail to capture.
- **Reducing Overfitting**: By averaging multiple decision trees, it reduces the risk of overfitting to the training data.
- **Importance of Features**: Random Forest can provide insights into the relative importance of each feature in prediction.

### Implementing Random Forest Regression

Let's implement Random Forest regression to predict the concentration of sulfur dioxide (SO2) using 'TEMP' (temperature), 'PRES' (pressure), and 'DEWP' (dew point). We have already preprocessed and scaled our dataset. Now, we'll apply Random Forest regression:

```{python}
from sklearn.ensemble import RandomForestRegressor

# Create a Random Forest model
rf_model = RandomForestRegressor(random_state=0)

# Fit the model to the training data
rf_model.fit(X_train, y_train)

# Predicting the SO2 values using the test set
rf_y_pred = rf_model.predict(X_test)
```

### Visualization: Feature Importance and Prediction vs Actual

1. **Feature Importance Plot**: This graph illustrates the relative importance of each feature in predicting the SO2 levels.

```{python}
import matplotlib.pyplot as plt

feature_importances = rf_model.feature_importances_
plt.barh(['TEMP', 'PRES', 'DEWP'], feature_importances)
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title('Feature Importance in Random Forest Model')
plt.show()
```

The feature importance plot shows 'TEMP' with the highest score, indicating it has the most significant impact on predicting SO2 levels, followed by 'PRES' and 'DEWP'. This suggests that temperature changes are potentially a more dominant factor in influencing SO2 concentrations in the atmosphere.

2. **Prediction vs Actual Plot**: This plot compares the actual vs. predicted SO2 levels using the Random Forest model.

```{python}
plt.scatter(y_test, rf_y_pred)
plt.xlabel('Actual SO2 Levels')
plt.ylabel('Predicted SO2 Levels')
plt.title('Random Forest: Actual vs Predicted SO2 Levels')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)
plt.show()
```

The Prediction vs Actual Plot for the Random Forest model reveals a tighter clustering of data points along the line of perfect prediction compared to the Linear Regression model. This clustering indicates a higher accuracy in predictions made by the Random Forest model.

## Comparative Analysis and Conclusion

We compare the performance metrics of Random Forest and Linear Regression:

- **Random Forest**
  - MSE: 204.29218141691157
  - R²: 0.5579337989410323

- **Linear Regression**
  - MSE: 411.5799313674985
  - R²: 0.10938551133078755

#### Interpretation

The Random Forest model shows a significantly lower Mean Squared Error (MSE) and higher R² value compared to Linear Regression. This indicates that the Random Forest model fits the data better and has a greater predictive accuracy. The reduced MSE suggests that the Random Forest model's predictions are closer to the actual data. The higher R² value indicates that a larger proportion of the variance in the SO2 concentration is being explained by the model.

### Visual Comparison: Prediction vs Actual Plot for Both Models

This plot will compare the predictions of both models against the actual SO2 levels. Here, 'lr_y_pred' represents the predicted values from the Linear Regression model.

```{python}
plt.scatter(y_test, lr_y_pred, label='Linear Regression', alpha=0.5, color='b', marker='o')
plt.scatter(y_test, rf_y_pred, label='Random Forest', alpha=0.5, color='r', marker='+')
plt.xlabel('Actual SO2 Levels')
plt.ylabel('Predicted SO2 Levels')
plt.title('Comparison of Predictions: Linear Regression vs Random Forest')
plt.legend()
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.show()
```

The combined Prediction vs Actual Plot demonstrates a stark contrast between the two models. The Random Forest predictions are more concentrated around the line of perfect fit, while the Linear Regression predictions are more dispersed, indicating more errors in prediction. This visual reaffirms the quantitative metrics, illustrating that Random Forest provides a more accurate model for predicting SO2 levels based on 'TEMP', 'PRES', and 'DEWP'.

### Limitation

As depicted in the visualizations, there appear to be a few outliers in the graph. Conducting an outlier analysis before proceeding with modeling could potentially enhance the accuracy of our predictions.
