[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog, a personal journey through the intricate and fascinating world of Machine Learning. My name is Ziming ‘Joanna’ Fang, and I’m thrilled to have you join me in exploring various aspects of this dynamic field.\nCurrently pursuing my Master’s in Computer Science and Applications at Virginia Tech, I have a solid background in computer science and a keen interest in the practical applications of machine learning. This blog is not just a platform for sharing knowledge but also a reflection of my passion for understanding and leveraging the power of data and algorithms.\nHere’s what you can expect from my blog:\n\nProbability Theory and Random Variables: We’ll start with the basics, exploring how probability theory and random variables form the foundation of machine learning. Expect deep dives into theory, practical examples, and intriguing problems.\nClustering: Clustering algorithms are pivotal in understanding data segmentation. I’ll share insights on various clustering methods, their applications, and their role in data analysis.\nLinear and Nonlinear Regression: A journey through the realm of regression analysis, covering both linear and nonlinear approaches. I’ll elucidate these concepts with real-world examples and hands-on coding.\nClassification: Delve into the world of classification algorithms. I’ll break down complex concepts into digestible content, making it easier to understand how these algorithms categorize data.\nAnomaly/Outlier Detection: We’ll explore the techniques used to identify anomalies in data sets, an important aspect of data analysis and security.\n\nEach post will be enriched with machine learning code snippets and at least one data visualization, ensuring a comprehensive and practical learning experience.\nI’m building this blog with Quarto and hosting it on GitHub Pages. This approach aligns with my belief in transparency, accessibility, and the power of open-source tools. You’ll find my blog not just informative but also a testament to the power of programmatic website creation.\nOutside of academics, my experience as a Software Engineer at Avocado LLC and as a Research Assistant at the Mind Music Machine Lab has equipped me with a unique blend of skills. Whether it’s automating PDF news article processing with Python and ChatGPT, or applying sentiment extraction programs for emotional sonification in storytelling, I’ve always been at the intersection of innovation and practicality.\nSo, whether you’re a fellow machine learning enthusiast, a curious learner, or someone interested in the intersection of data and technology, I invite you to join me on this enlightening journey. Let’s explore the world of machine learning together!\nHappy reading and learning! Ziming ‘Joanna’ Fang 🌟🤖📊\nOpenAI. (2023). ChatGPT [Large language model]. https://chat.openai.com"
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "The analysis of driving behaviors, a task of both complexity and nuance, is increasingly significant in the realms of traffic safety and autonomous vehicle development. Machine learning, particularly clustering algorithms, stands at the forefront of this analysis, offering the ability to dissect and understand the myriad patterns hidden within driving data. Clustering, a form of unsupervised learning, excels in finding structure within unlabeled data, grouping similar data points together based on shared characteristics.\nIn this blog, we dive into a practical application of clustering using a dataset from Kaggle titled ‘Driving Behavior’. This dataset is intriguing because it comes with labeled data, categorizing driving behaviors into three types: SLOW, NORMAL, and AGGRESSIVE. While clustering typically operates on unlabeled data, we use this labeled dataset to our advantage. It will allow us to compare the clusters formed by our machine learning algorithm against the pre-labeled categories, providing an insightful backdrop to understand and validate our clustering results.\nOur aim is to illustrate the entire process of applying clustering to analyze driving behavior. We start with a predefined notion of having three clusters based on the labels present in the dataset. However, as we delve deeper into the analysis, we realize the necessity to step back and question our initial assumptions. We employ the Elbow Method, supplemented by the kneed Python package, to determine the optimal number of clusters in a more data-driven manner. This journey from assumption to optimization is not just about applying a machine learning technique but also about understanding the nuances and intricacies that come with real-world data analysis.\nJoin us as we unravel the complexities of driving behaviors through clustering, starting with predefined labels and moving towards an optimized, data-driven approach.\n\n\n\nIn the world of machine learning, the choice of dataset is as crucial as the algorithm itself. For our endeavor into clustering driving behaviors, we’ve chosen a dataset from Kaggle named ‘Driving Behavior’. This dataset is particularly interesting for a few reasons. Firstly, it’s labeled with specific driving behaviors, which, while not necessary for clustering, provides us with a unique opportunity to validate and understand our clustering results in a more tangible way.\n\n\nThis dataset offers a detailed glimpse into various aspects of driving, captured through different features:\n\nAcceleration (AccX, AccY, AccZ): These features measure the vehicle’s acceleration in meters per second squared (\\[m/s^2\\]) along the X, Y, and Z axes. Acceleration data is crucial for understanding sudden movements and changes in driving speed.\nRotation (GyroX, GyroY, GyroZ): Here, we have the vehicle’s angular velocity around the X, Y, and Z axes, measured in degrees per second (\\[°/s\\]). This data helps in identifying steering patterns and gauging the stability of the vehicle during various maneuvers.\nClassification Labels (SLOW, NORMAL, AGGRESSIVE): Each data point is tagged with one of these labels. It’s important to note that in clustering, such labels are not typically required. However, in our case, these labels will serve as a benchmark, enabling us to compare our unsupervised learning results with pre-defined categories.\n\nThis dataset not only provides a foundation for applying clustering techniques but also allows us to explore how well unsupervised learning can mirror human-labeled classifications.\n\n\n\nOur analysis will be powered by several key tools and libraries, each bringing its own strengths to the table:\n\nPython: Our language of choice, renowned for its ease of use and strong community support, especially in data science.\nScikit-learn: A powerful Python library for machine learning. We will use its clustering algorithms, specifically KMeans, to group our data.\nMatplotlib: This library will help us in visualizing our data and the results of our clustering, making the insights more accessible.\nkneed: An invaluable tool when it comes to determining the optimal number of clusters. It programmatically identifies the ‘elbow point’ in our dataset, a crucial step in ensuring our clustering approach is as effective as possible.\n\nIn the next sections, we’ll dive into the process of applying these tools to our dataset, starting with our initial approach and gradually moving towards a more refined, data-driven methodology.\n\n\n\n\nEmbarking on our journey to unravel the driving patterns hidden in the dataset, we initially gravitated towards a straightforward approach: clustering the data into three groups. This decision was influenced by the dataset’s labels - SLOW, NORMAL, and AGGRESSIVE. It seemed logical to align our clusters with these pre-defined categories, under the assumption that they would naturally encapsulate the essence of the driving behaviors.\n\n\nThe rationale was straightforward: the dataset labels suggested three distinct types of driving behaviors. Clustering aims to group similar data points, and with these labels as a guide, it seemed reasonable to start our analysis by segmenting the data into three clusters, hoping they would align with the labeled behaviors.\n\n\n\nThe KMeans algorithm was chosen for its simplicity and effectiveness in many clustering scenarios. Here’s a breakdown of how we applied it:\n\nimport sys\n!{sys.executable} -m pip install numpy\n!{sys.executable} -m pip install matplotlib\n!{sys.executable} -m pip install scipy\n!{sys.executable} -m pip install scikit-learn\n!{sys.executable} -m pip install pandas\n!{sys.executable} -m pip install kneed\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\n# Load the datasets\ndata_1 = pd.read_csv('motion_data_1.csv')\ndata_2 = pd.read_csv('motion_data_2.csv')\n\n# Combine datasets\ndata = pd.concat([data_1, data_2])\n\n# Data preprocessing\nscaler = StandardScaler()\nfeatures = data[['AccX', 'AccY', 'AccZ', 'GyroX', 'GyroY', 'GyroZ']]\nscaled_features = scaler.fit_transform(features)\n\n# Applying KMeans with 3 clusters\nkmeans = KMeans(n_clusters=3, random_state=42)\nclusters = kmeans.fit_predict(scaled_features)\n\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: numpy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.26.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: matplotlib in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (3.8.2)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.1.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (23.2)\nRequirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.1.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.45.1)\nRequirement already satisfied: numpy&lt;2,&gt;=1.21 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.2)\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib) (3.17.0)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: scipy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.11.4)\nRequirement already satisfied: numpy&lt;1.28.0,&gt;=1.21.6 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scipy) (1.26.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: scikit-learn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.3.2)\nRequirement already satisfied: joblib&gt;=1.1.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.3.2)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: scipy&gt;=1.5.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: numpy&lt;2.0,&gt;=1.17.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pandas in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (2.1.3)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: numpy&lt;2,&gt;=1.22.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: kneed in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.8.5)\nRequirement already satisfied: numpy&gt;=1.14.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from kneed) (1.26.2)\nRequirement already satisfied: scipy&gt;=1.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from kneed) (1.11.4)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\n\n\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nIn this code snippet, we begin by loading the dataset and selecting our features of interest, namely the acceleration and rotation measurements. These features are then scaled using StandardScaler, a crucial step to ensure that all features contribute equally to the clustering process. The KMeans algorithm is then applied to the scaled data, specifying three clusters.\n\n\n\nTo understand our initial clustering, we visualized the results:\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(scaled_features[:,0], scaled_features[:,1], c=clusters, cmap='viridis')\nplt.xlabel('Scaled AccX')\nplt.ylabel('Scaled AccY')\nplt.title('Initial Clustering with 3 Clusters')\nplt.show()\n\n\n\n\nThis visualization plots the scaled acceleration data on the X and Y axes, colored by the assigned cluster. It provides a first glimpse into how the data points are grouped by the algorithm.\n\n\n\nWhile starting with three clusters seemed logical, this approach had its limitations. Clustering, especially KMeans, doesn’t inherently consider pre-existing labels. It simply groups data based on feature similarity. Moreover, the true complexity of driving behavior might not be accurately captured in just three categories. This realization led us to question our initial assumption: Were three clusters really sufficient to capture the diverse nuances of driving behavior?\nIn the following section, we explore how we addressed these limitations by seeking an optimal number of clusters beyond the confines of the initial labeled categories.\n\n\n\n\nAfter the initial clustering, a crucial question arose: were three clusters truly representative of the driving behaviors in our dataset? This led us to the concept of cluster optimization, a pivotal step in ensuring that our machine learning model accurately reflects the complexities of the data.\n\n\nCluster optimization revolves around finding the ‘just right’ number of clusters in a dataset. Too few clusters, and the model might oversimplify the data, missing out on important nuances. Too many, and it might overfit, capturing random noise as meaningful patterns. The optimal number of clusters strikes a balance, grouping the data in a way that maximizes both similarity within clusters and differences between them.\n\n\n\nOur initial model with three clusters was a natural starting point, mirroring the three labeled categories in the dataset. However, this approach had its pitfalls:\n\nAssuming Label Completeness: The labeled categories in the dataset might not encompass all the distinct driving behaviors present. Real-world driving is complex and might not fit neatly into just three categories.\nIgnoring Unsupervised Learning Nature: Clustering, especially KMeans, is an unsupervised technique. It groups data based on feature similarity, independent of any pre-existing labels. Our model needed to reflect the data’s structure, not just its labels.\n\nThese considerations led us to explore beyond the confines of the three predefined categories, seeking a more data-driven approach to determine the number of clusters.\n\n\n\nThe Elbow Method emerged as our tool of choice for finding the optimal number of clusters. It involves plotting the model’s performance (measured in terms of Sum of Squared Errors, or SSE) against a range of cluster numbers. The ‘elbow point’ in this plot, where the rate of decrease in SSE sharply changes, indicates the optimal cluster count.\nIn the next section, we’ll delve into how we applied the Elbow Method to our dataset, using a combination of scikit-learn and the kneed package to not just visualize but also programmatically identify the elbow point, marking a significant step in refining our clustering model.\n\n\n\n\nWith the realization that our initial model might not optimally capture the driving behaviors in our dataset, we turned to the Elbow Method. This technique is a cornerstone in determining the appropriate number of clusters in unsupervised learning, particularly in KMeans clustering.\n\n\nThe Elbow Method is a heuristic used in cluster analysis to determine the number of clusters in a dataset. The method involves running the clustering algorithm multiple times, each time with a different number of clusters (\\[ k \\]), and calculating the Sum of Squared Errors (SSE) for each. SSE is defined as the sum of the squared distances between each member of a cluster and its centroid. As \\[ k \\] increases, SSE tends to decrease as the clusters will be smaller and tighter. However, after a certain point, the marginal decrease in SSE with increasing \\[ k \\] becomes insignificant, forming an ‘elbow’ in the plot. This point is considered the optimal number of clusters.\n\n\n\nTo apply the Elbow Method to our dataset, we first needed to calculate the SSE for a range of cluster numbers. Here’s how we did it:\n\nsse = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(scaled_features)\n    sse.append(kmeans.inertia_)\n\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nIn this code block, we looped over a range of cluster numbers (from 1 to 10) and for each number, we applied the KMeans algorithm to our scaled features. The inertia_ attribute of KMeans gives us the SSE for that particular cluster count. We stored these SSE values in a list.\n\n\n\nTo find the elbow point, we plotted the SSE against the number of clusters:\n\nplt.plot(range(1, 11), sse, marker='o')\nplt.title('Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Sum of Squared Errors (SSE)')\nplt.show()\n\n\n\n\nThis visualization was crucial as it allowed us to observe the point at which the decrease in SSE becomes less sharp, indicating the optimal number of clusters. In the next section, we’ll discuss how we used the kneed package to programmatically identify this elbow point, further refining our clustering approach.\n\n\n\n\nHaving visualized the Sum of Squared Errors (SSE) against various cluster counts using the Elbow Method, our next step was to pinpoint the elusive ‘elbow point’ programmatically. This is where the kneed Python package comes into play, offering a sophisticated yet straightforward approach to identifying the optimal number of clusters in our dataset.\n\n\nkneed is a Python library specifically designed to identify the knee point or elbow point in a dataset, which is often subjective and hard to pinpoint manually. It works by fitting a piecewise linear function to the data points and identifying the point of maximum curvature, which is the knee or elbow. This is particularly useful in KMeans clustering, as it takes the guesswork out of selecting the number of clusters.\n\n\n\nTo utilize kneed in our analysis, we first calculated the SSE for different cluster counts, just as we did for the Elbow Method. Then, kneed took over to programmatically identify the elbow point:\n\nfrom kneed import KneeLocator\n\n# Calculating SSE for a range of cluster counts\nsse = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(scaled_features)\n    sse.append(kmeans.inertia_)\n\n# Using kneed to find the elbow point\nknee_locator = KneeLocator(range(1, 11), sse, curve='convex', direction='decreasing')\nelbow_point = knee_locator.elbow\n\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nIn this code snippet, we first computed the SSE for cluster counts from 1 to 10. We then instantiated the KneeLocator class from the kneed package, passing the range of cluster counts and the corresponding SSE values. The curve='convex' and direction='decreasing' parameters helped kneed understand the nature of our SSE plot. The elbow attribute of the KneeLocator object gave us the optimal cluster count.\n\n\n\nTo our surprise, kneed identified 4 as the optimal number of clusters, deviating from our initial assumption of 3 clusters based on the dataset’s labels. This revelation was pivotal as it highlighted the importance of relying on data-driven methods rather than assumptions or external labeling.\n\nprint(f\"The optimal number of clusters identified by kneed: {elbow_point}\")\n\nThe optimal number of clusters identified by kneed: 4\n\n\nThis insight led us to reapply our clustering model with four clusters, a decision driven entirely by the underlying data structure rather than pre-defined categories. In the next section, we will revisit our clustering approach with this newfound understanding and analyze how this adjustment impacts our analysis of driving behaviors.\n\n\n\n\nArmed with the knowledge that four clusters might better represent our driving behavior data, we embarked on a re-clustering journey. This step was crucial for our analysis as it aligned our clustering approach more closely with the inherent structure of the data, moving beyond our initial assumptions.\n\n\nGuided by the kneed package’s recommendation, we reconfigured our KMeans clustering model to partition the data into four clusters instead of three. Here’s how we proceeded:\n\n# Applying KMeans with the optimized number of clusters\noptimized_kmeans = KMeans(n_clusters=4, random_state=42)\noptimized_clusters = optimized_kmeans.fit_predict(scaled_features)\n\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nIn this code, the KMeans class from scikit-learn was re-initialized with n_clusters set to 4. We then fit the model to our scaled features and predicted the cluster for each data point.\n\n\n\nVisualization plays a key role in understanding the implications of our clustering:\n\nplt.scatter(scaled_features[:,0], scaled_features[:,1], c=optimized_clusters, cmap='viridis')\nplt.xlabel('Scaled AccX')\nplt.ylabel('Scaled AccY')\nplt.title('Clustering with 4 Clusters')\nplt.show()\n\n\n\n\nIn this plot, we observed how the data points were grouped into four distinct clusters, providing a fresh perspective on the driving behaviors represented in our dataset.\n\n\n\nWith four clusters, the model had the flexibility to identify a more nuanced pattern within the data. This could mean uncovering a behavior that was previously merged with others in the three-cluster model or providing a clearer separation between behaviors.\n\n\n\nThe transition from three to four clusters was not just a numerical change but also a conceptual shift. It underscored the significance of letting the data guide our analysis:\n\nIncreased Granularity: The four-cluster model offered a finer categorization of driving behaviors, potentially leading to more accurate insights.\nData-Driven Approach: This shift highlighted the importance of relying on data-driven techniques, such as the Elbow Method and kneed, in machine learning tasks.\n\nThe decision to increase the number of clusters was a pivotal moment in our project. It exemplified the dynamic nature of machine learning, where initial hypotheses are continuously tested and refined in light of new evidence.\nIn the final section, we will summarize our findings, reflect on the journey, and discuss the potential applications and implications of our analysis in real-world scenarios.\n\n\n\n\nAs we conclude our exploration into clustering driving behaviors using the Kaggle dataset, it’s crucial to reflect on the key insights gained and the broader implications of our findings.\n\n\nOur journey through clustering revealed several important insights:\n\nBeyond Assumptions: While assumptions and labels can provide direction, they should not be the sole basis for decision-making in clustering problems.\nEmbracing Flexibility in Analysis: Machine learning, especially unsupervised learning like clustering, requires an openness to adapt analysis strategies based on what the data reveals.\nIterative Process: Optimization in machine learning is an iterative process. It often involves revisiting and refining models as new data or techniques become available.\n\nOur journey through clustering driving behaviors illustrates the dynamic and iterative nature of machine learning. By embracing a data-driven approach, we were able to uncover more about the dataset than what was initially apparent. Such methodologies are not just limited to driving behavior analysis but are applicable across various domains where data-driven insights are crucial.\nAs we continue to advance in the field of machine learning, the lessons learned here about the importance of flexibility, iteration, and data-driven analysis will remain fundamental to unlocking the full potential of our data."
  },
  {
    "objectID": "posts/clustering/index.html#introduction",
    "href": "posts/clustering/index.html#introduction",
    "title": "Clustering",
    "section": "",
    "text": "The analysis of driving behaviors, a task of both complexity and nuance, is increasingly significant in the realms of traffic safety and autonomous vehicle development. Machine learning, particularly clustering algorithms, stands at the forefront of this analysis, offering the ability to dissect and understand the myriad patterns hidden within driving data. Clustering, a form of unsupervised learning, excels in finding structure within unlabeled data, grouping similar data points together based on shared characteristics.\nIn this blog, we dive into a practical application of clustering using a dataset from Kaggle titled ‘Driving Behavior’. This dataset is intriguing because it comes with labeled data, categorizing driving behaviors into three types: SLOW, NORMAL, and AGGRESSIVE. While clustering typically operates on unlabeled data, we use this labeled dataset to our advantage. It will allow us to compare the clusters formed by our machine learning algorithm against the pre-labeled categories, providing an insightful backdrop to understand and validate our clustering results.\nOur aim is to illustrate the entire process of applying clustering to analyze driving behavior. We start with a predefined notion of having three clusters based on the labels present in the dataset. However, as we delve deeper into the analysis, we realize the necessity to step back and question our initial assumptions. We employ the Elbow Method, supplemented by the kneed Python package, to determine the optimal number of clusters in a more data-driven manner. This journey from assumption to optimization is not just about applying a machine learning technique but also about understanding the nuances and intricacies that come with real-world data analysis.\nJoin us as we unravel the complexities of driving behaviors through clustering, starting with predefined labels and moving towards an optimized, data-driven approach."
  },
  {
    "objectID": "posts/clustering/index.html#setting-the-stage-with-data-and-tools",
    "href": "posts/clustering/index.html#setting-the-stage-with-data-and-tools",
    "title": "Clustering",
    "section": "",
    "text": "In the world of machine learning, the choice of dataset is as crucial as the algorithm itself. For our endeavor into clustering driving behaviors, we’ve chosen a dataset from Kaggle named ‘Driving Behavior’. This dataset is particularly interesting for a few reasons. Firstly, it’s labeled with specific driving behaviors, which, while not necessary for clustering, provides us with a unique opportunity to validate and understand our clustering results in a more tangible way.\n\n\nThis dataset offers a detailed glimpse into various aspects of driving, captured through different features:\n\nAcceleration (AccX, AccY, AccZ): These features measure the vehicle’s acceleration in meters per second squared (\\[m/s^2\\]) along the X, Y, and Z axes. Acceleration data is crucial for understanding sudden movements and changes in driving speed.\nRotation (GyroX, GyroY, GyroZ): Here, we have the vehicle’s angular velocity around the X, Y, and Z axes, measured in degrees per second (\\[°/s\\]). This data helps in identifying steering patterns and gauging the stability of the vehicle during various maneuvers.\nClassification Labels (SLOW, NORMAL, AGGRESSIVE): Each data point is tagged with one of these labels. It’s important to note that in clustering, such labels are not typically required. However, in our case, these labels will serve as a benchmark, enabling us to compare our unsupervised learning results with pre-defined categories.\n\nThis dataset not only provides a foundation for applying clustering techniques but also allows us to explore how well unsupervised learning can mirror human-labeled classifications.\n\n\n\nOur analysis will be powered by several key tools and libraries, each bringing its own strengths to the table:\n\nPython: Our language of choice, renowned for its ease of use and strong community support, especially in data science.\nScikit-learn: A powerful Python library for machine learning. We will use its clustering algorithms, specifically KMeans, to group our data.\nMatplotlib: This library will help us in visualizing our data and the results of our clustering, making the insights more accessible.\nkneed: An invaluable tool when it comes to determining the optimal number of clusters. It programmatically identifies the ‘elbow point’ in our dataset, a crucial step in ensuring our clustering approach is as effective as possible.\n\nIn the next sections, we’ll dive into the process of applying these tools to our dataset, starting with our initial approach and gradually moving towards a more refined, data-driven methodology."
  },
  {
    "objectID": "posts/clustering/index.html#initial-clustering-approach",
    "href": "posts/clustering/index.html#initial-clustering-approach",
    "title": "Clustering",
    "section": "",
    "text": "Embarking on our journey to unravel the driving patterns hidden in the dataset, we initially gravitated towards a straightforward approach: clustering the data into three groups. This decision was influenced by the dataset’s labels - SLOW, NORMAL, and AGGRESSIVE. It seemed logical to align our clusters with these pre-defined categories, under the assumption that they would naturally encapsulate the essence of the driving behaviors.\n\n\nThe rationale was straightforward: the dataset labels suggested three distinct types of driving behaviors. Clustering aims to group similar data points, and with these labels as a guide, it seemed reasonable to start our analysis by segmenting the data into three clusters, hoping they would align with the labeled behaviors.\n\n\n\nThe KMeans algorithm was chosen for its simplicity and effectiveness in many clustering scenarios. Here’s a breakdown of how we applied it:\n\nimport sys\n!{sys.executable} -m pip install numpy\n!{sys.executable} -m pip install matplotlib\n!{sys.executable} -m pip install scipy\n!{sys.executable} -m pip install scikit-learn\n!{sys.executable} -m pip install pandas\n!{sys.executable} -m pip install kneed\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\n# Load the datasets\ndata_1 = pd.read_csv('motion_data_1.csv')\ndata_2 = pd.read_csv('motion_data_2.csv')\n\n# Combine datasets\ndata = pd.concat([data_1, data_2])\n\n# Data preprocessing\nscaler = StandardScaler()\nfeatures = data[['AccX', 'AccY', 'AccZ', 'GyroX', 'GyroY', 'GyroZ']]\nscaled_features = scaler.fit_transform(features)\n\n# Applying KMeans with 3 clusters\nkmeans = KMeans(n_clusters=3, random_state=42)\nclusters = kmeans.fit_predict(scaled_features)\n\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: numpy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.26.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: matplotlib in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (3.8.2)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.1.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (23.2)\nRequirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.1.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.45.1)\nRequirement already satisfied: numpy&lt;2,&gt;=1.21 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.2)\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib) (3.17.0)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: scipy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.11.4)\nRequirement already satisfied: numpy&lt;1.28.0,&gt;=1.21.6 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scipy) (1.26.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: scikit-learn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.3.2)\nRequirement already satisfied: joblib&gt;=1.1.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.3.2)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: scipy&gt;=1.5.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: numpy&lt;2.0,&gt;=1.17.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pandas in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (2.1.3)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: numpy&lt;2,&gt;=1.22.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: kneed in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.8.5)\nRequirement already satisfied: numpy&gt;=1.14.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from kneed) (1.26.2)\nRequirement already satisfied: scipy&gt;=1.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from kneed) (1.11.4)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\n\n\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nIn this code snippet, we begin by loading the dataset and selecting our features of interest, namely the acceleration and rotation measurements. These features are then scaled using StandardScaler, a crucial step to ensure that all features contribute equally to the clustering process. The KMeans algorithm is then applied to the scaled data, specifying three clusters.\n\n\n\nTo understand our initial clustering, we visualized the results:\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(scaled_features[:,0], scaled_features[:,1], c=clusters, cmap='viridis')\nplt.xlabel('Scaled AccX')\nplt.ylabel('Scaled AccY')\nplt.title('Initial Clustering with 3 Clusters')\nplt.show()\n\n\n\n\nThis visualization plots the scaled acceleration data on the X and Y axes, colored by the assigned cluster. It provides a first glimpse into how the data points are grouped by the algorithm.\n\n\n\nWhile starting with three clusters seemed logical, this approach had its limitations. Clustering, especially KMeans, doesn’t inherently consider pre-existing labels. It simply groups data based on feature similarity. Moreover, the true complexity of driving behavior might not be accurately captured in just three categories. This realization led us to question our initial assumption: Were three clusters really sufficient to capture the diverse nuances of driving behavior?\nIn the following section, we explore how we addressed these limitations by seeking an optimal number of clusters beyond the confines of the initial labeled categories."
  },
  {
    "objectID": "posts/clustering/index.html#realizing-the-need-for-optimization",
    "href": "posts/clustering/index.html#realizing-the-need-for-optimization",
    "title": "Clustering",
    "section": "",
    "text": "After the initial clustering, a crucial question arose: were three clusters truly representative of the driving behaviors in our dataset? This led us to the concept of cluster optimization, a pivotal step in ensuring that our machine learning model accurately reflects the complexities of the data.\n\n\nCluster optimization revolves around finding the ‘just right’ number of clusters in a dataset. Too few clusters, and the model might oversimplify the data, missing out on important nuances. Too many, and it might overfit, capturing random noise as meaningful patterns. The optimal number of clusters strikes a balance, grouping the data in a way that maximizes both similarity within clusters and differences between them.\n\n\n\nOur initial model with three clusters was a natural starting point, mirroring the three labeled categories in the dataset. However, this approach had its pitfalls:\n\nAssuming Label Completeness: The labeled categories in the dataset might not encompass all the distinct driving behaviors present. Real-world driving is complex and might not fit neatly into just three categories.\nIgnoring Unsupervised Learning Nature: Clustering, especially KMeans, is an unsupervised technique. It groups data based on feature similarity, independent of any pre-existing labels. Our model needed to reflect the data’s structure, not just its labels.\n\nThese considerations led us to explore beyond the confines of the three predefined categories, seeking a more data-driven approach to determine the number of clusters.\n\n\n\nThe Elbow Method emerged as our tool of choice for finding the optimal number of clusters. It involves plotting the model’s performance (measured in terms of Sum of Squared Errors, or SSE) against a range of cluster numbers. The ‘elbow point’ in this plot, where the rate of decrease in SSE sharply changes, indicates the optimal cluster count.\nIn the next section, we’ll delve into how we applied the Elbow Method to our dataset, using a combination of scikit-learn and the kneed package to not just visualize but also programmatically identify the elbow point, marking a significant step in refining our clustering model."
  },
  {
    "objectID": "posts/clustering/index.html#implementing-the-elbow-method",
    "href": "posts/clustering/index.html#implementing-the-elbow-method",
    "title": "Clustering",
    "section": "",
    "text": "With the realization that our initial model might not optimally capture the driving behaviors in our dataset, we turned to the Elbow Method. This technique is a cornerstone in determining the appropriate number of clusters in unsupervised learning, particularly in KMeans clustering.\n\n\nThe Elbow Method is a heuristic used in cluster analysis to determine the number of clusters in a dataset. The method involves running the clustering algorithm multiple times, each time with a different number of clusters (\\[ k \\]), and calculating the Sum of Squared Errors (SSE) for each. SSE is defined as the sum of the squared distances between each member of a cluster and its centroid. As \\[ k \\] increases, SSE tends to decrease as the clusters will be smaller and tighter. However, after a certain point, the marginal decrease in SSE with increasing \\[ k \\] becomes insignificant, forming an ‘elbow’ in the plot. This point is considered the optimal number of clusters.\n\n\n\nTo apply the Elbow Method to our dataset, we first needed to calculate the SSE for a range of cluster numbers. Here’s how we did it:\n\nsse = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(scaled_features)\n    sse.append(kmeans.inertia_)\n\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nIn this code block, we looped over a range of cluster numbers (from 1 to 10) and for each number, we applied the KMeans algorithm to our scaled features. The inertia_ attribute of KMeans gives us the SSE for that particular cluster count. We stored these SSE values in a list.\n\n\n\nTo find the elbow point, we plotted the SSE against the number of clusters:\n\nplt.plot(range(1, 11), sse, marker='o')\nplt.title('Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Sum of Squared Errors (SSE)')\nplt.show()\n\n\n\n\nThis visualization was crucial as it allowed us to observe the point at which the decrease in SSE becomes less sharp, indicating the optimal number of clusters. In the next section, we’ll discuss how we used the kneed package to programmatically identify this elbow point, further refining our clustering approach."
  },
  {
    "objectID": "posts/clustering/index.html#programmatic-elbow-point-detection-with-kneed",
    "href": "posts/clustering/index.html#programmatic-elbow-point-detection-with-kneed",
    "title": "Clustering",
    "section": "",
    "text": "Having visualized the Sum of Squared Errors (SSE) against various cluster counts using the Elbow Method, our next step was to pinpoint the elusive ‘elbow point’ programmatically. This is where the kneed Python package comes into play, offering a sophisticated yet straightforward approach to identifying the optimal number of clusters in our dataset.\n\n\nkneed is a Python library specifically designed to identify the knee point or elbow point in a dataset, which is often subjective and hard to pinpoint manually. It works by fitting a piecewise linear function to the data points and identifying the point of maximum curvature, which is the knee or elbow. This is particularly useful in KMeans clustering, as it takes the guesswork out of selecting the number of clusters.\n\n\n\nTo utilize kneed in our analysis, we first calculated the SSE for different cluster counts, just as we did for the Elbow Method. Then, kneed took over to programmatically identify the elbow point:\n\nfrom kneed import KneeLocator\n\n# Calculating SSE for a range of cluster counts\nsse = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(scaled_features)\n    sse.append(kmeans.inertia_)\n\n# Using kneed to find the elbow point\nknee_locator = KneeLocator(range(1, 11), sse, curve='convex', direction='decreasing')\nelbow_point = knee_locator.elbow\n\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nIn this code snippet, we first computed the SSE for cluster counts from 1 to 10. We then instantiated the KneeLocator class from the kneed package, passing the range of cluster counts and the corresponding SSE values. The curve='convex' and direction='decreasing' parameters helped kneed understand the nature of our SSE plot. The elbow attribute of the KneeLocator object gave us the optimal cluster count.\n\n\n\nTo our surprise, kneed identified 4 as the optimal number of clusters, deviating from our initial assumption of 3 clusters based on the dataset’s labels. This revelation was pivotal as it highlighted the importance of relying on data-driven methods rather than assumptions or external labeling.\n\nprint(f\"The optimal number of clusters identified by kneed: {elbow_point}\")\n\nThe optimal number of clusters identified by kneed: 4\n\n\nThis insight led us to reapply our clustering model with four clusters, a decision driven entirely by the underlying data structure rather than pre-defined categories. In the next section, we will revisit our clustering approach with this newfound understanding and analyze how this adjustment impacts our analysis of driving behaviors."
  },
  {
    "objectID": "posts/clustering/index.html#re-clustering-with-optimized-cluster-count",
    "href": "posts/clustering/index.html#re-clustering-with-optimized-cluster-count",
    "title": "Clustering",
    "section": "",
    "text": "Armed with the knowledge that four clusters might better represent our driving behavior data, we embarked on a re-clustering journey. This step was crucial for our analysis as it aligned our clustering approach more closely with the inherent structure of the data, moving beyond our initial assumptions.\n\n\nGuided by the kneed package’s recommendation, we reconfigured our KMeans clustering model to partition the data into four clusters instead of three. Here’s how we proceeded:\n\n# Applying KMeans with the optimized number of clusters\noptimized_kmeans = KMeans(n_clusters=4, random_state=42)\noptimized_clusters = optimized_kmeans.fit_predict(scaled_features)\n\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nIn this code, the KMeans class from scikit-learn was re-initialized with n_clusters set to 4. We then fit the model to our scaled features and predicted the cluster for each data point.\n\n\n\nVisualization plays a key role in understanding the implications of our clustering:\n\nplt.scatter(scaled_features[:,0], scaled_features[:,1], c=optimized_clusters, cmap='viridis')\nplt.xlabel('Scaled AccX')\nplt.ylabel('Scaled AccY')\nplt.title('Clustering with 4 Clusters')\nplt.show()\n\n\n\n\nIn this plot, we observed how the data points were grouped into four distinct clusters, providing a fresh perspective on the driving behaviors represented in our dataset.\n\n\n\nWith four clusters, the model had the flexibility to identify a more nuanced pattern within the data. This could mean uncovering a behavior that was previously merged with others in the three-cluster model or providing a clearer separation between behaviors.\n\n\n\nThe transition from three to four clusters was not just a numerical change but also a conceptual shift. It underscored the significance of letting the data guide our analysis:\n\nIncreased Granularity: The four-cluster model offered a finer categorization of driving behaviors, potentially leading to more accurate insights.\nData-Driven Approach: This shift highlighted the importance of relying on data-driven techniques, such as the Elbow Method and kneed, in machine learning tasks.\n\nThe decision to increase the number of clusters was a pivotal moment in our project. It exemplified the dynamic nature of machine learning, where initial hypotheses are continuously tested and refined in light of new evidence.\nIn the final section, we will summarize our findings, reflect on the journey, and discuss the potential applications and implications of our analysis in real-world scenarios."
  },
  {
    "objectID": "posts/clustering/index.html#conclusions-and-insights",
    "href": "posts/clustering/index.html#conclusions-and-insights",
    "title": "Clustering",
    "section": "",
    "text": "As we conclude our exploration into clustering driving behaviors using the Kaggle dataset, it’s crucial to reflect on the key insights gained and the broader implications of our findings.\n\n\nOur journey through clustering revealed several important insights:\n\nBeyond Assumptions: While assumptions and labels can provide direction, they should not be the sole basis for decision-making in clustering problems.\nEmbracing Flexibility in Analysis: Machine learning, especially unsupervised learning like clustering, requires an openness to adapt analysis strategies based on what the data reveals.\nIterative Process: Optimization in machine learning is an iterative process. It often involves revisiting and refining models as new data or techniques become available.\n\nOur journey through clustering driving behaviors illustrates the dynamic and iterative nature of machine learning. By embracing a data-driven approach, we were able to uncover more about the dataset than what was initially apparent. Such methodologies are not just limited to driving behavior analysis but are applicable across various domains where data-driven insights are crucial.\nAs we continue to advance in the field of machine learning, the lessons learned here about the importance of flexibility, iteration, and data-driven analysis will remain fundamental to unlocking the full potential of our data."
  },
  {
    "objectID": "posts/probability/index.html",
    "href": "posts/probability/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "The Binomial distribution is a cornerstone of probability theory, serving as a foundation for more complex distributions, including the Poisson distribution. This section aims to clarify the basics of the Binomial distribution.\n\n\nThe Binomial distribution is a probability distribution that models the number of successes in a fixed number of independent trials, with each trial having the same probability of success. It is particularly useful in scenarios with two possible outcomes, often labeled as “success” and “failure.”\n\n\n\n\nNumber of Trials (\\(n\\)): This denotes the total number of independent trials or experiments.\nProbability of Success (\\(p\\)): The probability of achieving a successful outcome in an individual trial.\nNumber of Successes (\\(k\\)): The specific count of successful outcomes we are interested in.\n\n\n\n\nThe probability of observing exactly \\(k\\) successes in \\(n\\) trials is described by the Binomial formula: \\[ P(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k} \\] Here, \\(\\binom{n}{k}\\) (pronounced “n choose k”) represents the number of ways to select \\(k\\) successes from \\(n\\) trials.\n\n\n\nConsider a scenario where you flip a coin 10 times. What is the probability of flipping exactly 4 heads? In this example: - \\(n = 10\\) (the total number of coin flips), - \\(p = 0.5\\) (the probability of flipping heads on any single coin flip), - \\(k = 4\\) (the number of heads we are trying to achieve).\nUsing the Binomial formula, the probability is: \\[ P(X = 4) = \\binom{10}{4} (0.5)^4 (1 - 0.5)^{10 - 4} \\]\n\n\n\nThe Binomial distribution is crucial in understanding binary outcomes across various fields such as psychology, medicine, and quality control. It provides a framework for scenarios with fixed trial numbers and clear success/failure outcomes. However, for large-scale or continuous-event contexts, the Poisson distribution becomes more relevant, as we will explore in subsequent sections.\n\n\n\n\nIn this section, we explore the intriguing relationship between the Binomial and Poisson distributions and how one transitions into the other under certain conditions. This transition is particularly important in scenarios involving a large number of trials and a small probability of success.\n\n\nThe Binomial distribution effectively models situations with a fixed number of independent trials and a constant probability of success in each trial. However, when we consider scenarios where the number of trials (\\(n\\)) is very large, and the probability of success in each trial (\\(p\\)) is very small, the Binomial distribution becomes less practical for calculations. This is where the Poisson distribution becomes relevant.\nThe key to this transition lies in the product of \\(n\\) and \\(p\\). As \\(n\\) becomes larger and \\(p\\) smaller, while their product \\(np\\) (representing the average number of successes) remains constant, the Binomial distribution approaches the Poisson distribution. This constant product, \\(np\\), is what we denote as \\(\\lambda\\) in the Poisson distribution.\n\n\n\nThe formula for the Poisson distribution is as follows: \\[ P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!} \\] In this equation, \\(X\\) is the random variable representing the number of successes, \\(k\\) is the specific number of successes we are interested in, \\(\\lambda\\) is the average rate of success, \\(e\\) is the base of the natural logarithm, and \\(k!\\) is the factorial of \\(k\\).\n\n\n\nTo further our understanding of the transition from the Binomial to the Poisson distribution, visual aids can be immensely helpful. In this section, we will use a series of graphs to illustrate how the Binomial distribution morphs into the Poisson distribution as the number of trials increases and the probability of success decreases.\n\nImporting Libraries First, we install and import the necessary Python libraries for our calculations and visualizations.\n\nimport sys\n!{sys.executable} -m pip install numpy\n!{sys.executable} -m pip install matplotlib\n!{sys.executable} -m pip install scipy\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom, poisson\n\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: numpy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.26.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: matplotlib in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (3.8.2)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.45.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.1.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.1.0)\nRequirement already satisfied: numpy&lt;2,&gt;=1.21 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.2)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (23.2)\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib) (3.17.0)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: scipy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.11.4)\nRequirement already satisfied: numpy&lt;1.28.0,&gt;=1.21.6 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scipy) (1.26.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\n\n\nSetting Parameters for the Distributions We define a range of \\(n\\) values to show how increasing the number of trials and decreasing the probability of success in each trial impacts the distribution. We also set a constant value for \\(p\\), the probability of success, and choose a value for \\(k\\), the number of successes we’re interested in.\n\nn_values = [20, 50, 100, 500]  # Increasing number of trials\np_values = [0.9, 0.8, 0.3, 0.01]  # Decreasing number of trials\nk = 5                          # Number of successes\n\nCalculating and Plotting the Distributions For each value of \\(n\\), we calculate the probabilities using both the Binomial and Poisson distributions and plot them for comparison.\n\nplt.figure(figsize=(12, 8))\n\nfor i, n in enumerate(n_values):\n    lambda_ = n * p_values[i]\n    x = np.arange(0, 20)\n    binom_pmf = binom.pmf(x, n, p_values[i])\n    poisson_pmf = poisson.pmf(x, lambda_)\n\n    plt.subplot(2, 2, i+1)\n    plt.plot(x, binom_pmf, 'o-', label=\"Binomial\")\n    plt.plot(x, poisson_pmf, 'x-', label=\"Poisson\")\n    plt.title(f'n = {n}, p = {p_values[i]}, lambda = {lambda_}')\n    plt.xlabel('k (Number of successes)')\n    plt.ylabel('Probability')\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nShifting Shapes: As \\(n\\) increases and \\(p\\) decreases, we observe that the shape of the Binomial distribution starts resembling that of the Poisson distribution. Initially, for smaller values of \\(n\\), the Binomial distribution might appear distinctly different. However, as \\(n\\) grows, the graphs showcase a closer alignment between the two distributions.\nConvergence to Poisson: The convergence of the Binomial distribution to the Poisson distribution is evident in these plots. The Poisson distribution begins to effectively approximate the Binomial distribution, especially as the product \\(np\\) (or \\(\\lambda\\)) remains constant.\nPractical Implications: This visual demonstration is crucial for understanding how the Poisson distribution can be used in real-life scenarios where the Binomial distribution is impractical due to a large number of trials. It highlights the flexibility and applicability of the Poisson distribution in various fields, from telecommunications to natural event modeling.\n\n\n\n\n\nNow that we have a foundational understanding of the Poisson distribution and its relationship with the Binomial distribution, let’s apply this knowledge to a practical scenario: the number of emails received per hour. This real-world example will illustrate how the Poisson distribution is used to model and understand everyday phenomena.\n\n\nConsider a situation where you’re monitoring the number of emails received in your office inbox. After some observation, you determine that, on average, you receive 5 emails per hour. In the context of the Poisson distribution, this average rate, 5 emails per hour, is our \\(\\lambda\\) (lambda).\n\n\n\nTo understand how the Poisson distribution works in this scenario, we will calculate the probabilities of receiving exactly 3, 5, or 10 emails in an hour. We’ll use Python to perform these calculations.\n\nDefining the Average Rate (\\(\\lambda\\)) Our average rate \\(\\lambda\\) is 5 emails per hour.\n\nlambda_ = 5  # Average number of emails per hour\n\nCalculating Probabilities We calculate the probability for receiving 3, 5, and 10 emails respectively.\n\nprobs = {}\nfor k in [0, 3, 5, 10, 15]:\n    probs[k] = poisson.pmf(k, lambda_)\n\nInterpreting the Results Let’s print out the probabilities.\n\nfor k, prob in probs.items():\n    print(f\"Probability of receiving exactly {k} emails: {prob:.4f}\")\n\nProbability of receiving exactly 0 emails: 0.0067\nProbability of receiving exactly 3 emails: 0.1404\nProbability of receiving exactly 5 emails: 0.1755\nProbability of receiving exactly 10 emails: 0.0181\nProbability of receiving exactly 15 emails: 0.0002\n\n\n\n\n\n\nNow, we’ll graph the Poisson distribution for our email scenario to visualize these probabilities.\n\nSetting Up the Plot We will create a plot that shows the probability of receiving a range of emails in an hour.\n\nx = np.arange(0, 15)  # Define the range of emails\ny = poisson.pmf(x, lambda_)\n\nplt.bar(x, y)\nplt.title(\"Poisson Distribution of Emails Received Per Hour\")\nplt.xlabel(\"Number of Emails\")\nplt.ylabel(\"Probability\")\nplt.xticks(x)\nplt.show()\n\n\n\n\n\n\n\n\n\nProbability Results: The calculated probabilities provide insights into the likelihood of different email counts. For instance, if the probability of receiving exactly 10 emails is low, it could indicate an unusually busy hour if it happens.\nGraphical Representation: The bar graph visually demonstrates the probabilities of different email counts per hour, emphasizing the most likely outcomes and showcasing the typical variance one might expect in their inbox.\n\n\n\n\nNext, just for fun, we will calculate and visualize the probability of receiving fewer than a certain number of emails per hour in case you want to know what’s the possibility of you need to handle less than 0, 3, 5, or 10 emails. For each threshold, we will calculate the cumulative probability of receiving less than that number of emails and visualize these probabilities using bar graphs with highlighted sections.\nThe cumulative probability for receiving fewer than a certain number of emails can be calculated using the cumulative distribution function (CDF) of the Poisson distribution.\n\nDefining the Average Rate (\\(\\lambda\\)) and Thresholds Our average rate, \\(\\lambda\\), is still 5 emails per hour. We also define our thresholds.\n\nlambda_ = 5  # Average number of emails per hour\nthresholds = [0, 3, 5, 10]\n\nCalculating Cumulative Probabilities We calculate the cumulative probability for each threshold.\n\ncdf_values = {}\nfor threshold in thresholds:\n    cdf_values[threshold] = poisson.cdf(threshold, lambda_)\n    print(f\"Probability of receiving less than {threshold} emails in an hour: {cdf_values[threshold]:.4f}\")\n\nProbability of receiving less than 0 emails in an hour: 0.0067\nProbability of receiving less than 3 emails in an hour: 0.2650\nProbability of receiving less than 5 emails in an hour: 0.6160\nProbability of receiving less than 10 emails in an hour: 0.9863\n\n\n\n\n\n\nWe will create a series of bar graphs to visually represent these probabilities. Each graph will highlight the bars representing the number of emails up to the threshold.\n\nSetting Up the Plot We define the range for the number of emails.\n\nx = np.arange(0, 15)  # Define the range of emails\ny = poisson.pmf(x, lambda_)\n\nCreating and Coloring the Graphs We create a separate graph for each threshold, coloring the bars up to that threshold differently.\n\nfor threshold in thresholds:\n    plt.figure(figsize=(6, 4))\n    plt.bar(x, y, color='grey')  # Default color\n    plt.bar(x[:threshold+1], y[:threshold+1], color='#E83283')  # Highlight up to the threshold\n    plt.title(f\"Probability of Receiving Fewer than {threshold} Emails\")\n    plt.xlabel(\"Number of Emails\")\n    plt.ylabel(\"Probability\")\n    plt.xticks(x)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Cumulative Probabilities: These graphs provide a visual representation of the cumulative probability of receiving fewer than a certain number of emails. A higher highlighted area indicates a greater likelihood of receiving fewer emails than the specified threshold.\nPractical Insights: Such visualizations can help individuals and businesses to anticipate and prepare for varying email volumes, thereby aiding in effective time management and resource allocation.\n\n\n\n\n\nHaving applied the Poisson distribution to the scenario of receiving emails per hour, let’s delve deeper into how this analysis can be beneficial for businesses or individuals and attempt to compare our theoretical findings with actual data.\n\n\nUnderstanding the pattern of email arrivals using the Poisson distribution can have significant practical applications, particularly in business settings.\n\nWorkload Management: For individuals or teams managing large volumes of emails, understanding the likelihood of receiving a certain number of emails can help in planning their workload. If the probability of receiving a high number of emails at certain hours is more, one can allocate more resources or time to manage this influx.\nStaffing in Customer Service: Customer service departments that rely heavily on email communication can use these predictions to staff their teams more efficiently. During hours predicted to have a higher volume of emails, more staff can be scheduled to ensure timely responses.\nPredictive Analysis for Planning: Businesses can use this data for predictive analysis. If certain days or times are consistently seeing a higher volume of emails, this information can be used for strategic planning, such as launching marketing emails or scheduling maintenance activities.\n\n\n\n\nTo demonstrate the practical application of our theoretical analysis, let’s compare the Poisson distribution with actual email data. For this example, we’ll assume a sample data set representing the number of emails received per hour over a week.\n\nGenerating Sample Data Let’s simulate some sample email data for this comparison.\n\nnp.random.seed(0)  # For reproducibility\nsample_data = np.random.poisson(lambda_, size=168)  # Simulating for 7 days (24 hours each)\n\nComparison with Theoretical Distribution We will plot the actual data alongside our theoretical Poisson distribution.\n\nplt.figure(figsize=(10, 5))\nplt.hist(sample_data, bins=range(15), alpha=0.7, label=\"Actual Data\")\nplt.plot(x, y * 168, 'o-', label=\"Theoretical Poisson Distribution\")  # 168 hours in a week\nplt.title(\"Comparison of Actual Email Data with Poisson Distribution\")\nplt.xlabel(\"Number of Emails\")\nplt.ylabel(\"Frequency\")\nplt.xticks(range(15))\nplt.legend()\nplt.show()\n\n\n\n\nIn the histogram generated above, the y-axis, labeled “Frequency,” represents the number of hours during the week when a specific number of emails were received.\nEach bar in the histogram corresponds to a particular number of emails received per hour (shown on the x-axis). The height of each bar indicates how many hours in the simulated week had that exact count of emails.\nFor example, if one of the bars represents 4 emails and its height reaches up to 10 on the y-axis, this means that there were 10 hours in the simulated week during which exactly 4 emails were received. The y-axis in this context is a count of the number of occurrences of each email count per hour across the entire week.\nTo calculate and plot the theoretical poisson distribution, x is an array representing different possible email counts per hour. y is calculated using the Poisson probability mass function (pmf) for each count in x, given the average rate lambda_. This y represents the theoretical probability of each email count per hour according to the Poisson distribution. The values in y are then multiplied by 168 (the total number of hours in the week) to scale these probabilities to the same total time frame as the actual data. The result is plotted as a line plot with markers (‘o-’) overlaid on the histogram. This line represents the expected frequency of each email count per hour over a week according to the Poisson distribution.\n\n\n\n\n\nAlignment with Theoretical Model: If the actual data closely aligns with the theoretical Poisson distribution, it validates the use of this model for predicting email patterns.\nIdentifying Anomalies: Any significant deviations from the theoretical distribution might indicate anomalies or special events, prompting further investigation.\nReal-World Relevance: This comparison underscores the relevance of the Poisson distribution in modeling real-world scenarios, providing a valuable tool for data-driven decision-making.\n\n\n\n\n\nThroughout this exploration, we’ve uncovered the Poisson distribution’s powerful utility in analyzing and forecasting infrequent but regular events, exemplified by email reception rates. This transition from the Binomial distribution, as visually depicted, underscores the Poisson distribution’s efficiency in scenarios with numerous trials and low success probabilities. In professional domains such as customer service, IT, and communications, it aids in workload management and resource allocation, ensuring adaptability. Likewise, in personal life, it assists in time and expectation management. This journey through probability and statistics highlights the Poisson distribution’s practical significance as a tool for deciphering our world, revealing hidden patterns in daily events like email flows. Ultimately, it’s not just a formula but a perspective enriching our personal and professional toolkits."
  },
  {
    "objectID": "posts/probability/index.html#understanding-the-binomial-distribution",
    "href": "posts/probability/index.html#understanding-the-binomial-distribution",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "The Binomial distribution is a cornerstone of probability theory, serving as a foundation for more complex distributions, including the Poisson distribution. This section aims to clarify the basics of the Binomial distribution.\n\n\nThe Binomial distribution is a probability distribution that models the number of successes in a fixed number of independent trials, with each trial having the same probability of success. It is particularly useful in scenarios with two possible outcomes, often labeled as “success” and “failure.”\n\n\n\n\nNumber of Trials (\\(n\\)): This denotes the total number of independent trials or experiments.\nProbability of Success (\\(p\\)): The probability of achieving a successful outcome in an individual trial.\nNumber of Successes (\\(k\\)): The specific count of successful outcomes we are interested in.\n\n\n\n\nThe probability of observing exactly \\(k\\) successes in \\(n\\) trials is described by the Binomial formula: \\[ P(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k} \\] Here, \\(\\binom{n}{k}\\) (pronounced “n choose k”) represents the number of ways to select \\(k\\) successes from \\(n\\) trials.\n\n\n\nConsider a scenario where you flip a coin 10 times. What is the probability of flipping exactly 4 heads? In this example: - \\(n = 10\\) (the total number of coin flips), - \\(p = 0.5\\) (the probability of flipping heads on any single coin flip), - \\(k = 4\\) (the number of heads we are trying to achieve).\nUsing the Binomial formula, the probability is: \\[ P(X = 4) = \\binom{10}{4} (0.5)^4 (1 - 0.5)^{10 - 4} \\]\n\n\n\nThe Binomial distribution is crucial in understanding binary outcomes across various fields such as psychology, medicine, and quality control. It provides a framework for scenarios with fixed trial numbers and clear success/failure outcomes. However, for large-scale or continuous-event contexts, the Poisson distribution becomes more relevant, as we will explore in subsequent sections."
  },
  {
    "objectID": "posts/probability/index.html#transitioning-to-the-poisson-distribution",
    "href": "posts/probability/index.html#transitioning-to-the-poisson-distribution",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "In this section, we explore the intriguing relationship between the Binomial and Poisson distributions and how one transitions into the other under certain conditions. This transition is particularly important in scenarios involving a large number of trials and a small probability of success.\n\n\nThe Binomial distribution effectively models situations with a fixed number of independent trials and a constant probability of success in each trial. However, when we consider scenarios where the number of trials (\\(n\\)) is very large, and the probability of success in each trial (\\(p\\)) is very small, the Binomial distribution becomes less practical for calculations. This is where the Poisson distribution becomes relevant.\nThe key to this transition lies in the product of \\(n\\) and \\(p\\). As \\(n\\) becomes larger and \\(p\\) smaller, while their product \\(np\\) (representing the average number of successes) remains constant, the Binomial distribution approaches the Poisson distribution. This constant product, \\(np\\), is what we denote as \\(\\lambda\\) in the Poisson distribution.\n\n\n\nThe formula for the Poisson distribution is as follows: \\[ P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!} \\] In this equation, \\(X\\) is the random variable representing the number of successes, \\(k\\) is the specific number of successes we are interested in, \\(\\lambda\\) is the average rate of success, \\(e\\) is the base of the natural logarithm, and \\(k!\\) is the factorial of \\(k\\).\n\n\n\nTo further our understanding of the transition from the Binomial to the Poisson distribution, visual aids can be immensely helpful. In this section, we will use a series of graphs to illustrate how the Binomial distribution morphs into the Poisson distribution as the number of trials increases and the probability of success decreases.\n\nImporting Libraries First, we install and import the necessary Python libraries for our calculations and visualizations.\n\nimport sys\n!{sys.executable} -m pip install numpy\n!{sys.executable} -m pip install matplotlib\n!{sys.executable} -m pip install scipy\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom, poisson\n\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: numpy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.26.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: matplotlib in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (3.8.2)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.45.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.1.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.1.0)\nRequirement already satisfied: numpy&lt;2,&gt;=1.21 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.2)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (23.2)\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib) (3.17.0)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: scipy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.11.4)\nRequirement already satisfied: numpy&lt;1.28.0,&gt;=1.21.6 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scipy) (1.26.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\n\n\nSetting Parameters for the Distributions We define a range of \\(n\\) values to show how increasing the number of trials and decreasing the probability of success in each trial impacts the distribution. We also set a constant value for \\(p\\), the probability of success, and choose a value for \\(k\\), the number of successes we’re interested in.\n\nn_values = [20, 50, 100, 500]  # Increasing number of trials\np_values = [0.9, 0.8, 0.3, 0.01]  # Decreasing number of trials\nk = 5                          # Number of successes\n\nCalculating and Plotting the Distributions For each value of \\(n\\), we calculate the probabilities using both the Binomial and Poisson distributions and plot them for comparison.\n\nplt.figure(figsize=(12, 8))\n\nfor i, n in enumerate(n_values):\n    lambda_ = n * p_values[i]\n    x = np.arange(0, 20)\n    binom_pmf = binom.pmf(x, n, p_values[i])\n    poisson_pmf = poisson.pmf(x, lambda_)\n\n    plt.subplot(2, 2, i+1)\n    plt.plot(x, binom_pmf, 'o-', label=\"Binomial\")\n    plt.plot(x, poisson_pmf, 'x-', label=\"Poisson\")\n    plt.title(f'n = {n}, p = {p_values[i]}, lambda = {lambda_}')\n    plt.xlabel('k (Number of successes)')\n    plt.ylabel('Probability')\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nShifting Shapes: As \\(n\\) increases and \\(p\\) decreases, we observe that the shape of the Binomial distribution starts resembling that of the Poisson distribution. Initially, for smaller values of \\(n\\), the Binomial distribution might appear distinctly different. However, as \\(n\\) grows, the graphs showcase a closer alignment between the two distributions.\nConvergence to Poisson: The convergence of the Binomial distribution to the Poisson distribution is evident in these plots. The Poisson distribution begins to effectively approximate the Binomial distribution, especially as the product \\(np\\) (or \\(\\lambda\\)) remains constant.\nPractical Implications: This visual demonstration is crucial for understanding how the Poisson distribution can be used in real-life scenarios where the Binomial distribution is impractical due to a large number of trials. It highlights the flexibility and applicability of the Poisson distribution in various fields, from telecommunications to natural event modeling."
  },
  {
    "objectID": "posts/probability/index.html#real-world-application---emails-per-hour",
    "href": "posts/probability/index.html#real-world-application---emails-per-hour",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "Now that we have a foundational understanding of the Poisson distribution and its relationship with the Binomial distribution, let’s apply this knowledge to a practical scenario: the number of emails received per hour. This real-world example will illustrate how the Poisson distribution is used to model and understand everyday phenomena.\n\n\nConsider a situation where you’re monitoring the number of emails received in your office inbox. After some observation, you determine that, on average, you receive 5 emails per hour. In the context of the Poisson distribution, this average rate, 5 emails per hour, is our \\(\\lambda\\) (lambda).\n\n\n\nTo understand how the Poisson distribution works in this scenario, we will calculate the probabilities of receiving exactly 3, 5, or 10 emails in an hour. We’ll use Python to perform these calculations.\n\nDefining the Average Rate (\\(\\lambda\\)) Our average rate \\(\\lambda\\) is 5 emails per hour.\n\nlambda_ = 5  # Average number of emails per hour\n\nCalculating Probabilities We calculate the probability for receiving 3, 5, and 10 emails respectively.\n\nprobs = {}\nfor k in [0, 3, 5, 10, 15]:\n    probs[k] = poisson.pmf(k, lambda_)\n\nInterpreting the Results Let’s print out the probabilities.\n\nfor k, prob in probs.items():\n    print(f\"Probability of receiving exactly {k} emails: {prob:.4f}\")\n\nProbability of receiving exactly 0 emails: 0.0067\nProbability of receiving exactly 3 emails: 0.1404\nProbability of receiving exactly 5 emails: 0.1755\nProbability of receiving exactly 10 emails: 0.0181\nProbability of receiving exactly 15 emails: 0.0002\n\n\n\n\n\n\nNow, we’ll graph the Poisson distribution for our email scenario to visualize these probabilities.\n\nSetting Up the Plot We will create a plot that shows the probability of receiving a range of emails in an hour.\n\nx = np.arange(0, 15)  # Define the range of emails\ny = poisson.pmf(x, lambda_)\n\nplt.bar(x, y)\nplt.title(\"Poisson Distribution of Emails Received Per Hour\")\nplt.xlabel(\"Number of Emails\")\nplt.ylabel(\"Probability\")\nplt.xticks(x)\nplt.show()\n\n\n\n\n\n\n\n\n\nProbability Results: The calculated probabilities provide insights into the likelihood of different email counts. For instance, if the probability of receiving exactly 10 emails is low, it could indicate an unusually busy hour if it happens.\nGraphical Representation: The bar graph visually demonstrates the probabilities of different email counts per hour, emphasizing the most likely outcomes and showcasing the typical variance one might expect in their inbox.\n\n\n\n\nNext, just for fun, we will calculate and visualize the probability of receiving fewer than a certain number of emails per hour in case you want to know what’s the possibility of you need to handle less than 0, 3, 5, or 10 emails. For each threshold, we will calculate the cumulative probability of receiving less than that number of emails and visualize these probabilities using bar graphs with highlighted sections.\nThe cumulative probability for receiving fewer than a certain number of emails can be calculated using the cumulative distribution function (CDF) of the Poisson distribution.\n\nDefining the Average Rate (\\(\\lambda\\)) and Thresholds Our average rate, \\(\\lambda\\), is still 5 emails per hour. We also define our thresholds.\n\nlambda_ = 5  # Average number of emails per hour\nthresholds = [0, 3, 5, 10]\n\nCalculating Cumulative Probabilities We calculate the cumulative probability for each threshold.\n\ncdf_values = {}\nfor threshold in thresholds:\n    cdf_values[threshold] = poisson.cdf(threshold, lambda_)\n    print(f\"Probability of receiving less than {threshold} emails in an hour: {cdf_values[threshold]:.4f}\")\n\nProbability of receiving less than 0 emails in an hour: 0.0067\nProbability of receiving less than 3 emails in an hour: 0.2650\nProbability of receiving less than 5 emails in an hour: 0.6160\nProbability of receiving less than 10 emails in an hour: 0.9863\n\n\n\n\n\n\nWe will create a series of bar graphs to visually represent these probabilities. Each graph will highlight the bars representing the number of emails up to the threshold.\n\nSetting Up the Plot We define the range for the number of emails.\n\nx = np.arange(0, 15)  # Define the range of emails\ny = poisson.pmf(x, lambda_)\n\nCreating and Coloring the Graphs We create a separate graph for each threshold, coloring the bars up to that threshold differently.\n\nfor threshold in thresholds:\n    plt.figure(figsize=(6, 4))\n    plt.bar(x, y, color='grey')  # Default color\n    plt.bar(x[:threshold+1], y[:threshold+1], color='#E83283')  # Highlight up to the threshold\n    plt.title(f\"Probability of Receiving Fewer than {threshold} Emails\")\n    plt.xlabel(\"Number of Emails\")\n    plt.ylabel(\"Probability\")\n    plt.xticks(x)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Cumulative Probabilities: These graphs provide a visual representation of the cumulative probability of receiving fewer than a certain number of emails. A higher highlighted area indicates a greater likelihood of receiving fewer emails than the specified threshold.\nPractical Insights: Such visualizations can help individuals and businesses to anticipate and prepare for varying email volumes, thereby aiding in effective time management and resource allocation."
  },
  {
    "objectID": "posts/probability/index.html#deep-dive-into-the-email-example",
    "href": "posts/probability/index.html#deep-dive-into-the-email-example",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "Having applied the Poisson distribution to the scenario of receiving emails per hour, let’s delve deeper into how this analysis can be beneficial for businesses or individuals and attempt to compare our theoretical findings with actual data.\n\n\nUnderstanding the pattern of email arrivals using the Poisson distribution can have significant practical applications, particularly in business settings.\n\nWorkload Management: For individuals or teams managing large volumes of emails, understanding the likelihood of receiving a certain number of emails can help in planning their workload. If the probability of receiving a high number of emails at certain hours is more, one can allocate more resources or time to manage this influx.\nStaffing in Customer Service: Customer service departments that rely heavily on email communication can use these predictions to staff their teams more efficiently. During hours predicted to have a higher volume of emails, more staff can be scheduled to ensure timely responses.\nPredictive Analysis for Planning: Businesses can use this data for predictive analysis. If certain days or times are consistently seeing a higher volume of emails, this information can be used for strategic planning, such as launching marketing emails or scheduling maintenance activities.\n\n\n\n\nTo demonstrate the practical application of our theoretical analysis, let’s compare the Poisson distribution with actual email data. For this example, we’ll assume a sample data set representing the number of emails received per hour over a week.\n\nGenerating Sample Data Let’s simulate some sample email data for this comparison.\n\nnp.random.seed(0)  # For reproducibility\nsample_data = np.random.poisson(lambda_, size=168)  # Simulating for 7 days (24 hours each)\n\nComparison with Theoretical Distribution We will plot the actual data alongside our theoretical Poisson distribution.\n\nplt.figure(figsize=(10, 5))\nplt.hist(sample_data, bins=range(15), alpha=0.7, label=\"Actual Data\")\nplt.plot(x, y * 168, 'o-', label=\"Theoretical Poisson Distribution\")  # 168 hours in a week\nplt.title(\"Comparison of Actual Email Data with Poisson Distribution\")\nplt.xlabel(\"Number of Emails\")\nplt.ylabel(\"Frequency\")\nplt.xticks(range(15))\nplt.legend()\nplt.show()\n\n\n\n\nIn the histogram generated above, the y-axis, labeled “Frequency,” represents the number of hours during the week when a specific number of emails were received.\nEach bar in the histogram corresponds to a particular number of emails received per hour (shown on the x-axis). The height of each bar indicates how many hours in the simulated week had that exact count of emails.\nFor example, if one of the bars represents 4 emails and its height reaches up to 10 on the y-axis, this means that there were 10 hours in the simulated week during which exactly 4 emails were received. The y-axis in this context is a count of the number of occurrences of each email count per hour across the entire week.\nTo calculate and plot the theoretical poisson distribution, x is an array representing different possible email counts per hour. y is calculated using the Poisson probability mass function (pmf) for each count in x, given the average rate lambda_. This y represents the theoretical probability of each email count per hour according to the Poisson distribution. The values in y are then multiplied by 168 (the total number of hours in the week) to scale these probabilities to the same total time frame as the actual data. The result is plotted as a line plot with markers (‘o-’) overlaid on the histogram. This line represents the expected frequency of each email count per hour over a week according to the Poisson distribution.\n\n\n\n\n\nAlignment with Theoretical Model: If the actual data closely aligns with the theoretical Poisson distribution, it validates the use of this model for predicting email patterns.\nIdentifying Anomalies: Any significant deviations from the theoretical distribution might indicate anomalies or special events, prompting further investigation.\nReal-World Relevance: This comparison underscores the relevance of the Poisson distribution in modeling real-world scenarios, providing a valuable tool for data-driven decision-making."
  },
  {
    "objectID": "posts/probability/index.html#conclusion",
    "href": "posts/probability/index.html#conclusion",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "Throughout this exploration, we’ve uncovered the Poisson distribution’s powerful utility in analyzing and forecasting infrequent but regular events, exemplified by email reception rates. This transition from the Binomial distribution, as visually depicted, underscores the Poisson distribution’s efficiency in scenarios with numerous trials and low success probabilities. In professional domains such as customer service, IT, and communications, it aids in workload management and resource allocation, ensuring adaptability. Likewise, in personal life, it assists in time and expectation management. This journey through probability and statistics highlights the Poisson distribution’s practical significance as a tool for deciphering our world, revealing hidden patterns in daily events like email flows. Ultimately, it’s not just a formula but a perspective enriching our personal and professional toolkits."
  },
  {
    "objectID": "posts/outlier4/index.html",
    "href": "posts/outlier4/index.html",
    "title": "5. Anomaly/Outlier Detection",
    "section": "",
    "text": "Welcome to our exploration of “Detecting Anomalies in Air Pollution Data,” a vital project in the realm of environmental monitoring. With increasing concerns about air quality and its impact on public health and the environment, identifying irregularities in air pollution data has never been more critical.\nThis project leverages a comprehensive dataset from the Beijing Multi-site Air Quality Data, which offers a rich tapestry of air pollutant measurements and meteorological data across various sites in Beijing. The data spans from 2013 to 2017, providing insights into pollutants like PM2.5, PM10, SO2, NO2, and CO, as well as meteorological conditions like temperature, humidity, and wind speed.\nOur primary goal is to detect unusual patterns or outliers in air quality data that might signify environmental hazards, technical errors in data collection, or significant meteorological impacts. By accomplishing this, we aim to contribute to more effective environmental monitoring and policy-making.\n\n\n\n\n\nThe first step in our data science journey involves getting acquainted with the dataset’s structure and characteristics. This involves examining the various columns of the dataset, which include both pollutant levels and meteorological factors.\n\n# Install and import all libraries\nimport sys\n!{sys.executable} -m pip install seaborn\n!{sys.executable} -m pip install matplotlib\n!{sys.executable} -m pip install scikit-learn\n!{sys.executable} -m pip install pandas\n!{sys.executable} -m pip install numpy\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import IsolationForest\n\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: seaborn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.13.0)\nRequirement already satisfied: matplotlib!=3.6.1,&gt;=3.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (3.8.2)\nRequirement already satisfied: numpy!=1.24.0,&gt;=1.20 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (1.26.2)\nRequirement already satisfied: pandas&gt;=1.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (2.1.3)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (0.12.1)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (23.2)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (6.1.1)\nRequirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (10.1.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (4.45.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.4.5)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (2.8.2)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (3.1.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.2.0)\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (3.17.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3.post1)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: matplotlib in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (3.8.2)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.1.1)\nRequirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.1.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.45.1)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (23.2)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: numpy&lt;2,&gt;=1.21 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib) (3.17.0)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: scikit-learn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.3.2)\nRequirement already satisfied: numpy&lt;2.0,&gt;=1.17.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.2)\nRequirement already satisfied: scipy&gt;=1.5.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: joblib&gt;=1.1.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.3.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pandas in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (2.1.3)\nRequirement already satisfied: numpy&lt;2,&gt;=1.22.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: numpy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.26.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\n\n\n\nsample_data_org = pd.read_csv('air_data_all.csv')\nsample_data_org.head()\n\n\n\n\n\n\n\n\nNo\nyear\nmonth\nday\nhour\nPM2.5\nPM10\nSO2\nNO2\nCO\nO3\nTEMP\nPRES\nDEWP\nRAIN\nwd\nWSPM\nstation\n\n\n\n\n0\n1\n2013\n3\n1\n0\n6.0\n18.0\n5.0\nNaN\n800.0\n88.0\n0.1\n1021.1\n-18.6\n0.0\nNW\n4.4\nGucheng\n\n\n1\n2\n2013\n3\n1\n1\n6.0\n15.0\n5.0\nNaN\n800.0\n88.0\n-0.3\n1021.5\n-19.0\n0.0\nNW\n4.0\nGucheng\n\n\n2\n3\n2013\n3\n1\n2\n5.0\n18.0\nNaN\nNaN\n700.0\n52.0\n-0.7\n1021.5\n-19.8\n0.0\nWNW\n4.6\nGucheng\n\n\n3\n4\n2013\n3\n1\n3\n6.0\n20.0\n6.0\nNaN\nNaN\nNaN\n-1.0\n1022.7\n-21.2\n0.0\nW\n2.8\nGucheng\n\n\n4\n5\n2013\n3\n1\n4\n5.0\n17.0\n5.0\nNaN\n600.0\n73.0\n-1.3\n1023.0\n-21.4\n0.0\nWNW\n3.6\nGucheng\n\n\n\n\n\n\n\nBy running this code, we get a glimpse of the first few rows of our dataset, allowing us to understand the types of data we will be working with.\n\n\n\nDealing with missing data and categorical variables is a crucial part of data preprocessing. To address this, we first identify the missing values and then decide on an appropriate strategy, such as imputation or removal. However, in later processing, we realized we hadn’t done enough here.\n\nmissing_values = sample_data_org.isnull().sum()\nmissing_values\n\nNo             0\nyear           0\nmonth          0\nday            0\nhour           0\nPM2.5       8739\nPM10        6449\nSO2         9021\nNO2        12116\nCO         20701\nO3         13277\nTEMP         398\nPRES         393\nDEWP         403\nRAIN         390\nwd          1822\nWSPM         318\nstation        0\ndtype: int64\n\n\n\n# Remove rows with missing values\nsample_data = sample_data_org.dropna()\n\n# Identify numerical columns\nnumerical_cols = sample_data.select_dtypes(include=['int64', 'float64']).columns\n\n# Create a pipeline for imputing missing values and scaling\npipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),  # Replace missing values with mean\n    ('scaler', StandardScaler()),                # Scale the data\n])\n\n# Apply the pipeline to the numerical columns\nscaled_data = pipeline.fit_transform(sample_data[numerical_cols])\n\n# Apply PCA\npca = PCA(n_components=0.95)  # Retain 95% of the variance\nprincipal_components = pca.fit_transform(scaled_data)\n\n# Identify non-numeric (categorical) columns\ncategorical_cols = sample_data.select_dtypes(include=['object']).columns\n\n# One-hot encode the categorical data\nencoder = OneHotEncoder(sparse=False)\ncategorical_encoded = encoder.fit_transform(sample_data[categorical_cols])\n\n# Check for 'get_feature_names_out' method for naming columns\nif hasattr(encoder, 'get_feature_names_out'):\n    encoded_columns = pd.DataFrame(categorical_encoded, columns=encoder.get_feature_names_out(categorical_cols))\nelse:\n    # Fallback: manually create feature names\n    encoded_columns = pd.DataFrame(categorical_encoded)\n    encoded_columns.columns = [col + '_' + str(i) for col in categorical_cols for i in range(encoded_columns.shape[1])]\n\n# Concatenate the encoded columns with the original dataset and drop the original categorical columns\nsample_data_encoded = pd.concat([sample_data.drop(categorical_cols, axis=1), encoded_columns], axis=1)\n\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n\n\nFor categorical variables like wind direction, we use encoding techniques to convert them into numerical form, making them suitable for analysis.\n\n\n\nGiven the varying scales of our numerical features, normalization or standardization becomes necessary. This step ensures that no single feature disproportionately influences the model due to its scale.\n\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(sample_data[['PM2.5', 'PM10', 'TEMP', 'PRES']])\n\n\n\n\nFinally, we perform feature selection and engineering. This process involves attempting to choose the most relevant features and possibly creating new features to improve our model’s performance.\n\nCorrelation Analysis: First, we can perform a correlation analysis to understand the relationships between different features. This can help in identifying features that are strongly correlated with each other, from which we can select the most relevant ones.\n\n\n# Now perform the correlation analysis on the numerical data\ncorr = sample_data_encoded.corr()\n\n# Generate a heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, fmt=\".2f\")\nplt.show()\n\n\n\n\nThe heatmap visualizes the correlation coefficients between different variables related to air quality and weather conditions. The scale on the right indicates the strength of the correlation, ranging from -1 (a perfect negative correlation) to 1 (a perfect positive correlation). Dark red shades indicate strong positive correlations, whereas dark blue shades represent strong negative correlations. Most variables do not exhibit a strong correlation with each other, as indicated by the shades of white (near-zero correlation). This kind of visualization helps to identify relationships between variables, which can be further investigated for causal connections or dependencies.\n\nPrincipal Component Analysis (PCA): PCA is a technique used to reduce the dimensionality of the data, enhancing the interpretability while minimizing information loss.\n\n\npca = PCA(n_components=0.95) # Retain 95% of the variance\nprincipal_components = pca.fit_transform(scaled_data)\n\nThis code applies PCA to the scaled data, reducing the number of features while retaining 95% of the variance in the data.\n\nFeature Engineering: Here, we are attempting to create a new feature that might be more indicative of anomalies by creating a composite air quality index from multiple pollutants.\n\n\nsample_data['Air_Quality_Index'] = sample_data['PM2.5'] * 0.4 + sample_data['PM10'] * 0.2 + sample_data['NO2'] * 0.2 + sample_data['SO2'] * 0.1 + sample_data['CO'] * 0.1\n\n/var/folders/pt/983cdyd950gd2j6l1gv1376r0000gn/T/ipykernel_18793/3115101989.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  sample_data['Air_Quality_Index'] = sample_data['PM2.5'] * 0.4 + sample_data['PM10'] * 0.2 + sample_data['NO2'] * 0.2 + sample_data['SO2'] * 0.1 + sample_data['CO'] * 0.1\n\n\nThis code creates a new feature, ‘Air_Quality_Index’, as a weighted sum of various pollutants, hypothesizing that this composite index might be a more effective predictor of anomalies.\nThrough these steps, we attempted to refine our dataset to include the most relevant features for anomaly detection, enhancing the model’s accuracy and efficiency.\n\n\n\n\n\n\nAfter trying out other models, which took more than 5 minutes to run, for our project on air pollution data, we have opted for the Isolation Forest algorithm due to its efficiency and effectiveness, especially in dealing with large and high-dimensional datasets like ours.\n\niso_forest = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)\n\n\n\n\nWe split our dataset into training and test sets, ensuring that the model is evaluated on unseen data, reflecting its performance in real-world scenarios.\n\n# Handling NaN Values with Imputation\n# Impute missing values and then scale the numerical columns\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),  # Replace missing values with mean\n    ('std_scaler', StandardScaler())\n])\n# Apply the pipeline to the numerical columns\nnumerical_cols = sample_data_org.select_dtypes(include=['int64', 'float64']).columns\nsample_data_org[numerical_cols] = num_pipeline.fit_transform(sample_data_org[numerical_cols])\n# One-hot encode the categorical columns\ncategorical_cols = sample_data_org.select_dtypes(include=['object']).columns\nsample_data_org = pd.get_dummies(sample_data_org, columns=categorical_cols, drop_first=True)\n\n# Spliting the Dataset \nX_train, X_test = train_test_split(sample_data_org, test_size=0.3, random_state=42)\n\n\n\n\nThe training process involves fitting the Isolation Forest model to our training data.\n\niso_forest.fit(X_train)\n\nIsolationForest(contamination=0.1, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.IsolationForestIsolationForest(contamination=0.1, random_state=42)\n\n\n\n\n\n\n\n\nAfter training, we assess the model’s performance on the test set. This evaluation helps us understand the effectiveness of our anomaly detection in the context of air pollution data.\nIn an unsupervised dataset scenario, where we don’t have labeled data (y_test), the evaluation of an anomaly detection model like Isolation Forest is more about understanding and interpreting the anomalies it detects rather than calculating quantitative metrics. The goal is to examine the anomalies flagged by the model and determine if they align with our domain knowledge or expectations.\n\n\nFirst, use the model to predict anomalies in your test set. The Isolation Forest model marks an anomaly with -1 and normal with 1.\n\n# Predict anomalies on the test set\nanomalies = iso_forest.predict(X_test)\n\n# Convert predictions: -1 (anomalies) to 1 and 1 (normal) to 0\nanomalies = np.where(anomalies == -1, 1, 0)\n\n\n\n\nThe next step is to analyze these detected anomalies.\n\n# Count the number of anomalies detected\nnum_anomalies = np.sum(anomalies)\ntotal_points = len(anomalies)\nprint(f\"Total data points: {total_points}\")\nprint(f\"Number of anomalies detected: {num_anomalies}\")\nprint(f\"Proportion of anomalies detected: {num_anomalies / total_points:.2%}\")\n\nTotal data points: 126231\nNumber of anomalies detected: 12510\nProportion of anomalies detected: 9.91%\n\n\n\n\n\nIt can be insightful to examine the data points that the model flagged as anomalies. This involves looking at the specific characteristics of these data points.\n\n# Create a DataFrame of the test set with a column for anomaly labels\ntest_set_with_predictions = X_test.copy()\ntest_set_with_predictions['Anomaly'] = anomalies\n\n# Display some of the anomalies\nanomalous_data = test_set_with_predictions[test_set_with_predictions['Anomaly'] == 1]\nprint(\"Sample of detected anomalies:\")\nprint(anomalous_data.sample(min(10, len(anomalous_data))))  # Display up to 10 anomalous points\n\nSample of detected anomalies:\n              No      year     month       day      hour     PM2.5      PM10  \\\n227923  0.000741  0.286647 -1.021523 -1.673805  1.083473 -0.597578 -0.325067   \n7715   -0.969808 -0.562829 -1.601451  0.030723 -0.072232  3.903627  3.166894   \n262102 -0.086691  0.286647 -1.601451  0.826169  1.516862  2.590776  2.002907   \n104857  1.699004  1.985599 -1.311487 -0.082912 -1.516862  1.928098  1.596610   \n2914   -1.444117 -1.412304 -0.151631  1.621615 -0.216695  2.478246  1.827211   \n68714   1.592406  1.985599 -1.601451 -1.673805 -1.372399  1.828072  1.926040   \n243867  1.575907  1.136123  1.588154  1.053439 -1.227936  1.015354  0.740091   \n219210 -0.860048 -0.562829 -1.021523 -1.446535  0.939010  2.190669  2.266451   \n213542 -1.420011 -1.412304  0.138333 -0.651088  0.361158 -0.635088 -0.797250   \n34431   1.669564  1.985599 -1.311487 -1.560170  0.505621 -0.109947 -0.270162   \n\n             SO2       NO2        CO  ...  station_Dongsi  station_Guanyuan  \\\n227923 -0.178867  0.472622 -0.469173  ...           False             False   \n7715    6.591382  3.332371  4.038987  ...           False             False   \n262102  2.482541  2.783530  3.420220  ...            True             False   \n104857 -0.365632  2.032485  2.005896  ...           False             False   \n2914   -0.505707  0.877031  2.094291  ...           False             False   \n68714  -0.552398  0.530394  1.652314  ...           False             False   \n243867  0.474812  1.743621  2.005896  ...           False             False   \n219210  4.817110  3.476802  3.243430  ...           False             False   \n213542 -0.592478 -0.454601 -0.734358  ...           False             False   \n34431   0.428121  0.270417  0.061199  ...           False             False   \n\n        station_Gucheng  station_Huairou  station_Nongzhanguan  \\\n227923            False            False                 False   \n7715               True            False                 False   \n262102            False            False                 False   \n104857            False            False                 False   \n2914               True            False                 False   \n68714             False             True                 False   \n243867            False            False                 False   \n219210            False            False                 False   \n213542            False            False                 False   \n34431              True            False                 False   \n\n        station_Shunyi  station_Tiantan  station_Wanliu  \\\n227923           False            False            True   \n7715             False            False           False   \n262102           False            False           False   \n104857           False             True           False   \n2914             False            False           False   \n68714            False            False           False   \n243867           False            False            True   \n219210           False            False            True   \n213542           False            False            True   \n34431            False            False           False   \n\n        station_Wanshouxigong  Anomaly  \n227923                  False        1  \n7715                    False        1  \n262102                  False        1  \n104857                  False        1  \n2914                    False        1  \n68714                   False        1  \n243867                  False        1  \n219210                  False        1  \n213542                  False        1  \n34431                   False        1  \n\n[10 rows x 43 columns]\n\n\n\n\n\n\n\n\n\nTo showcase the detected anomalies, we employ various types of visualizations. Here, we’ll focus on two primary types: scatter plots and heatmaps. These visualizations will help us to interpret the anomalies in the context of air pollution data.\n\n\nScatter plots are excellent for visualizing the relationship between two variables and identifying points that stand out from the pattern.\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=anomalous_data, x='TEMP', y='PM2.5', hue='Anomaly')\nplt.title('Scatter Plot of PM2.5 vs Temperature')\nplt.xlabel('Temperature')\nplt.ylabel('PM2.5')\nplt.show()\n\n\n\n\nThe scatter plot visualizes PM2.5 levels against temperature, with each point representing an observation. The data points are standardized, as indicated by the temperature axis ranging from approximately -2 to 2. A dense clustering of points suggests that lower PM2.5 levels are common across the temperature range, with a noticeable spread in higher PM2.5 levels at mid-range temperatures. Points that stand out from the dense cloud indicate potential anomalies with higher PM2.5 levels, which could be of interest for further investigation into air quality issues at different temperatures.\n\n\n\nHeatmaps are useful for understanding the distribution and concentration of data points across two dimensions.\n\n# Create a heatmap to show the concentration of anomalies\n# Sample a subset of the anomalous data for quicker visualization\nsampled_anomalous_data = anomalous_data.sample(min(500, len(anomalous_data)), random_state=42)\n\n# Create a heatmap without annotations for quicker rendering\nplt.figure(figsize=(10, 8))\nsns.heatmap(data=sampled_anomalous_data[['PM2.5', 'PM10', 'SO2', 'NO2', 'CO']])\nplt.title('Heatmap of Pollutant Levels in Anomalous Data (Sampled)')\nplt.show()\n\n\n\n\nThe heatmap depicts pollutant levels in a subset of data identified as anomalous, with rows representing individual instances and columns for various pollutants. The color scale, ranging from dark purple to bright orange, illustrates the intensity of pollutant levels, with orange indicating higher concentrations. The stark contrast across the rows, primarily in the deep purple range, suggests that most selected data points do not have extremely high pollutant levels. However, occasional streaks of orange and red reveal instances where one or more pollutants reach notably higher levels, warranting closer scrutiny for potential environmental health concerns.\n\n\n\n\n\nIn the realm of anomaly detection, particularly with methods like the Isolation Forest, the concept of threshold tuning is pivotal. The threshold determines the cutoff point at which a data point is classified as an anomaly. Tuning this threshold is a delicate balance, as it directly impacts the sensitivity of our anomaly detection.\n\n\nThreshold tuning involves adjusting the parameters that define what we consider to be anomalous. In the case of the Isolation Forest, this often revolves around the contamination parameter, which represents the proportion of outliers we expect in the data.\n\n# Adjusting the contamination parameter\ncontamination_rate = 0.05  # Example rate\niso_forest = IsolationForest(contamination=contamination_rate)\niso_forest.fit(X_train)\n\nIsolationForest(contamination=0.05)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.IsolationForestIsolationForest(contamination=0.05)\n\n\nIn this code snippet, we adjust the contamination parameter, which dictates the model’s sensitivity to anomalies. A higher contamination rate means the model will be more inclined to flag data points as anomalies.\n\n\n\nThe setting of the threshold has a direct impact on the trade-off between false positives (normal points incorrectly identified as anomalies) and false negatives (actual anomalies not detected).\n\nHigher Threshold (Lower Contamination): This setting reduces the number of anomalies detected, potentially leading to more false negatives. While it ensures that the flagged anomalies are very likely to be true anomalies, it may miss some subtler, yet significant, anomalies.\nLower Threshold (Higher Contamination): Conversely, a lower threshold increases the sensitivity, potentially leading to more false positives. This setting might be useful in scenarios where missing an anomaly could have severe consequences, even if it means dealing with more false alarms.\n\n\n\n\nFinding the right balance for the threshold is crucial:\n\ndef evaluate_model(model, X_test):\n    # Predict anomalies\n    predictions = model.predict(X_test)\n\n    # Convert predictions to a more readable format: -1 (anomalies) to 1, 1 (normal) to 0\n    predictions = np.where(predictions == -1, 1, 0)\n\n    # Count and print the number of anomalies detected\n    num_anomalies = np.sum(predictions)\n    print(f\"Number of anomalies detected: {num_anomalies} out of {len(X_test)} data points\")\n\n# Experimenting with different contamination rates\nfor rate in [0.01, 0.05, 0.1]:\n    iso_forest = IsolationForest(contamination=rate)\n    iso_forest.fit(X_train)\n    # Evaluate the model\n    evaluate_model(iso_forest, X_test)\n    print(\"\\n\")\n\nNumber of anomalies detected: 1284 out of 126231 data points\n\n\nNumber of anomalies detected: 6391 out of 126231 data points\n\n\nNumber of anomalies detected: 12796 out of 126231 data points\n\n\n\n\nIn our exploration of the optimal contamination rate for the Isolation Forest model, we experimented with various rates and observed their impact on anomaly detection in our dataset of 126,231 data points. When we set the contamination rate at 0.01, our model identified 1,293 anomalies, suggesting a more conservative approach to anomaly detection. Increasing the rate to 0.05 led to a significant rise in detected anomalies, totaling 6,356, indicating a moderate level of sensitivity. Further amplifying the rate to 0.1 resulted in the detection of 12,702 anomalies, reflecting a highly sensitive setting that captures a broader spectrum of potential anomalies. These varying results illustrate the crucial influence of the contamination rate on the model’s behavior, underscoring the importance of fine-tuning this parameter to strike a balance between identifying true anomalies and avoiding excessive false positives. Our analysis highlights the need for a thoughtful approach to setting this threshold, considering both the nature of our data and the specific requirements of our air quality monitoring objectives.\n\n\n\n\nBased on our analysis of the air pollution data using the Isolation Forest model, we’ve uncovered some intriguing insights. Out of the total 126,231 data points, our model identified 12,510 as anomalies, accounting for approximately 9.91% of the dataset. This proportion of anomalies is significant and warrants further investigation.\n\n\nWhen we delve into the sample of detected anomalies, several key observations emerge:\n\nElevated Pollutant Levels: Many of the anomalies exhibit unusually high levels of pollutants such as PM2.5, PM10, NO2, and CO. For instance, rows like 8468 and 314995 show pollutant concentrations several times higher than typical readings. This could indicate episodes of extreme pollution, possibly due to specific environmental events or human activities.\nMeteorological Influences: The anomalies also reveal interesting patterns in meteorological conditions. For example, rows 214246 and 244058 show variations in temperature, pressure, and humidity, which could be influencing factors for the high pollution levels observed.\nStation-Specific Anomalies: The data points flagged as anomalies are distributed across different monitoring stations, as seen in the ‘station’ columns. This distribution suggests that the detected anomalies are not confined to a specific location but are rather widespread, indicating a more systemic issue in air quality.\nTemporal Patterns: The presence of anomalies across different years, months, and hours, such as in rows 216296 and 392113, hints at temporal patterns in air pollution. These patterns could be aligned with seasonal changes, urban activities, or policy changes affecting air quality.\n\n\n\n\n\nEnvironmental Policy and Health: The identified anomalies are crucial for understanding the dynamics of air pollution. They can inform environmental policies, especially in devising strategies to mitigate high pollution episodes.\nFurther Research: These findings can be a starting point for more detailed research. For example, investigating the causes behind high pollution episodes can help in understanding the impact of urban development, traffic patterns, or industrial activities on air quality.\nPublic Awareness: Disseminating information about such high pollution episodes can raise public awareness and encourage preventive measures, especially for vulnerable populations.\n\nIn summary, our analysis using the Isolation Forest model provides us with valuable insights into the air quality data, highlighting instances of unusually high pollution levels. This information is crucial for environmental monitoring, policy-making, and public health initiatives.\n\n\n\n\nhttps://www.knowledgehut.com/blog/data-science/machine-learning-for-anomaly-detection https://www.techtarget.com/searchenterpriseai/definition/anomaly-detection https://medium.com/@corymaklin/isolation-forest-799fceacdda4 OpenAI. (2023). ChatGPT [Large language model]. https://chat.openai.com"
  },
  {
    "objectID": "posts/outlier4/index.html#introduction",
    "href": "posts/outlier4/index.html#introduction",
    "title": "5. Anomaly/Outlier Detection",
    "section": "",
    "text": "Welcome to our exploration of “Detecting Anomalies in Air Pollution Data,” a vital project in the realm of environmental monitoring. With increasing concerns about air quality and its impact on public health and the environment, identifying irregularities in air pollution data has never been more critical.\nThis project leverages a comprehensive dataset from the Beijing Multi-site Air Quality Data, which offers a rich tapestry of air pollutant measurements and meteorological data across various sites in Beijing. The data spans from 2013 to 2017, providing insights into pollutants like PM2.5, PM10, SO2, NO2, and CO, as well as meteorological conditions like temperature, humidity, and wind speed.\nOur primary goal is to detect unusual patterns or outliers in air quality data that might signify environmental hazards, technical errors in data collection, or significant meteorological impacts. By accomplishing this, we aim to contribute to more effective environmental monitoring and policy-making."
  },
  {
    "objectID": "posts/outlier4/index.html#data-exploration-and-preprocessing",
    "href": "posts/outlier4/index.html#data-exploration-and-preprocessing",
    "title": "5. Anomaly/Outlier Detection",
    "section": "",
    "text": "The first step in our data science journey involves getting acquainted with the dataset’s structure and characteristics. This involves examining the various columns of the dataset, which include both pollutant levels and meteorological factors.\n\n# Install and import all libraries\nimport sys\n!{sys.executable} -m pip install seaborn\n!{sys.executable} -m pip install matplotlib\n!{sys.executable} -m pip install scikit-learn\n!{sys.executable} -m pip install pandas\n!{sys.executable} -m pip install numpy\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import IsolationForest\n\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: seaborn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.13.0)\nRequirement already satisfied: matplotlib!=3.6.1,&gt;=3.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (3.8.2)\nRequirement already satisfied: numpy!=1.24.0,&gt;=1.20 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (1.26.2)\nRequirement already satisfied: pandas&gt;=1.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (2.1.3)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (0.12.1)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (23.2)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (6.1.1)\nRequirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (10.1.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (4.45.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.4.5)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (2.8.2)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (3.1.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.2.0)\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (3.17.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3.post1)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: matplotlib in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (3.8.2)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.1.1)\nRequirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.1.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.45.1)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (23.2)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: numpy&lt;2,&gt;=1.21 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib) (3.17.0)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: scikit-learn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.3.2)\nRequirement already satisfied: numpy&lt;2.0,&gt;=1.17.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.2)\nRequirement already satisfied: scipy&gt;=1.5.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: joblib&gt;=1.1.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.3.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pandas in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (2.1.3)\nRequirement already satisfied: numpy&lt;2,&gt;=1.22.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: numpy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.26.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\n\n\n\nsample_data_org = pd.read_csv('air_data_all.csv')\nsample_data_org.head()\n\n\n\n\n\n\n\n\nNo\nyear\nmonth\nday\nhour\nPM2.5\nPM10\nSO2\nNO2\nCO\nO3\nTEMP\nPRES\nDEWP\nRAIN\nwd\nWSPM\nstation\n\n\n\n\n0\n1\n2013\n3\n1\n0\n6.0\n18.0\n5.0\nNaN\n800.0\n88.0\n0.1\n1021.1\n-18.6\n0.0\nNW\n4.4\nGucheng\n\n\n1\n2\n2013\n3\n1\n1\n6.0\n15.0\n5.0\nNaN\n800.0\n88.0\n-0.3\n1021.5\n-19.0\n0.0\nNW\n4.0\nGucheng\n\n\n2\n3\n2013\n3\n1\n2\n5.0\n18.0\nNaN\nNaN\n700.0\n52.0\n-0.7\n1021.5\n-19.8\n0.0\nWNW\n4.6\nGucheng\n\n\n3\n4\n2013\n3\n1\n3\n6.0\n20.0\n6.0\nNaN\nNaN\nNaN\n-1.0\n1022.7\n-21.2\n0.0\nW\n2.8\nGucheng\n\n\n4\n5\n2013\n3\n1\n4\n5.0\n17.0\n5.0\nNaN\n600.0\n73.0\n-1.3\n1023.0\n-21.4\n0.0\nWNW\n3.6\nGucheng\n\n\n\n\n\n\n\nBy running this code, we get a glimpse of the first few rows of our dataset, allowing us to understand the types of data we will be working with.\n\n\n\nDealing with missing data and categorical variables is a crucial part of data preprocessing. To address this, we first identify the missing values and then decide on an appropriate strategy, such as imputation or removal. However, in later processing, we realized we hadn’t done enough here.\n\nmissing_values = sample_data_org.isnull().sum()\nmissing_values\n\nNo             0\nyear           0\nmonth          0\nday            0\nhour           0\nPM2.5       8739\nPM10        6449\nSO2         9021\nNO2        12116\nCO         20701\nO3         13277\nTEMP         398\nPRES         393\nDEWP         403\nRAIN         390\nwd          1822\nWSPM         318\nstation        0\ndtype: int64\n\n\n\n# Remove rows with missing values\nsample_data = sample_data_org.dropna()\n\n# Identify numerical columns\nnumerical_cols = sample_data.select_dtypes(include=['int64', 'float64']).columns\n\n# Create a pipeline for imputing missing values and scaling\npipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),  # Replace missing values with mean\n    ('scaler', StandardScaler()),                # Scale the data\n])\n\n# Apply the pipeline to the numerical columns\nscaled_data = pipeline.fit_transform(sample_data[numerical_cols])\n\n# Apply PCA\npca = PCA(n_components=0.95)  # Retain 95% of the variance\nprincipal_components = pca.fit_transform(scaled_data)\n\n# Identify non-numeric (categorical) columns\ncategorical_cols = sample_data.select_dtypes(include=['object']).columns\n\n# One-hot encode the categorical data\nencoder = OneHotEncoder(sparse=False)\ncategorical_encoded = encoder.fit_transform(sample_data[categorical_cols])\n\n# Check for 'get_feature_names_out' method for naming columns\nif hasattr(encoder, 'get_feature_names_out'):\n    encoded_columns = pd.DataFrame(categorical_encoded, columns=encoder.get_feature_names_out(categorical_cols))\nelse:\n    # Fallback: manually create feature names\n    encoded_columns = pd.DataFrame(categorical_encoded)\n    encoded_columns.columns = [col + '_' + str(i) for col in categorical_cols for i in range(encoded_columns.shape[1])]\n\n# Concatenate the encoded columns with the original dataset and drop the original categorical columns\nsample_data_encoded = pd.concat([sample_data.drop(categorical_cols, axis=1), encoded_columns], axis=1)\n\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n\n\nFor categorical variables like wind direction, we use encoding techniques to convert them into numerical form, making them suitable for analysis.\n\n\n\nGiven the varying scales of our numerical features, normalization or standardization becomes necessary. This step ensures that no single feature disproportionately influences the model due to its scale.\n\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(sample_data[['PM2.5', 'PM10', 'TEMP', 'PRES']])\n\n\n\n\nFinally, we perform feature selection and engineering. This process involves attempting to choose the most relevant features and possibly creating new features to improve our model’s performance.\n\nCorrelation Analysis: First, we can perform a correlation analysis to understand the relationships between different features. This can help in identifying features that are strongly correlated with each other, from which we can select the most relevant ones.\n\n\n# Now perform the correlation analysis on the numerical data\ncorr = sample_data_encoded.corr()\n\n# Generate a heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, fmt=\".2f\")\nplt.show()\n\n\n\n\nThe heatmap visualizes the correlation coefficients between different variables related to air quality and weather conditions. The scale on the right indicates the strength of the correlation, ranging from -1 (a perfect negative correlation) to 1 (a perfect positive correlation). Dark red shades indicate strong positive correlations, whereas dark blue shades represent strong negative correlations. Most variables do not exhibit a strong correlation with each other, as indicated by the shades of white (near-zero correlation). This kind of visualization helps to identify relationships between variables, which can be further investigated for causal connections or dependencies.\n\nPrincipal Component Analysis (PCA): PCA is a technique used to reduce the dimensionality of the data, enhancing the interpretability while minimizing information loss.\n\n\npca = PCA(n_components=0.95) # Retain 95% of the variance\nprincipal_components = pca.fit_transform(scaled_data)\n\nThis code applies PCA to the scaled data, reducing the number of features while retaining 95% of the variance in the data.\n\nFeature Engineering: Here, we are attempting to create a new feature that might be more indicative of anomalies by creating a composite air quality index from multiple pollutants.\n\n\nsample_data['Air_Quality_Index'] = sample_data['PM2.5'] * 0.4 + sample_data['PM10'] * 0.2 + sample_data['NO2'] * 0.2 + sample_data['SO2'] * 0.1 + sample_data['CO'] * 0.1\n\n/var/folders/pt/983cdyd950gd2j6l1gv1376r0000gn/T/ipykernel_18793/3115101989.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  sample_data['Air_Quality_Index'] = sample_data['PM2.5'] * 0.4 + sample_data['PM10'] * 0.2 + sample_data['NO2'] * 0.2 + sample_data['SO2'] * 0.1 + sample_data['CO'] * 0.1\n\n\nThis code creates a new feature, ‘Air_Quality_Index’, as a weighted sum of various pollutants, hypothesizing that this composite index might be a more effective predictor of anomalies.\nThrough these steps, we attempted to refine our dataset to include the most relevant features for anomaly detection, enhancing the model’s accuracy and efficiency."
  },
  {
    "objectID": "posts/outlier4/index.html#anomaly-detection-algorithms-and-model-training-and-evaluation",
    "href": "posts/outlier4/index.html#anomaly-detection-algorithms-and-model-training-and-evaluation",
    "title": "5. Anomaly/Outlier Detection",
    "section": "",
    "text": "After trying out other models, which took more than 5 minutes to run, for our project on air pollution data, we have opted for the Isolation Forest algorithm due to its efficiency and effectiveness, especially in dealing with large and high-dimensional datasets like ours.\n\niso_forest = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)\n\n\n\n\nWe split our dataset into training and test sets, ensuring that the model is evaluated on unseen data, reflecting its performance in real-world scenarios.\n\n# Handling NaN Values with Imputation\n# Impute missing values and then scale the numerical columns\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),  # Replace missing values with mean\n    ('std_scaler', StandardScaler())\n])\n# Apply the pipeline to the numerical columns\nnumerical_cols = sample_data_org.select_dtypes(include=['int64', 'float64']).columns\nsample_data_org[numerical_cols] = num_pipeline.fit_transform(sample_data_org[numerical_cols])\n# One-hot encode the categorical columns\ncategorical_cols = sample_data_org.select_dtypes(include=['object']).columns\nsample_data_org = pd.get_dummies(sample_data_org, columns=categorical_cols, drop_first=True)\n\n# Spliting the Dataset \nX_train, X_test = train_test_split(sample_data_org, test_size=0.3, random_state=42)\n\n\n\n\nThe training process involves fitting the Isolation Forest model to our training data.\n\niso_forest.fit(X_train)\n\nIsolationForest(contamination=0.1, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.IsolationForestIsolationForest(contamination=0.1, random_state=42)\n\n\n\n\n\n\n\n\nAfter training, we assess the model’s performance on the test set. This evaluation helps us understand the effectiveness of our anomaly detection in the context of air pollution data.\nIn an unsupervised dataset scenario, where we don’t have labeled data (y_test), the evaluation of an anomaly detection model like Isolation Forest is more about understanding and interpreting the anomalies it detects rather than calculating quantitative metrics. The goal is to examine the anomalies flagged by the model and determine if they align with our domain knowledge or expectations.\n\n\nFirst, use the model to predict anomalies in your test set. The Isolation Forest model marks an anomaly with -1 and normal with 1.\n\n# Predict anomalies on the test set\nanomalies = iso_forest.predict(X_test)\n\n# Convert predictions: -1 (anomalies) to 1 and 1 (normal) to 0\nanomalies = np.where(anomalies == -1, 1, 0)\n\n\n\n\nThe next step is to analyze these detected anomalies.\n\n# Count the number of anomalies detected\nnum_anomalies = np.sum(anomalies)\ntotal_points = len(anomalies)\nprint(f\"Total data points: {total_points}\")\nprint(f\"Number of anomalies detected: {num_anomalies}\")\nprint(f\"Proportion of anomalies detected: {num_anomalies / total_points:.2%}\")\n\nTotal data points: 126231\nNumber of anomalies detected: 12510\nProportion of anomalies detected: 9.91%\n\n\n\n\n\nIt can be insightful to examine the data points that the model flagged as anomalies. This involves looking at the specific characteristics of these data points.\n\n# Create a DataFrame of the test set with a column for anomaly labels\ntest_set_with_predictions = X_test.copy()\ntest_set_with_predictions['Anomaly'] = anomalies\n\n# Display some of the anomalies\nanomalous_data = test_set_with_predictions[test_set_with_predictions['Anomaly'] == 1]\nprint(\"Sample of detected anomalies:\")\nprint(anomalous_data.sample(min(10, len(anomalous_data))))  # Display up to 10 anomalous points\n\nSample of detected anomalies:\n              No      year     month       day      hour     PM2.5      PM10  \\\n227923  0.000741  0.286647 -1.021523 -1.673805  1.083473 -0.597578 -0.325067   \n7715   -0.969808 -0.562829 -1.601451  0.030723 -0.072232  3.903627  3.166894   \n262102 -0.086691  0.286647 -1.601451  0.826169  1.516862  2.590776  2.002907   \n104857  1.699004  1.985599 -1.311487 -0.082912 -1.516862  1.928098  1.596610   \n2914   -1.444117 -1.412304 -0.151631  1.621615 -0.216695  2.478246  1.827211   \n68714   1.592406  1.985599 -1.601451 -1.673805 -1.372399  1.828072  1.926040   \n243867  1.575907  1.136123  1.588154  1.053439 -1.227936  1.015354  0.740091   \n219210 -0.860048 -0.562829 -1.021523 -1.446535  0.939010  2.190669  2.266451   \n213542 -1.420011 -1.412304  0.138333 -0.651088  0.361158 -0.635088 -0.797250   \n34431   1.669564  1.985599 -1.311487 -1.560170  0.505621 -0.109947 -0.270162   \n\n             SO2       NO2        CO  ...  station_Dongsi  station_Guanyuan  \\\n227923 -0.178867  0.472622 -0.469173  ...           False             False   \n7715    6.591382  3.332371  4.038987  ...           False             False   \n262102  2.482541  2.783530  3.420220  ...            True             False   \n104857 -0.365632  2.032485  2.005896  ...           False             False   \n2914   -0.505707  0.877031  2.094291  ...           False             False   \n68714  -0.552398  0.530394  1.652314  ...           False             False   \n243867  0.474812  1.743621  2.005896  ...           False             False   \n219210  4.817110  3.476802  3.243430  ...           False             False   \n213542 -0.592478 -0.454601 -0.734358  ...           False             False   \n34431   0.428121  0.270417  0.061199  ...           False             False   \n\n        station_Gucheng  station_Huairou  station_Nongzhanguan  \\\n227923            False            False                 False   \n7715               True            False                 False   \n262102            False            False                 False   \n104857            False            False                 False   \n2914               True            False                 False   \n68714             False             True                 False   \n243867            False            False                 False   \n219210            False            False                 False   \n213542            False            False                 False   \n34431              True            False                 False   \n\n        station_Shunyi  station_Tiantan  station_Wanliu  \\\n227923           False            False            True   \n7715             False            False           False   \n262102           False            False           False   \n104857           False             True           False   \n2914             False            False           False   \n68714            False            False           False   \n243867           False            False            True   \n219210           False            False            True   \n213542           False            False            True   \n34431            False            False           False   \n\n        station_Wanshouxigong  Anomaly  \n227923                  False        1  \n7715                    False        1  \n262102                  False        1  \n104857                  False        1  \n2914                    False        1  \n68714                   False        1  \n243867                  False        1  \n219210                  False        1  \n213542                  False        1  \n34431                   False        1  \n\n[10 rows x 43 columns]"
  },
  {
    "objectID": "posts/outlier4/index.html#visualization-of-anomalies",
    "href": "posts/outlier4/index.html#visualization-of-anomalies",
    "title": "5. Anomaly/Outlier Detection",
    "section": "",
    "text": "To showcase the detected anomalies, we employ various types of visualizations. Here, we’ll focus on two primary types: scatter plots and heatmaps. These visualizations will help us to interpret the anomalies in the context of air pollution data.\n\n\nScatter plots are excellent for visualizing the relationship between two variables and identifying points that stand out from the pattern.\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=anomalous_data, x='TEMP', y='PM2.5', hue='Anomaly')\nplt.title('Scatter Plot of PM2.5 vs Temperature')\nplt.xlabel('Temperature')\nplt.ylabel('PM2.5')\nplt.show()\n\n\n\n\nThe scatter plot visualizes PM2.5 levels against temperature, with each point representing an observation. The data points are standardized, as indicated by the temperature axis ranging from approximately -2 to 2. A dense clustering of points suggests that lower PM2.5 levels are common across the temperature range, with a noticeable spread in higher PM2.5 levels at mid-range temperatures. Points that stand out from the dense cloud indicate potential anomalies with higher PM2.5 levels, which could be of interest for further investigation into air quality issues at different temperatures.\n\n\n\nHeatmaps are useful for understanding the distribution and concentration of data points across two dimensions.\n\n# Create a heatmap to show the concentration of anomalies\n# Sample a subset of the anomalous data for quicker visualization\nsampled_anomalous_data = anomalous_data.sample(min(500, len(anomalous_data)), random_state=42)\n\n# Create a heatmap without annotations for quicker rendering\nplt.figure(figsize=(10, 8))\nsns.heatmap(data=sampled_anomalous_data[['PM2.5', 'PM10', 'SO2', 'NO2', 'CO']])\nplt.title('Heatmap of Pollutant Levels in Anomalous Data (Sampled)')\nplt.show()\n\n\n\n\nThe heatmap depicts pollutant levels in a subset of data identified as anomalous, with rows representing individual instances and columns for various pollutants. The color scale, ranging from dark purple to bright orange, illustrates the intensity of pollutant levels, with orange indicating higher concentrations. The stark contrast across the rows, primarily in the deep purple range, suggests that most selected data points do not have extremely high pollutant levels. However, occasional streaks of orange and red reveal instances where one or more pollutants reach notably higher levels, warranting closer scrutiny for potential environmental health concerns."
  },
  {
    "objectID": "posts/outlier4/index.html#threshold-tuning",
    "href": "posts/outlier4/index.html#threshold-tuning",
    "title": "5. Anomaly/Outlier Detection",
    "section": "",
    "text": "In the realm of anomaly detection, particularly with methods like the Isolation Forest, the concept of threshold tuning is pivotal. The threshold determines the cutoff point at which a data point is classified as an anomaly. Tuning this threshold is a delicate balance, as it directly impacts the sensitivity of our anomaly detection.\n\n\nThreshold tuning involves adjusting the parameters that define what we consider to be anomalous. In the case of the Isolation Forest, this often revolves around the contamination parameter, which represents the proportion of outliers we expect in the data.\n\n# Adjusting the contamination parameter\ncontamination_rate = 0.05  # Example rate\niso_forest = IsolationForest(contamination=contamination_rate)\niso_forest.fit(X_train)\n\nIsolationForest(contamination=0.05)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.IsolationForestIsolationForest(contamination=0.05)\n\n\nIn this code snippet, we adjust the contamination parameter, which dictates the model’s sensitivity to anomalies. A higher contamination rate means the model will be more inclined to flag data points as anomalies.\n\n\n\nThe setting of the threshold has a direct impact on the trade-off between false positives (normal points incorrectly identified as anomalies) and false negatives (actual anomalies not detected).\n\nHigher Threshold (Lower Contamination): This setting reduces the number of anomalies detected, potentially leading to more false negatives. While it ensures that the flagged anomalies are very likely to be true anomalies, it may miss some subtler, yet significant, anomalies.\nLower Threshold (Higher Contamination): Conversely, a lower threshold increases the sensitivity, potentially leading to more false positives. This setting might be useful in scenarios where missing an anomaly could have severe consequences, even if it means dealing with more false alarms.\n\n\n\n\nFinding the right balance for the threshold is crucial:\n\ndef evaluate_model(model, X_test):\n    # Predict anomalies\n    predictions = model.predict(X_test)\n\n    # Convert predictions to a more readable format: -1 (anomalies) to 1, 1 (normal) to 0\n    predictions = np.where(predictions == -1, 1, 0)\n\n    # Count and print the number of anomalies detected\n    num_anomalies = np.sum(predictions)\n    print(f\"Number of anomalies detected: {num_anomalies} out of {len(X_test)} data points\")\n\n# Experimenting with different contamination rates\nfor rate in [0.01, 0.05, 0.1]:\n    iso_forest = IsolationForest(contamination=rate)\n    iso_forest.fit(X_train)\n    # Evaluate the model\n    evaluate_model(iso_forest, X_test)\n    print(\"\\n\")\n\nNumber of anomalies detected: 1284 out of 126231 data points\n\n\nNumber of anomalies detected: 6391 out of 126231 data points\n\n\nNumber of anomalies detected: 12796 out of 126231 data points\n\n\n\n\nIn our exploration of the optimal contamination rate for the Isolation Forest model, we experimented with various rates and observed their impact on anomaly detection in our dataset of 126,231 data points. When we set the contamination rate at 0.01, our model identified 1,293 anomalies, suggesting a more conservative approach to anomaly detection. Increasing the rate to 0.05 led to a significant rise in detected anomalies, totaling 6,356, indicating a moderate level of sensitivity. Further amplifying the rate to 0.1 resulted in the detection of 12,702 anomalies, reflecting a highly sensitive setting that captures a broader spectrum of potential anomalies. These varying results illustrate the crucial influence of the contamination rate on the model’s behavior, underscoring the importance of fine-tuning this parameter to strike a balance between identifying true anomalies and avoiding excessive false positives. Our analysis highlights the need for a thoughtful approach to setting this threshold, considering both the nature of our data and the specific requirements of our air quality monitoring objectives."
  },
  {
    "objectID": "posts/outlier4/index.html#interpretation-and-real-world-implications",
    "href": "posts/outlier4/index.html#interpretation-and-real-world-implications",
    "title": "5. Anomaly/Outlier Detection",
    "section": "",
    "text": "Based on our analysis of the air pollution data using the Isolation Forest model, we’ve uncovered some intriguing insights. Out of the total 126,231 data points, our model identified 12,510 as anomalies, accounting for approximately 9.91% of the dataset. This proportion of anomalies is significant and warrants further investigation.\n\n\nWhen we delve into the sample of detected anomalies, several key observations emerge:\n\nElevated Pollutant Levels: Many of the anomalies exhibit unusually high levels of pollutants such as PM2.5, PM10, NO2, and CO. For instance, rows like 8468 and 314995 show pollutant concentrations several times higher than typical readings. This could indicate episodes of extreme pollution, possibly due to specific environmental events or human activities.\nMeteorological Influences: The anomalies also reveal interesting patterns in meteorological conditions. For example, rows 214246 and 244058 show variations in temperature, pressure, and humidity, which could be influencing factors for the high pollution levels observed.\nStation-Specific Anomalies: The data points flagged as anomalies are distributed across different monitoring stations, as seen in the ‘station’ columns. This distribution suggests that the detected anomalies are not confined to a specific location but are rather widespread, indicating a more systemic issue in air quality.\nTemporal Patterns: The presence of anomalies across different years, months, and hours, such as in rows 216296 and 392113, hints at temporal patterns in air pollution. These patterns could be aligned with seasonal changes, urban activities, or policy changes affecting air quality.\n\n\n\n\n\nEnvironmental Policy and Health: The identified anomalies are crucial for understanding the dynamics of air pollution. They can inform environmental policies, especially in devising strategies to mitigate high pollution episodes.\nFurther Research: These findings can be a starting point for more detailed research. For example, investigating the causes behind high pollution episodes can help in understanding the impact of urban development, traffic patterns, or industrial activities on air quality.\nPublic Awareness: Disseminating information about such high pollution episodes can raise public awareness and encourage preventive measures, especially for vulnerable populations.\n\nIn summary, our analysis using the Isolation Forest model provides us with valuable insights into the air quality data, highlighting instances of unusually high pollution levels. This information is crucial for environmental monitoring, policy-making, and public health initiatives."
  },
  {
    "objectID": "posts/outlier4/index.html#reference",
    "href": "posts/outlier4/index.html#reference",
    "title": "5. Anomaly/Outlier Detection",
    "section": "",
    "text": "https://www.knowledgehut.com/blog/data-science/machine-learning-for-anomaly-detection https://www.techtarget.com/searchenterpriseai/definition/anomaly-detection https://medium.com/@corymaklin/isolation-forest-799fceacdda4 OpenAI. (2023). ChatGPT [Large language model]. https://chat.openai.com"
  },
  {
    "objectID": "posts/classification2/index.html",
    "href": "posts/classification2/index.html",
    "title": "4. Classification",
    "section": "",
    "text": "Welcome to our exploration into the world of machine learning and its application in predicting driving behaviors. In this project, we dive into the realm of vehicular safety, aiming to leverage sensor data to classify driving patterns into three categories: SLOW, NORMAL, and AGGRESSIVE. This endeavor is not just a technical challenge but a crucial step towards enhancing road safety and reducing traffic accidents.\n\n\nThe primary goal of this project is to accurately predict driving behaviors using data from commonly available sensors in smartphones. By analyzing accelerometer and gyroscope data, we aim to classify driving styles into SLOW, NORMAL, or AGGRESSIVE, contributing significantly to the prevention of road mishaps.\n\n\n\nThe importance of this project is underscored by the alarming statistics from the AAA Foundation for Traffic Safety, highlighting that over half of fatal crashes involve aggressive driving actions. Through this project, we offer a scalable and readily deployable solution to monitor and predict dangerous driving behaviors, potentially saving lives and making our roads safer. Applications of this model extend to insurance companies for risk assessment, ride-sharing services for driver monitoring, and personal safety apps to alert drivers of their driving patterns.\n\n\n\n\n\n\nThe dataset for this project is derived from a real-world scenario, specifically designed to capture driving behaviors. Utilizing a data collector application on Android devices, Ion Cojocaru and 2 Collaborators from Kaggle.com have gathered sensor readings directly relevant to driving dynamics.\n\n\n\nThe dataset is a rich collection of sensor data recorded from a Samsung Galaxy S21, chosen for its advanced sensor capabilities. Here’s a breakdown of the dataset features:\n\n\n\nAxes: X, Y, Z\nUnit: Meters per second squared (m/s²)\nNote: Gravitational acceleration has been filtered out to focus on the acceleration caused by driving actions.\n\n\n\n\n\nAxes: X, Y, Z\nUnit: Degrees per second (°/s)\nPurpose: Captures the angular changes during driving, indicative of turns and maneuvers.\n\n\n\n\n\nCategories: SLOW, NORMAL, AGGRESSIVE\nBasis: The driving behavior classification based on the sensor data patterns.\n\n\n\n\n\nSampling Rate: 2 samples per second, ensuring a fine-grained capture of driving dynamics.\nTimestamp: Included for each sample, allowing for temporal analysis of driving patterns.\n\nIn the following sections, we will delve into the preprocessing, exploratory analysis, and modeling of this dataset to build a robust classifier for driving behaviors.\n\n\n\n\n\nIn the realm of machine learning, data preprocessing is a critical step in preparing raw data for modeling. Our datasets, motion_data_1.csv (test data) and motion_data_2.csv (train data), contain acceleration and rotation measurements alongside driving behavior classifications. Let’s walk through the preprocessing steps:\n\n\nFirst, we’ll check for missing values in both datasets. Missing data can significantly impact the performance of a machine learning model. If missing values are found, strategies such as imputation or removal of the affected rows can be considered.\n\n# Install and import all libraries\nimport sys\n!{sys.executable} -m pip install seaborn\n!{sys.executable} -m pip install matplotlib\n!{sys.executable} -m pip install scikit-learn\n!{sys.executable} -m pip install pandas\n!{sys.executable} -m pip install numpy\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: seaborn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.13.0)\nRequirement already satisfied: pandas&gt;=1.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (2.1.3)\nRequirement already satisfied: matplotlib!=3.6.1,&gt;=3.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (3.8.2)\nRequirement already satisfied: numpy!=1.24.0,&gt;=1.20 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (1.26.2)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (2.8.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.2.0)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (23.2)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (3.1.1)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (6.1.1)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (0.12.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.4.5)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (4.45.1)\nRequirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (10.1.0)\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (3.17.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3.post1)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: matplotlib in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (3.8.2)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.1.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.1.0)\nRequirement already satisfied: numpy&lt;2,&gt;=1.21 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.2)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (23.2)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.45.1)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib) (3.17.0)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: scikit-learn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.3.2)\nRequirement already satisfied: joblib&gt;=1.1.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.3.2)\nRequirement already satisfied: scipy&gt;=1.5.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: numpy&lt;2.0,&gt;=1.17.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pandas in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (2.1.3)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)\nRequirement already satisfied: numpy&lt;2,&gt;=1.22.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: numpy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.26.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\n\n\n\n# Load the datasets\ntest_data = pd.read_csv('motion_data_1.csv')\ntrain_data = pd.read_csv('motion_data_2.csv')\n\n# Checking for missing values in both datasets\nmissing_values_test = test_data.isnull().sum()\nmissing_values_train = train_data.isnull().sum()\n\n\n\n\nNormalization or scaling is crucial when dealing with sensor data. It ensures that each feature contributes proportionately to the final prediction. Given the different scales of acceleration (in m/s²) and rotation (in °/s), applying a scaling method like Min-Max scaling or Standardization is important.\n\n# Standardizing the data\nscaler = StandardScaler()\ntrain_data_scaled = scaler.fit_transform(train_data.iloc[:, :-2]) # Excluding 'Class' and 'Timestamp' columns\ntest_data_scaled = scaler.transform(test_data.iloc[:, :-2])\n\n\n\n\nIn our case, we attempted to engineer a new deriving features: the total magnitude of acceleration.\n\n# Adding a feature: Total magnitude of acceleration\ntrain_data['TotalAcc'] = np.sqrt(train_data['AccX']**2 + train_data['AccY']**2 + train_data['AccZ']**2)\ntest_data['TotalAcc'] = np.sqrt(test_data['AccX']**2 + test_data['AccY']**2 + test_data['AccZ']**2)\n\n\n\n\n\n\n\nUnderstanding the basic statistics of the dataset is essential. This includes measures like mean, median, standard deviation, etc., providing insights into the data distribution.\n\n# Descriptive statistics of the training data\ntrain_data_description = train_data.describe()\n\n\n\n\n\n\nHistograms and box plots are effective for visualizing the distribution of sensor data and identifying outliers or skewness in the data.\n\n# Setting up the figure for multiple subplots\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 15))\nfig.suptitle('Histograms of Acceleration and Rotation Data', fontsize=16)\n\n# Plotting histograms for each sensor data column\nsns.histplot(train_data['AccX'], kde=True, ax=axes[0, 0], color='skyblue')\naxes[0, 0].set_title('Acceleration in X-axis (AccX)')\n\nsns.histplot(train_data['AccY'], kde=True, ax=axes[0, 1], color='olive')\naxes[0, 1].set_title('Acceleration in Y-axis (AccY)')\n\nsns.histplot(train_data['AccZ'], kde=True, ax=axes[1, 0], color='gold')\naxes[1, 0].set_title('Acceleration in Z-axis (AccZ)')\n\nsns.histplot(train_data['GyroX'], kde=True, ax=axes[1, 1], color='teal')\naxes[1, 1].set_title('Rotation in X-axis (GyroX)')\n\nsns.histplot(train_data['GyroY'], kde=True, ax=axes[2, 0], color='salmon')\naxes[2, 0].set_title('Rotation in Y-axis (GyroY)')\n\nsns.histplot(train_data['GyroZ'], kde=True, ax=axes[2, 1], color='violet')\naxes[2, 1].set_title('Rotation in Z-axis (GyroZ)')\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n\n\n\n\nAcceleration (AccX, AccY, AccZ): The distributions for acceleration on all three axes appear to be roughly bell-shaped, indicating that most of the readings are clustered around the mean, with fewer readings at the extreme ends. This suggests normal driving conditions with occasional variances that could indicate moments of acceleration or deceleration.\nRotation (GyroX, GyroY, GyroZ): The rotation data histograms show a similar bell-shaped distribution, especially for the X and Y axes, indicating consistent turning behavior with some outliers potentially representing more aggressive turns or corrections. The GyroZ histogram is notably narrower, which might suggest that rotation around the Z-axis (often corresponding to yaw movements) is less variable during normal driving conditions.\n\n\n\n\nA correlation heatmap helps in understanding the relationships between different sensor readings. It’s crucial for identifying features that are highly correlated and might need to be addressed during feature selection.\n\n# For the correlation heatmap, we exclude non-numeric columns (Class and Timestamp)\ntrain_data_numeric = train_data.select_dtypes(include=['float64', 'int64'])\n\n# Generating the correlation heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(train_data_numeric.corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap of Numeric Features')\nplt.show()\n\n\n\n\nIn this heatmap:\n\nValues close to 1 or -1 indicate a strong positive or negative correlation, respectively.\nValues close to 0 suggest no linear correlation between the variables.\n\nIt appears that:\n\nThere are no exceptionally strong correlations between the different axes of acceleration and rotation, which is desirable in a dataset used for behavior prediction as it indicates that the sensor readings provide unique information.\nThe strongest negative correlation is observed between GyroZ and AccX, which might suggest that certain types of aggressive driving behaviors cause inverse changes in these two measurements.\n\nThe absence of very high correlations means that there may not be redundant features in the dataset, which is good for a machine learning model that relies on diverse data points to make predictions. However, the subtle correlations that do exist can still provide valuable insights when developing features and training models.\nIn the next sections, we will delve into model selection, training, and evaluation, using the insights and data preparations we’ve just discussed. Stay tuned!\n\n\n\n\n\nPreparing the data for modeling is a crucial step in the machine learning pipeline. It ensures that the data fed into the model is clean, representative, and well-formatted.\n\n\nWe have two datasets: train_data and test_data, pre-split for our convenience. Typically, we would use a function like train_test_split from scikit-learn to divide our dataset into a training set and a test set, ensuring that both sets are representative of the overall distribution. However, in this scenario, that step is already accounted for.\n\n\n\n\nSelecting the right model is about finding the balance between prediction accuracy, computational efficiency, and the ease of interpretation.\n\n\nFor our classification task, we have several models at our disposal:\n\nDecision Tree: A good baseline that is easy to interpret.\nRandom Forest: An ensemble method that can improve on the performance of a single decision tree.\nSupport Vector Machine (SVM): Effective in high-dimensional spaces.\nNeural Networks: Potentially high performance but may require more data and compute resources.\n\n\n\n\nWhen selecting the model, we consider several criteria:\n\nAccuracy: How often the model makes the correct prediction.\nSpeed: How quickly the model can be trained and used for prediction.\nInterpretability: The ease with which we can understand the model’s predictions.\n\n\n\n\n\nTraining our models is like teaching them to understand patterns within the data that distinguish between SLOW, NORMAL, and AGGRESSIVE driving behaviors. We will employ four different machine learning models to find the best predictor.\n\n\nWe start by training different types of models. Each has its own strengths and might capture different aspects of the driving behavior.\n\n# Initializing models with default parameters\ndecision_tree = DecisionTreeClassifier()\nrandom_forest = RandomForestClassifier()\nsvm = SVC()\nneural_network = MLPClassifier()\n\ndecision_tree.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\nrandom_forest.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\nsvm.fit(train_data.drop(['Class'], axis=1), train_data['Class']) \nneural_network.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\n\nMLPClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MLPClassifierMLPClassifier()\n\n\nThe fit method allows the models to learn from the training data. It’s during this process that they identify which features are most important for predicting driving behavior.\n\n\n\nTo optimize our models, we tweak their settings, known as hyperparameters. This process is akin to fine-tuning an instrument to ensure it plays the perfect note.\n\n# Reduce the number of iterations and cross-validation folds\nn_iter_search = 5\ncv_folds = 3\n\n# Simplify the models - example for Random Forest\nrf_param_grid = {\n    'n_estimators': [50, 100],  # reduced number of trees\n    'max_depth': [10, None],  # fewer options\n    'min_samples_split': [2, 5]\n}\n\n# Initialize the RandomizedSearchCV object for Random Forest with fewer iterations and folds\nrandom_search_rf = RandomizedSearchCV(\n    RandomForestClassifier(), \n    param_distributions=rf_param_grid, \n    n_iter=n_iter_search, \n    cv=cv_folds, \n    n_jobs=-1, \n    random_state=42\n)\nrandom_search_rf.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\nbest_params_rf = random_search_rf.best_params_\n\n# RandomizedSearchCV for Decision Tree\ndt_param_grid = {\n    'max_depth': [10, 20, 30],\n    'min_samples_leaf': [1, 2, 4]\n}\nrandom_search_dt = RandomizedSearchCV(\n    DecisionTreeClassifier(), \n    param_distributions=dt_param_grid, \n    n_iter=n_iter_search, \n    cv=cv_folds, \n    random_state=42\n)\nrandom_search_dt.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\nbest_params_dt = random_search_dt.best_params_\n\n# RandomizedSearchCV for SVM\n# Using an extremely small subset of the data\nsubset_size = 50  # Very small subset for extremely quick execution\ntrain_subset = train_data.sample(n=subset_size, random_state=42)\n# Simplified RandomizedSearchCV for SVM\nrandom_search_svm = RandomizedSearchCV(\n    SVC(), \n    param_distributions={'C': [1], 'kernel': ['linear']},  # Minimal hyperparameter space\n    n_iter=1,  # Only one iteration\n    cv=2,  # Reduced to 2-fold cross-validation\n    n_jobs=1,  # Limit the number of jobs to 1 for a quick run\n    random_state=42,\n    verbose=0  # No verbosity\n)\nrandom_search_svm.fit(train_subset.drop(['Class'], axis=1), train_subset['Class'])\nbest_params_svm = random_search_svm.best_params_\n\n# RandomizedSearchCV for Neural Network\n# Using a very small subset of the data\nsubset_size = 50  # Extremely small subset for quick execution\ntrain_subset = train_data.sample(n=subset_size, random_state=42)\n\n# Simplified parameter grid for Neural Network\nnn_param_grid = {\n    'hidden_layer_sizes': [(50,)],  # Smaller network\n    'activation': ['relu'],  # One activation function\n    'solver': ['adam'],  # One solver\n    'max_iter': [100]  # Fewer iterations\n}\n\n# RandomizedSearchCV for Neural Network with simplified settings\nrandom_search_nn = RandomizedSearchCV(\n    MLPClassifier(), \n    nn_param_grid, \n    n_iter=2,  # Fewer iterations\n    cv=2,  # Reduced cross-validation\n    n_jobs=1,  # Using single job for faster execution in limited environments\n    random_state=42,\n    verbose=0\n)\nrandom_search_nn.fit(train_subset.drop(['Class'], axis=1), train_subset['Class'])\nbest_params_nn = random_search_nn.best_params_\n\n# Print the best parameters for all models\nprint(\"Best parameters for Random Forest:\", best_params_rf)\nprint(\"Best parameters for Decision Tree:\", best_params_dt)\nprint(\"Best parameters for SVM:\", best_params_svm)\nprint(\"Best parameters for Neural Network:\", best_params_nn)\n\nBest parameters for Random Forest: {'n_estimators': 100, 'min_samples_split': 5, 'max_depth': None}\nBest parameters for Decision Tree: {'min_samples_leaf': 2, 'max_depth': 30}\nBest parameters for SVM: {'kernel': 'linear', 'C': 1}\nBest parameters for Neural Network: {'solver': 'adam', 'max_iter': 100, 'hidden_layer_sizes': (50,), 'activation': 'relu'}\n\n\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 1 is smaller than n_iter=2. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n  warnings.warn(\n\n\nUsing GridSearchCV, we systematically work through multiple combinations of parameter tunes, cross-validating. The summary of the best parameters for various machine learning models from a tuning process are as follows: For the Random Forest, the optimal settings are 50 trees (n_estimators), a minimum of 2 samples required to split a node (min_samples_split), and a maximum tree depth of 10 (max_depth). The Decision Tree’s best parameters include a maximum depth of 30 (max_depth) and a minimum of 2 samples required at a leaf node (min_samples_leaf). For the SVM, a linear kernel with a penalty parameter C of 1 yields the best results. Lastly, the optimal configuration for the Neural Network involves the ‘adam’ solver, a maximum of 100 iterations (max_iter), 50 neurons in the hidden layer (hidden_layer_sizes), and ‘relu’ activation function. These parameters are likely the result of a grid search optimization process aiming to enhance model performance.\n\n\n\n\nAfter training our models and tuning their hyperparameters, the next critical step is model evaluation. This step helps us understand how well our models perform and guides us in selecting the best one for our application.\n\n\nWe’ll evaluate each model using several key metrics: accuracy, precision, recall, and the F1-score. These metrics give us a well-rounded view of our models’ performance.\n\nAccuracy: The proportion of true results (both true positives and true negatives) among the total number of cases examined.\nPrecision: The ratio of correctly predicted positive observations to the total predicted positives.\nRecall (Sensitivity): The ratio of correctly predicted positive observations to all observations in the actual class.\nF1-Score: The weighted average of Precision and Recall, best if there’s an uneven class distribution.\n\nLet’s calculate and print these metrics for each model:\n\n# Models we've trained\nmodels = {\n    \"Decision Tree\": decision_tree,\n    \"Random Forest\": random_forest,\n    \"SVM\": svm,\n    \"Neural Network\": neural_network\n}\n\n# Evaluate each model\nfor name, model in models.items():\n    predictions = model.predict(test_data.drop(['Class'], axis=1))\n    accuracy = accuracy_score(test_data['Class'], predictions)\n    precision = precision_score(test_data['Class'], predictions, average='weighted')\n    recall = recall_score(test_data['Class'], predictions, average='weighted')\n    f1 = f1_score(test_data['Class'], predictions, average='weighted')\n\n    print(f\"{name}:\")\n    print(f\"  Accuracy: {accuracy:.4f}\")\n    print(f\"  Precision: {precision:.4f}\")\n    print(f\"  Recall: {recall:.4f}\")\n    print(f\"  F1-Score: {f1:.4f}\\n\")\n\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\nDecision Tree:\n  Accuracy: 0.3233\n  Precision: 0.1045\n  Recall: 0.3233\n  F1-Score: 0.1580\n\nRandom Forest:\n  Accuracy: 0.3233\n  Precision: 0.1045\n  Recall: 0.3233\n  F1-Score: 0.1580\n\nSVM:\n  Accuracy: 0.4128\n  Precision: 0.1704\n  Recall: 0.4128\n  F1-Score: 0.2412\n\nNeural Network:\n  Accuracy: 0.3233\n  Precision: 0.1045\n  Recall: 0.3233\n  F1-Score: 0.1580\n\n\n\n\n\n\nThe confusion matrix is a useful tool for understanding the performance of a classification model. It shows the actual vs. predicted classifications.\nLet’s plot the confusion matrix for each model and discuss what the scores mean:\n\nfor name, model in models.items():\n    predictions = model.predict(test_data.drop(['Class'], axis=1))\n    conf_mat = confusion_matrix(test_data['Class'], predictions)\n\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(conf_mat, annot=True, fmt='g')\n    plt.title(f'Confusion Matrix for {name}')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrue Positives (TP): Correctly predicted positive observations.\nTrue Negatives (TN): Correctly predicted negative observations.\nFalse Positives (FP): Incorrectly predicted positive observations (Type I error).\nFalse Negatives (FN): Incorrectly predicted negative observations (Type II error).\n\n(A model with high TP and TN values and low FP and FN values is generally considered good. However, the importance of each type of error can vary based on the application. For instance, in medical diagnosis, reducing FN (missed diagnoses) may be more critical than reducing FP.)\n\n\n\n\nInterpreting machine learning models involves understanding the decisions made by the model and assessing their importance in the context of the task. For our driving behavior classification project, the model interpretation will shed light on how well the models are distinguishing between SLOW, NORMAL, and AGGRESSIVE driving behaviors.\n\n\nThe confusion matrices provide a visual and quantitative way to measure the performance of the models. Ideally, a well-performing model would have high numbers along the diagonal, indicating correct predictions, and low numbers off the diagonal.\nIn all four confusion matrices, we observe that the models predict only one class, which indicates a severe imbalance in their learning, potentially caused by class imbalance or other data issues. They fail to predict any instances of the ‘SLOW’ and ‘AGGRESSIVE’ classes, classifying everything as ‘NORMAL’. This is also reflected in the low precision and F1-scores for all models, as these metrics account for both false positives and false negatives.\n\n\n\nUnderstanding which features are most important for making predictions can provide insights into the dataset and the model’s decision-making process. For tree-based models, we can directly retrieve feature importance. For other models, such as SVM and Neural Networks, the interpretation can be more complex and may require additional techniques like permutation importance.\nFor Decision Trees and Random Forests, feature importance is calculated based on how well they split the data and reduce impurity. Here’s how you might compute and plot the feature importances for the Random Forest model:\n\nimportances = random_forest.feature_importances_\nindices = np.argsort(importances)[::-1]\nfeatures = train_data.drop(['Class'], axis=1).columns\n\n# Plot the feature importances of the forest\nplt.figure(figsize=(12, 6))\nplt.title(\"Feature importances\")\nplt.bar(range(train_data.shape[1] - 1), importances[indices], align=\"center\")\nplt.xticks(range(train_data.shape[1] - 1), features[indices], rotation=90)\nplt.xlim([-1, train_data.shape[1] - 1])\nplt.show()\n\n\n\n\nFor the SVM and Neural Network models, feature importance is not as straightforward because these models do not inherently provide a method for evaluating the importance of features. However, for the SVM model with a linear kernel, we can look at the weights assigned to each feature to determine their importance. For neural networks, especially deep ones, feature importance is a more complex area of research and often involves using additional tools or methods.\n\n\nThe models’ inability to correctly classify all three driving behaviors indicates that further investigation into the data distribution, preprocessing, and model training is necessary. This could include resampling techniques to address class imbalance, feature engineering to better capture the characteristics of aggressive driving, or exploring more complex models and tuning strategies. It’s also important to consider the real-world applicability of these models — in a safety-critical domain such as driving behavior classification, we must strive for high precision and recall to ensure all aggressive behaviors are accurately detected.\n\n\n\n\n\nIn summarizing our findings, the machine learning models developed to classify driving behavior as SLOW, NORMAL, or AGGRESSIVE revealed critical insights. The evaluation metrics and confusion matrices indicated that our models predominantly predicted the NORMAL class while failing to recognize SLOW and AGGRESSIVE behaviors. The accuracy across the models ranged from approximately 26% to 41%, with the SVM model achieving the highest accuracy. However, the precision and recall scores were low for all models, indicating a significant area for improvement.\n\n\nThe limitations of our current approach are evident:\n\nClass Imbalance: The models’ tendency to predict mostly the NORMAL class suggests a possible class imbalance.\nModel Overfitting: The poor generalization to other classes may also indicate overfitting to the majority class.\nFeature Representation: The current features may not sufficiently represent the characteristics unique to each class.\n\n\n\n\nSeveral improvements can be proposed:\n\nData Resampling: Implementing techniques like SMOTE or random oversampling to balance the class distribution could be beneficial.\nFeature Engineering: Developing more sophisticated features or utilizing feature selection methods to capture more nuanced patterns of aggressive driving.\nModel Complexity: Exploring more complex models or deep learning approaches that might capture the intricacies in the data better.\nExtended Hyperparameter Tuning: Conducting a more thorough hyperparameter optimization, given more computational time and resources.\n\nIn conclusion, while the current models have provided a foundation, further work is needed to develop a robust classifier that can accurately identify various driving behaviors, which is crucial for enhancing road safety and reducing accidents.\n\n\n\n\n\nhttps://builtin.com/data-science/supervised-machine-learning-classification\nhttps://www.analyticsvidhya.com/blog/2021/09/a-complete-guide-to-understand-classification-in-machine-learning/\nOpenAI. (2023). ChatGPT [Large language model]. https://chat.openai.com\nhttps://www.kaggle.com/datasets/outofskills/driving-behavior"
  },
  {
    "objectID": "posts/classification2/index.html#project-introduction",
    "href": "posts/classification2/index.html#project-introduction",
    "title": "4. Classification",
    "section": "",
    "text": "Welcome to our exploration into the world of machine learning and its application in predicting driving behaviors. In this project, we dive into the realm of vehicular safety, aiming to leverage sensor data to classify driving patterns into three categories: SLOW, NORMAL, and AGGRESSIVE. This endeavor is not just a technical challenge but a crucial step towards enhancing road safety and reducing traffic accidents.\n\n\nThe primary goal of this project is to accurately predict driving behaviors using data from commonly available sensors in smartphones. By analyzing accelerometer and gyroscope data, we aim to classify driving styles into SLOW, NORMAL, or AGGRESSIVE, contributing significantly to the prevention of road mishaps.\n\n\n\nThe importance of this project is underscored by the alarming statistics from the AAA Foundation for Traffic Safety, highlighting that over half of fatal crashes involve aggressive driving actions. Through this project, we offer a scalable and readily deployable solution to monitor and predict dangerous driving behaviors, potentially saving lives and making our roads safer. Applications of this model extend to insurance companies for risk assessment, ride-sharing services for driver monitoring, and personal safety apps to alert drivers of their driving patterns."
  },
  {
    "objectID": "posts/classification2/index.html#data-collection-and-description",
    "href": "posts/classification2/index.html#data-collection-and-description",
    "title": "4. Classification",
    "section": "",
    "text": "The dataset for this project is derived from a real-world scenario, specifically designed to capture driving behaviors. Utilizing a data collector application on Android devices, Ion Cojocaru and 2 Collaborators from Kaggle.com have gathered sensor readings directly relevant to driving dynamics.\n\n\n\nThe dataset is a rich collection of sensor data recorded from a Samsung Galaxy S21, chosen for its advanced sensor capabilities. Here’s a breakdown of the dataset features:\n\n\n\nAxes: X, Y, Z\nUnit: Meters per second squared (m/s²)\nNote: Gravitational acceleration has been filtered out to focus on the acceleration caused by driving actions.\n\n\n\n\n\nAxes: X, Y, Z\nUnit: Degrees per second (°/s)\nPurpose: Captures the angular changes during driving, indicative of turns and maneuvers.\n\n\n\n\n\nCategories: SLOW, NORMAL, AGGRESSIVE\nBasis: The driving behavior classification based on the sensor data patterns.\n\n\n\n\n\nSampling Rate: 2 samples per second, ensuring a fine-grained capture of driving dynamics.\nTimestamp: Included for each sample, allowing for temporal analysis of driving patterns.\n\nIn the following sections, we will delve into the preprocessing, exploratory analysis, and modeling of this dataset to build a robust classifier for driving behaviors."
  },
  {
    "objectID": "posts/classification2/index.html#data-preprocessing",
    "href": "posts/classification2/index.html#data-preprocessing",
    "title": "4. Classification",
    "section": "",
    "text": "In the realm of machine learning, data preprocessing is a critical step in preparing raw data for modeling. Our datasets, motion_data_1.csv (test data) and motion_data_2.csv (train data), contain acceleration and rotation measurements alongside driving behavior classifications. Let’s walk through the preprocessing steps:\n\n\nFirst, we’ll check for missing values in both datasets. Missing data can significantly impact the performance of a machine learning model. If missing values are found, strategies such as imputation or removal of the affected rows can be considered.\n\n# Install and import all libraries\nimport sys\n!{sys.executable} -m pip install seaborn\n!{sys.executable} -m pip install matplotlib\n!{sys.executable} -m pip install scikit-learn\n!{sys.executable} -m pip install pandas\n!{sys.executable} -m pip install numpy\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: seaborn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.13.0)\nRequirement already satisfied: pandas&gt;=1.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (2.1.3)\nRequirement already satisfied: matplotlib!=3.6.1,&gt;=3.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (3.8.2)\nRequirement already satisfied: numpy!=1.24.0,&gt;=1.20 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (1.26.2)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (2.8.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.2.0)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (23.2)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (3.1.1)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (6.1.1)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (0.12.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.4.5)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (4.45.1)\nRequirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (10.1.0)\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (3.17.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3.post1)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: matplotlib in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (3.8.2)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.1.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.1.0)\nRequirement already satisfied: numpy&lt;2,&gt;=1.21 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.2)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (23.2)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.45.1)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib) (3.17.0)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: scikit-learn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.3.2)\nRequirement already satisfied: joblib&gt;=1.1.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.3.2)\nRequirement already satisfied: scipy&gt;=1.5.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: numpy&lt;2.0,&gt;=1.17.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pandas in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (2.1.3)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)\nRequirement already satisfied: numpy&lt;2,&gt;=1.22.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: numpy in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.26.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\n\n\n\n# Load the datasets\ntest_data = pd.read_csv('motion_data_1.csv')\ntrain_data = pd.read_csv('motion_data_2.csv')\n\n# Checking for missing values in both datasets\nmissing_values_test = test_data.isnull().sum()\nmissing_values_train = train_data.isnull().sum()\n\n\n\n\nNormalization or scaling is crucial when dealing with sensor data. It ensures that each feature contributes proportionately to the final prediction. Given the different scales of acceleration (in m/s²) and rotation (in °/s), applying a scaling method like Min-Max scaling or Standardization is important.\n\n# Standardizing the data\nscaler = StandardScaler()\ntrain_data_scaled = scaler.fit_transform(train_data.iloc[:, :-2]) # Excluding 'Class' and 'Timestamp' columns\ntest_data_scaled = scaler.transform(test_data.iloc[:, :-2])\n\n\n\n\nIn our case, we attempted to engineer a new deriving features: the total magnitude of acceleration.\n\n# Adding a feature: Total magnitude of acceleration\ntrain_data['TotalAcc'] = np.sqrt(train_data['AccX']**2 + train_data['AccY']**2 + train_data['AccZ']**2)\ntest_data['TotalAcc'] = np.sqrt(test_data['AccX']**2 + test_data['AccY']**2 + test_data['AccZ']**2)"
  },
  {
    "objectID": "posts/classification2/index.html#exploratory-data-analysis-eda",
    "href": "posts/classification2/index.html#exploratory-data-analysis-eda",
    "title": "4. Classification",
    "section": "",
    "text": "Understanding the basic statistics of the dataset is essential. This includes measures like mean, median, standard deviation, etc., providing insights into the data distribution.\n\n# Descriptive statistics of the training data\ntrain_data_description = train_data.describe()\n\n\n\n\n\n\nHistograms and box plots are effective for visualizing the distribution of sensor data and identifying outliers or skewness in the data.\n\n# Setting up the figure for multiple subplots\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 15))\nfig.suptitle('Histograms of Acceleration and Rotation Data', fontsize=16)\n\n# Plotting histograms for each sensor data column\nsns.histplot(train_data['AccX'], kde=True, ax=axes[0, 0], color='skyblue')\naxes[0, 0].set_title('Acceleration in X-axis (AccX)')\n\nsns.histplot(train_data['AccY'], kde=True, ax=axes[0, 1], color='olive')\naxes[0, 1].set_title('Acceleration in Y-axis (AccY)')\n\nsns.histplot(train_data['AccZ'], kde=True, ax=axes[1, 0], color='gold')\naxes[1, 0].set_title('Acceleration in Z-axis (AccZ)')\n\nsns.histplot(train_data['GyroX'], kde=True, ax=axes[1, 1], color='teal')\naxes[1, 1].set_title('Rotation in X-axis (GyroX)')\n\nsns.histplot(train_data['GyroY'], kde=True, ax=axes[2, 0], color='salmon')\naxes[2, 0].set_title('Rotation in Y-axis (GyroY)')\n\nsns.histplot(train_data['GyroZ'], kde=True, ax=axes[2, 1], color='violet')\naxes[2, 1].set_title('Rotation in Z-axis (GyroZ)')\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n\n\n\n\nAcceleration (AccX, AccY, AccZ): The distributions for acceleration on all three axes appear to be roughly bell-shaped, indicating that most of the readings are clustered around the mean, with fewer readings at the extreme ends. This suggests normal driving conditions with occasional variances that could indicate moments of acceleration or deceleration.\nRotation (GyroX, GyroY, GyroZ): The rotation data histograms show a similar bell-shaped distribution, especially for the X and Y axes, indicating consistent turning behavior with some outliers potentially representing more aggressive turns or corrections. The GyroZ histogram is notably narrower, which might suggest that rotation around the Z-axis (often corresponding to yaw movements) is less variable during normal driving conditions.\n\n\n\n\nA correlation heatmap helps in understanding the relationships between different sensor readings. It’s crucial for identifying features that are highly correlated and might need to be addressed during feature selection.\n\n# For the correlation heatmap, we exclude non-numeric columns (Class and Timestamp)\ntrain_data_numeric = train_data.select_dtypes(include=['float64', 'int64'])\n\n# Generating the correlation heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(train_data_numeric.corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap of Numeric Features')\nplt.show()\n\n\n\n\nIn this heatmap:\n\nValues close to 1 or -1 indicate a strong positive or negative correlation, respectively.\nValues close to 0 suggest no linear correlation between the variables.\n\nIt appears that:\n\nThere are no exceptionally strong correlations between the different axes of acceleration and rotation, which is desirable in a dataset used for behavior prediction as it indicates that the sensor readings provide unique information.\nThe strongest negative correlation is observed between GyroZ and AccX, which might suggest that certain types of aggressive driving behaviors cause inverse changes in these two measurements.\n\nThe absence of very high correlations means that there may not be redundant features in the dataset, which is good for a machine learning model that relies on diverse data points to make predictions. However, the subtle correlations that do exist can still provide valuable insights when developing features and training models.\nIn the next sections, we will delve into model selection, training, and evaluation, using the insights and data preparations we’ve just discussed. Stay tuned!"
  },
  {
    "objectID": "posts/classification2/index.html#data-preparation-for-modeling",
    "href": "posts/classification2/index.html#data-preparation-for-modeling",
    "title": "4. Classification",
    "section": "",
    "text": "Preparing the data for modeling is a crucial step in the machine learning pipeline. It ensures that the data fed into the model is clean, representative, and well-formatted.\n\n\nWe have two datasets: train_data and test_data, pre-split for our convenience. Typically, we would use a function like train_test_split from scikit-learn to divide our dataset into a training set and a test set, ensuring that both sets are representative of the overall distribution. However, in this scenario, that step is already accounted for."
  },
  {
    "objectID": "posts/classification2/index.html#model-selection",
    "href": "posts/classification2/index.html#model-selection",
    "title": "4. Classification",
    "section": "",
    "text": "Selecting the right model is about finding the balance between prediction accuracy, computational efficiency, and the ease of interpretation.\n\n\nFor our classification task, we have several models at our disposal:\n\nDecision Tree: A good baseline that is easy to interpret.\nRandom Forest: An ensemble method that can improve on the performance of a single decision tree.\nSupport Vector Machine (SVM): Effective in high-dimensional spaces.\nNeural Networks: Potentially high performance but may require more data and compute resources.\n\n\n\n\nWhen selecting the model, we consider several criteria:\n\nAccuracy: How often the model makes the correct prediction.\nSpeed: How quickly the model can be trained and used for prediction.\nInterpretability: The ease with which we can understand the model’s predictions."
  },
  {
    "objectID": "posts/classification2/index.html#model-training",
    "href": "posts/classification2/index.html#model-training",
    "title": "4. Classification",
    "section": "",
    "text": "Training our models is like teaching them to understand patterns within the data that distinguish between SLOW, NORMAL, and AGGRESSIVE driving behaviors. We will employ four different machine learning models to find the best predictor.\n\n\nWe start by training different types of models. Each has its own strengths and might capture different aspects of the driving behavior.\n\n# Initializing models with default parameters\ndecision_tree = DecisionTreeClassifier()\nrandom_forest = RandomForestClassifier()\nsvm = SVC()\nneural_network = MLPClassifier()\n\ndecision_tree.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\nrandom_forest.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\nsvm.fit(train_data.drop(['Class'], axis=1), train_data['Class']) \nneural_network.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\n\nMLPClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MLPClassifierMLPClassifier()\n\n\nThe fit method allows the models to learn from the training data. It’s during this process that they identify which features are most important for predicting driving behavior.\n\n\n\nTo optimize our models, we tweak their settings, known as hyperparameters. This process is akin to fine-tuning an instrument to ensure it plays the perfect note.\n\n# Reduce the number of iterations and cross-validation folds\nn_iter_search = 5\ncv_folds = 3\n\n# Simplify the models - example for Random Forest\nrf_param_grid = {\n    'n_estimators': [50, 100],  # reduced number of trees\n    'max_depth': [10, None],  # fewer options\n    'min_samples_split': [2, 5]\n}\n\n# Initialize the RandomizedSearchCV object for Random Forest with fewer iterations and folds\nrandom_search_rf = RandomizedSearchCV(\n    RandomForestClassifier(), \n    param_distributions=rf_param_grid, \n    n_iter=n_iter_search, \n    cv=cv_folds, \n    n_jobs=-1, \n    random_state=42\n)\nrandom_search_rf.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\nbest_params_rf = random_search_rf.best_params_\n\n# RandomizedSearchCV for Decision Tree\ndt_param_grid = {\n    'max_depth': [10, 20, 30],\n    'min_samples_leaf': [1, 2, 4]\n}\nrandom_search_dt = RandomizedSearchCV(\n    DecisionTreeClassifier(), \n    param_distributions=dt_param_grid, \n    n_iter=n_iter_search, \n    cv=cv_folds, \n    random_state=42\n)\nrandom_search_dt.fit(train_data.drop(['Class'], axis=1), train_data['Class'])\nbest_params_dt = random_search_dt.best_params_\n\n# RandomizedSearchCV for SVM\n# Using an extremely small subset of the data\nsubset_size = 50  # Very small subset for extremely quick execution\ntrain_subset = train_data.sample(n=subset_size, random_state=42)\n# Simplified RandomizedSearchCV for SVM\nrandom_search_svm = RandomizedSearchCV(\n    SVC(), \n    param_distributions={'C': [1], 'kernel': ['linear']},  # Minimal hyperparameter space\n    n_iter=1,  # Only one iteration\n    cv=2,  # Reduced to 2-fold cross-validation\n    n_jobs=1,  # Limit the number of jobs to 1 for a quick run\n    random_state=42,\n    verbose=0  # No verbosity\n)\nrandom_search_svm.fit(train_subset.drop(['Class'], axis=1), train_subset['Class'])\nbest_params_svm = random_search_svm.best_params_\n\n# RandomizedSearchCV for Neural Network\n# Using a very small subset of the data\nsubset_size = 50  # Extremely small subset for quick execution\ntrain_subset = train_data.sample(n=subset_size, random_state=42)\n\n# Simplified parameter grid for Neural Network\nnn_param_grid = {\n    'hidden_layer_sizes': [(50,)],  # Smaller network\n    'activation': ['relu'],  # One activation function\n    'solver': ['adam'],  # One solver\n    'max_iter': [100]  # Fewer iterations\n}\n\n# RandomizedSearchCV for Neural Network with simplified settings\nrandom_search_nn = RandomizedSearchCV(\n    MLPClassifier(), \n    nn_param_grid, \n    n_iter=2,  # Fewer iterations\n    cv=2,  # Reduced cross-validation\n    n_jobs=1,  # Using single job for faster execution in limited environments\n    random_state=42,\n    verbose=0\n)\nrandom_search_nn.fit(train_subset.drop(['Class'], axis=1), train_subset['Class'])\nbest_params_nn = random_search_nn.best_params_\n\n# Print the best parameters for all models\nprint(\"Best parameters for Random Forest:\", best_params_rf)\nprint(\"Best parameters for Decision Tree:\", best_params_dt)\nprint(\"Best parameters for SVM:\", best_params_svm)\nprint(\"Best parameters for Neural Network:\", best_params_nn)\n\nBest parameters for Random Forest: {'n_estimators': 100, 'min_samples_split': 5, 'max_depth': None}\nBest parameters for Decision Tree: {'min_samples_leaf': 2, 'max_depth': 30}\nBest parameters for SVM: {'kernel': 'linear', 'C': 1}\nBest parameters for Neural Network: {'solver': 'adam', 'max_iter': 100, 'hidden_layer_sizes': (50,), 'activation': 'relu'}\n\n\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 1 is smaller than n_iter=2. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n  warnings.warn(\n\n\nUsing GridSearchCV, we systematically work through multiple combinations of parameter tunes, cross-validating. The summary of the best parameters for various machine learning models from a tuning process are as follows: For the Random Forest, the optimal settings are 50 trees (n_estimators), a minimum of 2 samples required to split a node (min_samples_split), and a maximum tree depth of 10 (max_depth). The Decision Tree’s best parameters include a maximum depth of 30 (max_depth) and a minimum of 2 samples required at a leaf node (min_samples_leaf). For the SVM, a linear kernel with a penalty parameter C of 1 yields the best results. Lastly, the optimal configuration for the Neural Network involves the ‘adam’ solver, a maximum of 100 iterations (max_iter), 50 neurons in the hidden layer (hidden_layer_sizes), and ‘relu’ activation function. These parameters are likely the result of a grid search optimization process aiming to enhance model performance."
  },
  {
    "objectID": "posts/classification2/index.html#model-evaluation",
    "href": "posts/classification2/index.html#model-evaluation",
    "title": "4. Classification",
    "section": "",
    "text": "After training our models and tuning their hyperparameters, the next critical step is model evaluation. This step helps us understand how well our models perform and guides us in selecting the best one for our application.\n\n\nWe’ll evaluate each model using several key metrics: accuracy, precision, recall, and the F1-score. These metrics give us a well-rounded view of our models’ performance.\n\nAccuracy: The proportion of true results (both true positives and true negatives) among the total number of cases examined.\nPrecision: The ratio of correctly predicted positive observations to the total predicted positives.\nRecall (Sensitivity): The ratio of correctly predicted positive observations to all observations in the actual class.\nF1-Score: The weighted average of Precision and Recall, best if there’s an uneven class distribution.\n\nLet’s calculate and print these metrics for each model:\n\n# Models we've trained\nmodels = {\n    \"Decision Tree\": decision_tree,\n    \"Random Forest\": random_forest,\n    \"SVM\": svm,\n    \"Neural Network\": neural_network\n}\n\n# Evaluate each model\nfor name, model in models.items():\n    predictions = model.predict(test_data.drop(['Class'], axis=1))\n    accuracy = accuracy_score(test_data['Class'], predictions)\n    precision = precision_score(test_data['Class'], predictions, average='weighted')\n    recall = recall_score(test_data['Class'], predictions, average='weighted')\n    f1 = f1_score(test_data['Class'], predictions, average='weighted')\n\n    print(f\"{name}:\")\n    print(f\"  Accuracy: {accuracy:.4f}\")\n    print(f\"  Precision: {precision:.4f}\")\n    print(f\"  Recall: {recall:.4f}\")\n    print(f\"  F1-Score: {f1:.4f}\\n\")\n\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\nDecision Tree:\n  Accuracy: 0.3233\n  Precision: 0.1045\n  Recall: 0.3233\n  F1-Score: 0.1580\n\nRandom Forest:\n  Accuracy: 0.3233\n  Precision: 0.1045\n  Recall: 0.3233\n  F1-Score: 0.1580\n\nSVM:\n  Accuracy: 0.4128\n  Precision: 0.1704\n  Recall: 0.4128\n  F1-Score: 0.2412\n\nNeural Network:\n  Accuracy: 0.3233\n  Precision: 0.1045\n  Recall: 0.3233\n  F1-Score: 0.1580\n\n\n\n\n\n\nThe confusion matrix is a useful tool for understanding the performance of a classification model. It shows the actual vs. predicted classifications.\nLet’s plot the confusion matrix for each model and discuss what the scores mean:\n\nfor name, model in models.items():\n    predictions = model.predict(test_data.drop(['Class'], axis=1))\n    conf_mat = confusion_matrix(test_data['Class'], predictions)\n\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(conf_mat, annot=True, fmt='g')\n    plt.title(f'Confusion Matrix for {name}')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrue Positives (TP): Correctly predicted positive observations.\nTrue Negatives (TN): Correctly predicted negative observations.\nFalse Positives (FP): Incorrectly predicted positive observations (Type I error).\nFalse Negatives (FN): Incorrectly predicted negative observations (Type II error).\n\n(A model with high TP and TN values and low FP and FN values is generally considered good. However, the importance of each type of error can vary based on the application. For instance, in medical diagnosis, reducing FN (missed diagnoses) may be more critical than reducing FP.)"
  },
  {
    "objectID": "posts/classification2/index.html#model-interpretation",
    "href": "posts/classification2/index.html#model-interpretation",
    "title": "4. Classification",
    "section": "",
    "text": "Interpreting machine learning models involves understanding the decisions made by the model and assessing their importance in the context of the task. For our driving behavior classification project, the model interpretation will shed light on how well the models are distinguishing between SLOW, NORMAL, and AGGRESSIVE driving behaviors.\n\n\nThe confusion matrices provide a visual and quantitative way to measure the performance of the models. Ideally, a well-performing model would have high numbers along the diagonal, indicating correct predictions, and low numbers off the diagonal.\nIn all four confusion matrices, we observe that the models predict only one class, which indicates a severe imbalance in their learning, potentially caused by class imbalance or other data issues. They fail to predict any instances of the ‘SLOW’ and ‘AGGRESSIVE’ classes, classifying everything as ‘NORMAL’. This is also reflected in the low precision and F1-scores for all models, as these metrics account for both false positives and false negatives.\n\n\n\nUnderstanding which features are most important for making predictions can provide insights into the dataset and the model’s decision-making process. For tree-based models, we can directly retrieve feature importance. For other models, such as SVM and Neural Networks, the interpretation can be more complex and may require additional techniques like permutation importance.\nFor Decision Trees and Random Forests, feature importance is calculated based on how well they split the data and reduce impurity. Here’s how you might compute and plot the feature importances for the Random Forest model:\n\nimportances = random_forest.feature_importances_\nindices = np.argsort(importances)[::-1]\nfeatures = train_data.drop(['Class'], axis=1).columns\n\n# Plot the feature importances of the forest\nplt.figure(figsize=(12, 6))\nplt.title(\"Feature importances\")\nplt.bar(range(train_data.shape[1] - 1), importances[indices], align=\"center\")\nplt.xticks(range(train_data.shape[1] - 1), features[indices], rotation=90)\nplt.xlim([-1, train_data.shape[1] - 1])\nplt.show()\n\n\n\n\nFor the SVM and Neural Network models, feature importance is not as straightforward because these models do not inherently provide a method for evaluating the importance of features. However, for the SVM model with a linear kernel, we can look at the weights assigned to each feature to determine their importance. For neural networks, especially deep ones, feature importance is a more complex area of research and often involves using additional tools or methods.\n\n\nThe models’ inability to correctly classify all three driving behaviors indicates that further investigation into the data distribution, preprocessing, and model training is necessary. This could include resampling techniques to address class imbalance, feature engineering to better capture the characteristics of aggressive driving, or exploring more complex models and tuning strategies. It’s also important to consider the real-world applicability of these models — in a safety-critical domain such as driving behavior classification, we must strive for high precision and recall to ensure all aggressive behaviors are accurately detected."
  },
  {
    "objectID": "posts/classification2/index.html#conclusion",
    "href": "posts/classification2/index.html#conclusion",
    "title": "4. Classification",
    "section": "",
    "text": "In summarizing our findings, the machine learning models developed to classify driving behavior as SLOW, NORMAL, or AGGRESSIVE revealed critical insights. The evaluation metrics and confusion matrices indicated that our models predominantly predicted the NORMAL class while failing to recognize SLOW and AGGRESSIVE behaviors. The accuracy across the models ranged from approximately 26% to 41%, with the SVM model achieving the highest accuracy. However, the precision and recall scores were low for all models, indicating a significant area for improvement.\n\n\nThe limitations of our current approach are evident:\n\nClass Imbalance: The models’ tendency to predict mostly the NORMAL class suggests a possible class imbalance.\nModel Overfitting: The poor generalization to other classes may also indicate overfitting to the majority class.\nFeature Representation: The current features may not sufficiently represent the characteristics unique to each class.\n\n\n\n\nSeveral improvements can be proposed:\n\nData Resampling: Implementing techniques like SMOTE or random oversampling to balance the class distribution could be beneficial.\nFeature Engineering: Developing more sophisticated features or utilizing feature selection methods to capture more nuanced patterns of aggressive driving.\nModel Complexity: Exploring more complex models or deep learning approaches that might capture the intricacies in the data better.\nExtended Hyperparameter Tuning: Conducting a more thorough hyperparameter optimization, given more computational time and resources.\n\nIn conclusion, while the current models have provided a foundation, further work is needed to develop a robust classifier that can accurately identify various driving behaviors, which is crucial for enhancing road safety and reducing accidents."
  },
  {
    "objectID": "posts/classification2/index.html#reference",
    "href": "posts/classification2/index.html#reference",
    "title": "4. Classification",
    "section": "",
    "text": "https://builtin.com/data-science/supervised-machine-learning-classification\nhttps://www.analyticsvidhya.com/blog/2021/09/a-complete-guide-to-understand-classification-in-machine-learning/\nOpenAI. (2023). ChatGPT [Large language model]. https://chat.openai.com\nhttps://www.kaggle.com/datasets/outofskills/driving-behavior"
  },
  {
    "objectID": "posts/linear/index.html",
    "href": "posts/linear/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Air quality is a critical environmental factor impacting public health, ecosystem sustainability, and the global climate. Pollutants such as particulate matter (PM2.5 and PM10), sulfur dioxide (SO2), nitrogen dioxide (NO2), carbon monoxide (CO), and ozone (O3) can have severe health impacts, including respiratory and cardiovascular diseases. Understanding and predicting the concentrations of these pollutants is essential for creating effective environmental policies and public health interventions.\nIn this blog, we’ll delve into two powerful statistical methods used in predicting air pollutant concentrations: linear regression and Random Forest regression.\n\n\nLinear regression is a fundamental statistical approach used to model the relationship between a dependent variable and one or more independent variables. In the context of air quality, it helps us understand how various environmental factors like temperature, humidity, and wind speed influence pollutant levels. The model assumes a linear relationship between the variables, which can be represented as:\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n + \\epsilon\n\\]\nHere, ( Y ) is the pollutant concentration we want to predict, ( X_1, X_2, …, X_n ) are the environmental factors, ( _0, _1, …, _n ) are the coefficients to be estimated, and ( ) is the error term.\n\n\n\nRandom Forest, on the other hand, is a type of ensemble learning method, particularly useful for non-linear relationships. It operates by constructing multiple decision trees during training and outputting the mean prediction of the individual trees. This method is beneficial for handling complex interactions between variables and can provide more accurate predictions for complex datasets like those in air quality studies.\nThe purpose of this blog is to provide a step-by-step guide on how to use these methods, utilizing a Jupyter Notebook, to predict pollutant concentrations. We’ll start by exploring our dataset, air_data_all.csv, which includes a variety of environmental conditions and temporal factors, and then apply these regression techniques to gain insights into the factors affecting air quality.\nBy the end of this blog, you’ll have a clearer understanding of how to implement these techniques in Python and interpret their results, equipping you with the tools needed for insightful environmental data analysis.\n\n\n\n\nBefore delving into regression models, it’s essential to familiarize ourselves with the dataset at hand—air_data_all.csv. This dataset contains hourly air quality measurements and meteorological data from Beijing, spanning from March 1st, 2013, to February 28th, 2017. The dataset is sourced from the Beijing Municipal Environmental Monitoring Center and is matched with meteorological data from the China Meteorological Administration. However, it’s important to note that missing data points are marked as NA. Link to dataset is https://archive.ics.uci.edu/dataset/501/beijing+multi+site+air+quality+data.\n\n\nThe dataset is a valuable resource, encompassing a wide range of environmental conditions and pollutant concentrations. It records temporal information, including the year, month, day, and hour, alongside readings of key air pollutants such as PM2.5, PM10, SO2, NO2, CO, and O3. Additionally, meteorological factors like temperature (TEMP), pressure (PRES), dew point temperature (DEWP), precipitation (RAIN), wind direction (wd), and wind speed (WSPM) are included. This comprehensive data is instrumental for studying air pollution dynamics and its correlation with various environmental and temporal factors.\n\n\n\nEach column in the dataset serves a specific purpose:\n\nTemporal Data (year, month, day, hour): These columns provide insights into pollutant variations across different timescales.\nPollutant Concentrations (PM2.5, PM10, SO2, NO2, CO, O3): These are primary pollutants, crucial for urban air quality analysis.\nMeteorological Data (TEMP, PRES, DEWP, RAIN, wd, WSPM): Weather conditions significantly impact pollutant dispersion and concentration.\nStation: This column identifies the monitoring site, facilitating the study of geographical variations in air quality.\n\n\n\n\nUpon initial examination, it’s evident that the dataset offers a comprehensive foundation for in-depth analysis. However, several challenges may arise:\n\nMissing Data: Handling missing values is imperative to prevent biases in the models.\nHigh Dimensionality: The dataset’s numerous variables may lead to multicollinearity issues, where variables are highly correlated.\nNon-linear Relationships: Linear models may not capture all pollutant-environmental interactions, requiring more complex approaches like Random Forest.\n\nIn the upcoming sections, we will address these challenges while preparing the data for regression analysis. By the end of this process, we will be well-equipped to employ linear and Random Forest regression for accurate pollutant concentration predictions.\n\n\n\n\nIn our journey to understand and predict air quality, selecting the right target pollutants is crucial. For this analysis, we will focus on the following pollutants: PM2.5, PM10, SO2, NO2, CO, and O3. Let’s delve into the criteria and rationale behind choosing these specific pollutants.\n\n\nThe selection of target pollutants is based on the following criteria:\n\nHealth Impact: Pollutants known to have significant health effects are prioritized.\nPrevalence and Relevance: Common pollutants in urban and industrial areas are selected due to their higher relevance.\nData Availability: Pollutants with consistent and reliable data within the dataset are chosen to ensure the accuracy of the analysis.\n\n\n\n\nEach selected pollutant has its unique importance in air quality analysis:\n\nPM2.5 and PM10 (Particulate Matter): These are tiny particles in the air that reduce visibility and cause the air to appear hazy when levels are elevated. PM2.5 and PM10 are known for their ability to penetrate deep into the lungs and even into the bloodstream, causing respiratory and cardiovascular issues.\nSO2 (Sulfur Dioxide): A gas typically produced by burning fossil fuels containing sulfur. It’s associated with acid rain and has health implications, especially for individuals with asthma.\nNO2 (Nitrogen Dioxide): Primarily gets into the air from the burning of fuel. NO2 forms from emissions from cars, trucks and buses, power plants, and off-road equipment. It’s linked to various respiratory problems.\nCO (Carbon Monoxide): A colorless, odorless gas that is harmful when inhaled in large amounts. It’s released from vehicles and other combustion sources and can cause harmful health effects by reducing the amount of oxygen that can be transported in the bloodstream.\nO3 (Ozone): At ground level, ozone is a harmful air pollutant and a significant component of smog. It’s not emitted directly into the air but is created by chemical reactions between oxides of nitrogen (NOx) and volatile organic compounds (VOC) in the presence of sunlight.\n\nBy focusing on these pollutants, we can provide a comprehensive analysis of air quality and its health implications. Next, we will perform correlation analysis and multicollinearity checks to understand how these pollutants interact with each other and with different environmental factors.\n\n\n\n\nBefore delving into sophisticated regression models, it’s imperative to prepare our dataset, “air_data_all.csv,” for analysis. This stage, known as data cleaning and transformation, involves several key steps to ensure the data’s integrity and usability.\n\n\nThe initial step in data preprocessing is to identify and address any missing (NaN) or inconsistent data. This is crucial as such data can significantly skew our analysis.\n\nimport sys\n!{sys.executable} -m pip install seaborn\n!{sys.executable} -m pip install matplotlib\n!{sys.executable} -m pip install statsmodels\n!{sys.executable} -m pip install scikit-learn\n!{sys.executable} -m pip install pandas\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\n\n# Load the dataset\nsample_data = pd.read_csv('air_data_all.csv')\n\n# Identifying missing or infinite values\nsample_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# Checking for missing values\nmissing_values = sample_data.isnull().sum()\n\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: seaborn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.13.0)\nRequirement already satisfied: matplotlib!=3.6.1,&gt;=3.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (3.8.2)\nRequirement already satisfied: pandas&gt;=1.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (2.1.3)\nRequirement already satisfied: numpy!=1.24.0,&gt;=1.20 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (1.26.2)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (4.45.1)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (3.1.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.2.0)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (23.2)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (2.8.2)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.4.5)\nRequirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (10.1.0)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (6.1.1)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (0.12.1)\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (3.17.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3.post1)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: matplotlib in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (3.8.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.45.1)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (23.2)\nRequirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.1.0)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.1.1)\nRequirement already satisfied: numpy&lt;2,&gt;=1.21 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.2)\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib) (3.17.0)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: statsmodels in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.14.0)\nRequirement already satisfied: pandas&gt;=1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (2.1.3)\nRequirement already satisfied: patsy&gt;=0.5.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (0.5.3)\nRequirement already satisfied: scipy!=1.9.2,&gt;=1.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (1.11.4)\nRequirement already satisfied: packaging&gt;=21.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (23.2)\nRequirement already satisfied: numpy&gt;=1.18 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (1.26.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.0-&gt;statsmodels) (2023.3.post1)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.0-&gt;statsmodels) (2023.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.0-&gt;statsmodels) (2.8.2)\nRequirement already satisfied: six in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from patsy&gt;=0.5.2-&gt;statsmodels) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: scikit-learn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.3.2)\nRequirement already satisfied: joblib&gt;=1.1.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.3.2)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: scipy&gt;=1.5.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: numpy&lt;2.0,&gt;=1.17.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pandas in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (2.1.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: numpy&lt;2,&gt;=1.22.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\n\n\nIn this code block, we first replace any infinite values with NaNs. Then, we calculate the number of missing values in each column. Depending on the nature and volume of missing data, we can either fill these gaps using statistical methods (like mean, median) or consider removing the rows/columns entirely.\n\n\n\nNormalization (rescaling data to a range, like 0–1) and standardization (shifting the distribution to have a mean of zero and a standard deviation of one) are crucial for models sensitive to the scale of data, such as linear regression.\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardizing the dataset\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(sample_data[['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']])\n\n# Converting scaled data back to a DataFrame for further use\nscaled_df = pd.DataFrame(scaled_data, columns=['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM'])\n\nHere, we use StandardScaler from Scikit-learn to standardize the continuous variables such as temperature and pressure. This process aligns the data onto one scale, removing bias due to different units or scales.\n\n\n\nMany regression models require numerical input, so transforming categorical data into a numerical format is essential.\n\n# Creating dummy variables for categorical data\nwd_dummies = pd.get_dummies(sample_data['wd'])\nsample_data = pd.concat([sample_data, wd_dummies], axis=1)\n\nIn the above snippet, we create dummy variables for the wd column (wind direction), converting it into a format that can be efficiently processed by regression algorithms.\n\n\n\nVisualizations are effective for demonstrating the impact of data transformation. For instance, before and after standardization, we can plot histograms of a variable to observe changes in its distribution.\n\nimport matplotlib.pyplot as plt\n\n# Plotting before and after standardization\nplt.hist(sample_data['TEMP'], bins=30, alpha=0.5, label='Original TEMP')\nplt.hist(scaled_df['TEMP'], bins=30, alpha=0.5, label='Standardized TEMP')\nplt.legend()\nplt.show()\n\n\n\n\nThis histogram allows us to compare the distribution of the temperature data before and after standardization, showcasing the effects of our data transformation steps.\nBy completing these data cleaning and transformation processes, we ensure that our dataset is primed for accurate and effective regression analysis, laying a solid foundation for our subsequent modeling steps.\n\n\n\n\nAfter preparing our dataset, the next step in our analysis involves understanding the relationships between variables using correlation analysis and checking for multicollinearity. These steps are critical for ensuring the reliability and interpretability of our regression models.\n\n\nCorrelation analysis helps us understand the strength and direction of the relationship between two variables. In regression analysis, it’s important to identify how independent variables are related to the dependent variable and to each other.\n\n# Removing missing or infinite values from the scaled dataset\nscaled_df.replace([np.inf, -np.inf], np.nan, inplace=True)\nscaled_df.dropna(inplace=True)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Calculating the correlation matrix for key variables\ncorr_matrix = sample_data[['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']].corr()\n\n# Visualizing the correlation matrix using a heatmap\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title(\"Correlation Matrix of Environmental Factors and Pollutants\")\nplt.show()\n\n\n\n\nIn this code, we calculate and visualize the correlation matrix of key pollutants and environmental factors. This heatmap provides a clear visual representation of the relationships, where the color intensity and the value in each cell indicate the strength and direction of the correlation.\n\n\n\nMulticollinearity occurs when two or more independent variables in a regression model are highly correlated. This can lead to unreliable coefficient estimates, making it difficult to determine the effect of each independent variable.\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Preparing data for multicollinearity check\nfeatures = scaled_df[['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']]\n\n# Calculating VIF for each feature\nvif_data = pd.DataFrame()\nvif_data['Feature'] = features.columns\nvif_data['VIF'] = [variance_inflation_factor(features.values, i) for i in range(features.shape[1])]\n\nvif_data\n\n\n\n\n\n\n\n\nFeature\nVIF\n\n\n\n\n0\nTEMP\n5.355958\n\n\n1\nPRES\n3.155330\n\n\n2\nDEWP\n4.747345\n\n\n3\nRAIN\n1.020343\n\n\n4\nWSPM\n1.486136\n\n\n\n\n\n\n\nHere, we calculate the Variance Inflation Factor (VIF) for each feature. A VIF value greater than 5 or 10 indicates high multicollinearity, suggesting that the variable could be linearly predicted from the others with a substantial degree of accuracy.\n\n\n\nVisualizing these statistics can help in better understanding and communicating the findings.\n\n# Visualizing VIF values\nplt.bar(vif_data['Feature'], vif_data['VIF'])\nplt.xlabel('Features')\nplt.ylabel('Variance Inflation Factor (VIF)')\nplt.title('Multicollinearity Check - VIF Values')\nplt.show()\n\n\n\n\nThis bar chart provides a clear representation of the VIF values for each feature, helping us identify which variables might be contributing to multicollinearity in the model.\nBy conducting both correlation analysis and a multicollinearity check, we ensure the integrity and effectiveness of our regression models, setting a strong foundation for accurate and insightful analysis of the factors influencing air quality.\n\n\n\nBased on the results of Correlation Analysis and Multicollinearity Check. I decided to predict SO2 with ‘TEMP’, ‘PRES’, ‘DEWP’.\n\n\n\n\nIn this section, we will apply linear regression analysis to predict the concentration of sulfur dioxide (SO2) based on three key environmental factors: ‘TEMP’, ‘PRES’, and ‘DEWP’. Linear regression is a fundamental statistical method used to understand the relationship between a dependent variable and one or more independent variables.\n\n\nLinear regression is a widely used statistical technique for modeling and analyzing the relationship between a scalar response (dependent variable) and one or more explanatory variables (independent variables). The method assumes a linear relationship between the variables. In our context, we will use linear regression to understand how temperature (‘TEMP’), pressure (‘PRES’), and dew point (‘DEWP’) affect the concentration of SO2 in the air.\n\n\n\nNow, let’s conduct a linear regression analysis using Python in a Jupyter Notebook environment.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Filter out rows where any of the feature columns or 'SO2' is NaN\nfiltered_data = sample_data.dropna(subset=['TEMP', 'PRES', 'DEWP', 'SO2'])\n\n# Standardizing the relevant columns of the filtered data\nscaler = StandardScaler()\nscaled_columns = scaler.fit_transform(filtered_data[['TEMP', 'PRES', 'DEWP']])\n\n# Converting scaled data back to a DataFrame\nscaled_df = pd.DataFrame(scaled_columns, columns=['TEMP', 'PRES', 'DEWP'])\n\n# Defining features (X) and target variable (y)\nX = scaled_df\ny = filtered_data['SO2']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Creating and fitting the linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nIn this code, we first select our features and target variable, split the data into training and test sets, create a Linear Regression model, and then fit it to our training data.\n\n\n\nVisualizing the model’s predictions in comparison with the actual values is crucial for assessing its performance. We’ll also plot the best-fit line to better understand the linear relationship.\n\n# Predicting SO2 values for the test set\nlr_y_pred = model.predict(X_test)\n\n# Visualizing the actual vs predicted values and the best-fit line\nplt.scatter(y_test, lr_y_pred, alpha=0.6, color='blue')  # Actual vs Predicted scatter plot\nplt.xlabel('Actual SO2')\nplt.ylabel('Predicted SO2')\nplt.title('Actual vs Predicted SO2 Concentrations')\n\n# Plotting the best-fit line\nplt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, lr_y_pred, 1))(np.unique(y_test)), color='red')\n\nplt.show()\n\n# Zoom in \nplt.xlim(0, 200)\nplt.ylim(0, 40)\n\n# Visualizing the actual vs predicted values and the best-fit line\nplt.scatter(y_test, lr_y_pred, alpha=0.6, color='blue')  # Actual vs Predicted scatter plot\nplt.xlabel('Actual SO2')\nplt.ylabel('Predicted SO2')\nplt.title('Actual vs Predicted SO2 Concentrations')\n\n# Plotting the best-fit line\nplt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, lr_y_pred, 1))(np.unique(y_test)), color='red')\n\nplt.show()\n\n\n\n\n\n\n\nThe scatter plot shows the actual vs. predicted SO2 values, and the red line represents the linear fit, providing a visual indication of how well the model predicts SO2 concentration.\n\n\n\nFinally, we evaluate the performance of our model using common statistical metrics.\n\n# Computing performance metrics\nlr_mse = mean_squared_error(y_test, lr_y_pred)\nlr_r2 = r2_score(y_test, lr_y_pred)\n\nprint(f\"Mean Squared Error: {lr_mse}\")\nprint(f\"R² Score: {lr_r2}\")\n\nMean Squared Error: 411.5799313674985\nR² Score: 0.10938551133078755\n\n\nThe Mean Squared Error (MSE) provides an average of the squares of the errors, essentially quantifying the difference between predicted and actual values. The R² Score measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\nBy following these steps, we can use linear regression to effectively predict environmental factors’ impact on air quality, specifically sulfur dioxide concentrations, and evaluate the accuracy of our predictions.\nSure, I’ll help you generate text for the “Random Forest Regression Analysis” section of your blog. Here’s the content for that section:\n\n\n\n\n\n\nRandom Forest is an ensemble learning method predominantly used for classification and regression tasks. It operates by constructing a multitude of decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Compared to linear regression, Random Forest offers several advantages:\n\nHandling Non-linear Data: It can model complex relationships between features and the target variable, which linear regression may fail to capture.\nReducing Overfitting: By averaging multiple decision trees, it reduces the risk of overfitting to the training data.\nImportance of Features: Random Forest can provide insights into the relative importance of each feature in prediction.\n\n\n\n\nLet’s implement Random Forest regression to predict the concentration of sulfur dioxide (SO2) using ‘TEMP’ (temperature), ‘PRES’ (pressure), and ‘DEWP’ (dew point). We have already preprocessed and scaled our dataset. Now, we’ll apply Random Forest regression:\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Create a Random Forest model\nrf_model = RandomForestRegressor(random_state=0)\n\n# Fit the model to the training data\nrf_model.fit(X_train, y_train)\n\n# Predicting the SO2 values using the test set\nrf_y_pred = rf_model.predict(X_test)\n\n\n\n\n\nFeature Importance Plot: This graph illustrates the relative importance of each feature in predicting the SO2 levels.\n\n\nimport matplotlib.pyplot as plt\n\nfeature_importances = rf_model.feature_importances_\nplt.barh(['TEMP', 'PRES', 'DEWP'], feature_importances)\nplt.xlabel('Feature Importance')\nplt.ylabel('Feature')\nplt.title('Feature Importance in Random Forest Model')\nplt.show()\n\n\n\n\nThe feature importance plot shows ‘TEMP’ with the highest score, indicating it has the most significant impact on predicting SO2 levels, followed by ‘PRES’ and ‘DEWP’. This suggests that temperature changes are potentially a more dominant factor in influencing SO2 concentrations in the atmosphere.\n\nPrediction vs Actual Plot: This plot compares the actual vs. predicted SO2 levels using the Random Forest model.\n\n\nplt.scatter(y_test, rf_y_pred)\nplt.xlabel('Actual SO2 Levels')\nplt.ylabel('Predicted SO2 Levels')\nplt.title('Random Forest: Actual vs Predicted SO2 Levels')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nplt.show()\n\n\n\n\nThe Prediction vs Actual Plot for the Random Forest model reveals a tighter clustering of data points along the line of perfect prediction compared to the Linear Regression model. This clustering indicates a higher accuracy in predictions made by the Random Forest model.\n\n\n\n\nWe compare the performance metrics of Random Forest and Linear Regression:\n\nRandom Forest\n\nMSE: 204.29218141691157\nR²: 0.5579337989410323\n\nLinear Regression\n\nMSE: 411.5799313674985\nR²: 0.10938551133078755\n\n\n\n\nThe Random Forest model shows a significantly lower Mean Squared Error (MSE) and higher R² value compared to Linear Regression. This indicates that the Random Forest model fits the data better and has a greater predictive accuracy. The reduced MSE suggests that the Random Forest model’s predictions are closer to the actual data. The higher R² value indicates that a larger proportion of the variance in the SO2 concentration is being explained by the model.\n\n\n\nThis plot will compare the predictions of both models against the actual SO2 levels. Here, ‘lr_y_pred’ represents the predicted values from the Linear Regression model.\n\nplt.scatter(y_test, lr_y_pred, label='Linear Regression', alpha=0.5, color='b', marker='o')\nplt.scatter(y_test, rf_y_pred, label='Random Forest', alpha=0.5, color='r', marker='+')\nplt.xlabel('Actual SO2 Levels')\nplt.ylabel('Predicted SO2 Levels')\nplt.title('Comparison of Predictions: Linear Regression vs Random Forest')\nplt.legend()\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.show()\n\n\n\n\nThe combined Prediction vs Actual Plot demonstrates a stark contrast between the two models. The Random Forest predictions are more concentrated around the line of perfect fit, while the Linear Regression predictions are more dispersed, indicating more errors in prediction. This visual reaffirms the quantitative metrics, illustrating that Random Forest provides a more accurate model for predicting SO2 levels based on ‘TEMP’, ‘PRES’, and ‘DEWP’.\n\n\n\nAs depicted in the visualizations, there appear to be a few outliers in the graph. Conducting an outlier analysis before proceeding with modeling could potentially enhance the accuracy of our predictions."
  },
  {
    "objectID": "posts/linear/index.html#introduction",
    "href": "posts/linear/index.html#introduction",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Air quality is a critical environmental factor impacting public health, ecosystem sustainability, and the global climate. Pollutants such as particulate matter (PM2.5 and PM10), sulfur dioxide (SO2), nitrogen dioxide (NO2), carbon monoxide (CO), and ozone (O3) can have severe health impacts, including respiratory and cardiovascular diseases. Understanding and predicting the concentrations of these pollutants is essential for creating effective environmental policies and public health interventions.\nIn this blog, we’ll delve into two powerful statistical methods used in predicting air pollutant concentrations: linear regression and Random Forest regression.\n\n\nLinear regression is a fundamental statistical approach used to model the relationship between a dependent variable and one or more independent variables. In the context of air quality, it helps us understand how various environmental factors like temperature, humidity, and wind speed influence pollutant levels. The model assumes a linear relationship between the variables, which can be represented as:\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n + \\epsilon\n\\]\nHere, ( Y ) is the pollutant concentration we want to predict, ( X_1, X_2, …, X_n ) are the environmental factors, ( _0, _1, …, _n ) are the coefficients to be estimated, and ( ) is the error term.\n\n\n\nRandom Forest, on the other hand, is a type of ensemble learning method, particularly useful for non-linear relationships. It operates by constructing multiple decision trees during training and outputting the mean prediction of the individual trees. This method is beneficial for handling complex interactions between variables and can provide more accurate predictions for complex datasets like those in air quality studies.\nThe purpose of this blog is to provide a step-by-step guide on how to use these methods, utilizing a Jupyter Notebook, to predict pollutant concentrations. We’ll start by exploring our dataset, air_data_all.csv, which includes a variety of environmental conditions and temporal factors, and then apply these regression techniques to gain insights into the factors affecting air quality.\nBy the end of this blog, you’ll have a clearer understanding of how to implement these techniques in Python and interpret their results, equipping you with the tools needed for insightful environmental data analysis."
  },
  {
    "objectID": "posts/linear/index.html#understanding-the-dataset",
    "href": "posts/linear/index.html#understanding-the-dataset",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Before delving into regression models, it’s essential to familiarize ourselves with the dataset at hand—air_data_all.csv. This dataset contains hourly air quality measurements and meteorological data from Beijing, spanning from March 1st, 2013, to February 28th, 2017. The dataset is sourced from the Beijing Municipal Environmental Monitoring Center and is matched with meteorological data from the China Meteorological Administration. However, it’s important to note that missing data points are marked as NA. Link to dataset is https://archive.ics.uci.edu/dataset/501/beijing+multi+site+air+quality+data.\n\n\nThe dataset is a valuable resource, encompassing a wide range of environmental conditions and pollutant concentrations. It records temporal information, including the year, month, day, and hour, alongside readings of key air pollutants such as PM2.5, PM10, SO2, NO2, CO, and O3. Additionally, meteorological factors like temperature (TEMP), pressure (PRES), dew point temperature (DEWP), precipitation (RAIN), wind direction (wd), and wind speed (WSPM) are included. This comprehensive data is instrumental for studying air pollution dynamics and its correlation with various environmental and temporal factors.\n\n\n\nEach column in the dataset serves a specific purpose:\n\nTemporal Data (year, month, day, hour): These columns provide insights into pollutant variations across different timescales.\nPollutant Concentrations (PM2.5, PM10, SO2, NO2, CO, O3): These are primary pollutants, crucial for urban air quality analysis.\nMeteorological Data (TEMP, PRES, DEWP, RAIN, wd, WSPM): Weather conditions significantly impact pollutant dispersion and concentration.\nStation: This column identifies the monitoring site, facilitating the study of geographical variations in air quality.\n\n\n\n\nUpon initial examination, it’s evident that the dataset offers a comprehensive foundation for in-depth analysis. However, several challenges may arise:\n\nMissing Data: Handling missing values is imperative to prevent biases in the models.\nHigh Dimensionality: The dataset’s numerous variables may lead to multicollinearity issues, where variables are highly correlated.\nNon-linear Relationships: Linear models may not capture all pollutant-environmental interactions, requiring more complex approaches like Random Forest.\n\nIn the upcoming sections, we will address these challenges while preparing the data for regression analysis. By the end of this process, we will be well-equipped to employ linear and Random Forest regression for accurate pollutant concentration predictions."
  },
  {
    "objectID": "posts/linear/index.html#selecting-target-pollutants",
    "href": "posts/linear/index.html#selecting-target-pollutants",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "In our journey to understand and predict air quality, selecting the right target pollutants is crucial. For this analysis, we will focus on the following pollutants: PM2.5, PM10, SO2, NO2, CO, and O3. Let’s delve into the criteria and rationale behind choosing these specific pollutants.\n\n\nThe selection of target pollutants is based on the following criteria:\n\nHealth Impact: Pollutants known to have significant health effects are prioritized.\nPrevalence and Relevance: Common pollutants in urban and industrial areas are selected due to their higher relevance.\nData Availability: Pollutants with consistent and reliable data within the dataset are chosen to ensure the accuracy of the analysis.\n\n\n\n\nEach selected pollutant has its unique importance in air quality analysis:\n\nPM2.5 and PM10 (Particulate Matter): These are tiny particles in the air that reduce visibility and cause the air to appear hazy when levels are elevated. PM2.5 and PM10 are known for their ability to penetrate deep into the lungs and even into the bloodstream, causing respiratory and cardiovascular issues.\nSO2 (Sulfur Dioxide): A gas typically produced by burning fossil fuels containing sulfur. It’s associated with acid rain and has health implications, especially for individuals with asthma.\nNO2 (Nitrogen Dioxide): Primarily gets into the air from the burning of fuel. NO2 forms from emissions from cars, trucks and buses, power plants, and off-road equipment. It’s linked to various respiratory problems.\nCO (Carbon Monoxide): A colorless, odorless gas that is harmful when inhaled in large amounts. It’s released from vehicles and other combustion sources and can cause harmful health effects by reducing the amount of oxygen that can be transported in the bloodstream.\nO3 (Ozone): At ground level, ozone is a harmful air pollutant and a significant component of smog. It’s not emitted directly into the air but is created by chemical reactions between oxides of nitrogen (NOx) and volatile organic compounds (VOC) in the presence of sunlight.\n\nBy focusing on these pollutants, we can provide a comprehensive analysis of air quality and its health implications. Next, we will perform correlation analysis and multicollinearity checks to understand how these pollutants interact with each other and with different environmental factors."
  },
  {
    "objectID": "posts/linear/index.html#data-cleaning-and-transformation",
    "href": "posts/linear/index.html#data-cleaning-and-transformation",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Before delving into sophisticated regression models, it’s imperative to prepare our dataset, “air_data_all.csv,” for analysis. This stage, known as data cleaning and transformation, involves several key steps to ensure the data’s integrity and usability.\n\n\nThe initial step in data preprocessing is to identify and address any missing (NaN) or inconsistent data. This is crucial as such data can significantly skew our analysis.\n\nimport sys\n!{sys.executable} -m pip install seaborn\n!{sys.executable} -m pip install matplotlib\n!{sys.executable} -m pip install statsmodels\n!{sys.executable} -m pip install scikit-learn\n!{sys.executable} -m pip install pandas\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\n\n# Load the dataset\nsample_data = pd.read_csv('air_data_all.csv')\n\n# Identifying missing or infinite values\nsample_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# Checking for missing values\nmissing_values = sample_data.isnull().sum()\n\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: seaborn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.13.0)\nRequirement already satisfied: matplotlib!=3.6.1,&gt;=3.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (3.8.2)\nRequirement already satisfied: pandas&gt;=1.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (2.1.3)\nRequirement already satisfied: numpy!=1.24.0,&gt;=1.20 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from seaborn) (1.26.2)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (4.45.1)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (3.1.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.2.0)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (23.2)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (2.8.2)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.4.5)\nRequirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (10.1.0)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (6.1.1)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (0.12.1)\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (3.17.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3.post1)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: matplotlib in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (3.8.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.45.1)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (23.2)\nRequirement already satisfied: pillow&gt;=8 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.1.0)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.1.1)\nRequirement already satisfied: numpy&lt;2,&gt;=1.21 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.2)\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib) (3.17.0)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: statsmodels in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (0.14.0)\nRequirement already satisfied: pandas&gt;=1.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (2.1.3)\nRequirement already satisfied: patsy&gt;=0.5.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (0.5.3)\nRequirement already satisfied: scipy!=1.9.2,&gt;=1.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (1.11.4)\nRequirement already satisfied: packaging&gt;=21.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (23.2)\nRequirement already satisfied: numpy&gt;=1.18 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from statsmodels) (1.26.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.0-&gt;statsmodels) (2023.3.post1)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.0-&gt;statsmodels) (2023.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas&gt;=1.0-&gt;statsmodels) (2.8.2)\nRequirement already satisfied: six in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from patsy&gt;=0.5.2-&gt;statsmodels) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: scikit-learn in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (1.3.2)\nRequirement already satisfied: joblib&gt;=1.1.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.3.2)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: scipy&gt;=1.5.0 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: numpy&lt;2.0,&gt;=1.17.3 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.2)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pandas in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (2.1.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: numpy&lt;2,&gt;=1.22.4 in /Users/zimingfang/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.15.0)\nWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\n\n\nIn this code block, we first replace any infinite values with NaNs. Then, we calculate the number of missing values in each column. Depending on the nature and volume of missing data, we can either fill these gaps using statistical methods (like mean, median) or consider removing the rows/columns entirely.\n\n\n\nNormalization (rescaling data to a range, like 0–1) and standardization (shifting the distribution to have a mean of zero and a standard deviation of one) are crucial for models sensitive to the scale of data, such as linear regression.\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardizing the dataset\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(sample_data[['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']])\n\n# Converting scaled data back to a DataFrame for further use\nscaled_df = pd.DataFrame(scaled_data, columns=['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM'])\n\nHere, we use StandardScaler from Scikit-learn to standardize the continuous variables such as temperature and pressure. This process aligns the data onto one scale, removing bias due to different units or scales.\n\n\n\nMany regression models require numerical input, so transforming categorical data into a numerical format is essential.\n\n# Creating dummy variables for categorical data\nwd_dummies = pd.get_dummies(sample_data['wd'])\nsample_data = pd.concat([sample_data, wd_dummies], axis=1)\n\nIn the above snippet, we create dummy variables for the wd column (wind direction), converting it into a format that can be efficiently processed by regression algorithms.\n\n\n\nVisualizations are effective for demonstrating the impact of data transformation. For instance, before and after standardization, we can plot histograms of a variable to observe changes in its distribution.\n\nimport matplotlib.pyplot as plt\n\n# Plotting before and after standardization\nplt.hist(sample_data['TEMP'], bins=30, alpha=0.5, label='Original TEMP')\nplt.hist(scaled_df['TEMP'], bins=30, alpha=0.5, label='Standardized TEMP')\nplt.legend()\nplt.show()\n\n\n\n\nThis histogram allows us to compare the distribution of the temperature data before and after standardization, showcasing the effects of our data transformation steps.\nBy completing these data cleaning and transformation processes, we ensure that our dataset is primed for accurate and effective regression analysis, laying a solid foundation for our subsequent modeling steps."
  },
  {
    "objectID": "posts/linear/index.html#correlation-analysis-and-multicollinearity-check",
    "href": "posts/linear/index.html#correlation-analysis-and-multicollinearity-check",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "After preparing our dataset, the next step in our analysis involves understanding the relationships between variables using correlation analysis and checking for multicollinearity. These steps are critical for ensuring the reliability and interpretability of our regression models.\n\n\nCorrelation analysis helps us understand the strength and direction of the relationship between two variables. In regression analysis, it’s important to identify how independent variables are related to the dependent variable and to each other.\n\n# Removing missing or infinite values from the scaled dataset\nscaled_df.replace([np.inf, -np.inf], np.nan, inplace=True)\nscaled_df.dropna(inplace=True)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Calculating the correlation matrix for key variables\ncorr_matrix = sample_data[['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']].corr()\n\n# Visualizing the correlation matrix using a heatmap\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title(\"Correlation Matrix of Environmental Factors and Pollutants\")\nplt.show()\n\n\n\n\nIn this code, we calculate and visualize the correlation matrix of key pollutants and environmental factors. This heatmap provides a clear visual representation of the relationships, where the color intensity and the value in each cell indicate the strength and direction of the correlation.\n\n\n\nMulticollinearity occurs when two or more independent variables in a regression model are highly correlated. This can lead to unreliable coefficient estimates, making it difficult to determine the effect of each independent variable.\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Preparing data for multicollinearity check\nfeatures = scaled_df[['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']]\n\n# Calculating VIF for each feature\nvif_data = pd.DataFrame()\nvif_data['Feature'] = features.columns\nvif_data['VIF'] = [variance_inflation_factor(features.values, i) for i in range(features.shape[1])]\n\nvif_data\n\n\n\n\n\n\n\n\nFeature\nVIF\n\n\n\n\n0\nTEMP\n5.355958\n\n\n1\nPRES\n3.155330\n\n\n2\nDEWP\n4.747345\n\n\n3\nRAIN\n1.020343\n\n\n4\nWSPM\n1.486136\n\n\n\n\n\n\n\nHere, we calculate the Variance Inflation Factor (VIF) for each feature. A VIF value greater than 5 or 10 indicates high multicollinearity, suggesting that the variable could be linearly predicted from the others with a substantial degree of accuracy.\n\n\n\nVisualizing these statistics can help in better understanding and communicating the findings.\n\n# Visualizing VIF values\nplt.bar(vif_data['Feature'], vif_data['VIF'])\nplt.xlabel('Features')\nplt.ylabel('Variance Inflation Factor (VIF)')\nplt.title('Multicollinearity Check - VIF Values')\nplt.show()\n\n\n\n\nThis bar chart provides a clear representation of the VIF values for each feature, helping us identify which variables might be contributing to multicollinearity in the model.\nBy conducting both correlation analysis and a multicollinearity check, we ensure the integrity and effectiveness of our regression models, setting a strong foundation for accurate and insightful analysis of the factors influencing air quality.\n\n\n\nBased on the results of Correlation Analysis and Multicollinearity Check. I decided to predict SO2 with ‘TEMP’, ‘PRES’, ‘DEWP’."
  },
  {
    "objectID": "posts/linear/index.html#linear-regression-analysis",
    "href": "posts/linear/index.html#linear-regression-analysis",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "In this section, we will apply linear regression analysis to predict the concentration of sulfur dioxide (SO2) based on three key environmental factors: ‘TEMP’, ‘PRES’, and ‘DEWP’. Linear regression is a fundamental statistical method used to understand the relationship between a dependent variable and one or more independent variables.\n\n\nLinear regression is a widely used statistical technique for modeling and analyzing the relationship between a scalar response (dependent variable) and one or more explanatory variables (independent variables). The method assumes a linear relationship between the variables. In our context, we will use linear regression to understand how temperature (‘TEMP’), pressure (‘PRES’), and dew point (‘DEWP’) affect the concentration of SO2 in the air.\n\n\n\nNow, let’s conduct a linear regression analysis using Python in a Jupyter Notebook environment.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Filter out rows where any of the feature columns or 'SO2' is NaN\nfiltered_data = sample_data.dropna(subset=['TEMP', 'PRES', 'DEWP', 'SO2'])\n\n# Standardizing the relevant columns of the filtered data\nscaler = StandardScaler()\nscaled_columns = scaler.fit_transform(filtered_data[['TEMP', 'PRES', 'DEWP']])\n\n# Converting scaled data back to a DataFrame\nscaled_df = pd.DataFrame(scaled_columns, columns=['TEMP', 'PRES', 'DEWP'])\n\n# Defining features (X) and target variable (y)\nX = scaled_df\ny = filtered_data['SO2']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Creating and fitting the linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nIn this code, we first select our features and target variable, split the data into training and test sets, create a Linear Regression model, and then fit it to our training data.\n\n\n\nVisualizing the model’s predictions in comparison with the actual values is crucial for assessing its performance. We’ll also plot the best-fit line to better understand the linear relationship.\n\n# Predicting SO2 values for the test set\nlr_y_pred = model.predict(X_test)\n\n# Visualizing the actual vs predicted values and the best-fit line\nplt.scatter(y_test, lr_y_pred, alpha=0.6, color='blue')  # Actual vs Predicted scatter plot\nplt.xlabel('Actual SO2')\nplt.ylabel('Predicted SO2')\nplt.title('Actual vs Predicted SO2 Concentrations')\n\n# Plotting the best-fit line\nplt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, lr_y_pred, 1))(np.unique(y_test)), color='red')\n\nplt.show()\n\n# Zoom in \nplt.xlim(0, 200)\nplt.ylim(0, 40)\n\n# Visualizing the actual vs predicted values and the best-fit line\nplt.scatter(y_test, lr_y_pred, alpha=0.6, color='blue')  # Actual vs Predicted scatter plot\nplt.xlabel('Actual SO2')\nplt.ylabel('Predicted SO2')\nplt.title('Actual vs Predicted SO2 Concentrations')\n\n# Plotting the best-fit line\nplt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, lr_y_pred, 1))(np.unique(y_test)), color='red')\n\nplt.show()\n\n\n\n\n\n\n\nThe scatter plot shows the actual vs. predicted SO2 values, and the red line represents the linear fit, providing a visual indication of how well the model predicts SO2 concentration.\n\n\n\nFinally, we evaluate the performance of our model using common statistical metrics.\n\n# Computing performance metrics\nlr_mse = mean_squared_error(y_test, lr_y_pred)\nlr_r2 = r2_score(y_test, lr_y_pred)\n\nprint(f\"Mean Squared Error: {lr_mse}\")\nprint(f\"R² Score: {lr_r2}\")\n\nMean Squared Error: 411.5799313674985\nR² Score: 0.10938551133078755\n\n\nThe Mean Squared Error (MSE) provides an average of the squares of the errors, essentially quantifying the difference between predicted and actual values. The R² Score measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\nBy following these steps, we can use linear regression to effectively predict environmental factors’ impact on air quality, specifically sulfur dioxide concentrations, and evaluate the accuracy of our predictions.\nSure, I’ll help you generate text for the “Random Forest Regression Analysis” section of your blog. Here’s the content for that section:"
  },
  {
    "objectID": "posts/linear/index.html#random-forest-regression-analysis",
    "href": "posts/linear/index.html#random-forest-regression-analysis",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Random Forest is an ensemble learning method predominantly used for classification and regression tasks. It operates by constructing a multitude of decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Compared to linear regression, Random Forest offers several advantages:\n\nHandling Non-linear Data: It can model complex relationships between features and the target variable, which linear regression may fail to capture.\nReducing Overfitting: By averaging multiple decision trees, it reduces the risk of overfitting to the training data.\nImportance of Features: Random Forest can provide insights into the relative importance of each feature in prediction.\n\n\n\n\nLet’s implement Random Forest regression to predict the concentration of sulfur dioxide (SO2) using ‘TEMP’ (temperature), ‘PRES’ (pressure), and ‘DEWP’ (dew point). We have already preprocessed and scaled our dataset. Now, we’ll apply Random Forest regression:\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Create a Random Forest model\nrf_model = RandomForestRegressor(random_state=0)\n\n# Fit the model to the training data\nrf_model.fit(X_train, y_train)\n\n# Predicting the SO2 values using the test set\nrf_y_pred = rf_model.predict(X_test)\n\n\n\n\n\nFeature Importance Plot: This graph illustrates the relative importance of each feature in predicting the SO2 levels.\n\n\nimport matplotlib.pyplot as plt\n\nfeature_importances = rf_model.feature_importances_\nplt.barh(['TEMP', 'PRES', 'DEWP'], feature_importances)\nplt.xlabel('Feature Importance')\nplt.ylabel('Feature')\nplt.title('Feature Importance in Random Forest Model')\nplt.show()\n\n\n\n\nThe feature importance plot shows ‘TEMP’ with the highest score, indicating it has the most significant impact on predicting SO2 levels, followed by ‘PRES’ and ‘DEWP’. This suggests that temperature changes are potentially a more dominant factor in influencing SO2 concentrations in the atmosphere.\n\nPrediction vs Actual Plot: This plot compares the actual vs. predicted SO2 levels using the Random Forest model.\n\n\nplt.scatter(y_test, rf_y_pred)\nplt.xlabel('Actual SO2 Levels')\nplt.ylabel('Predicted SO2 Levels')\nplt.title('Random Forest: Actual vs Predicted SO2 Levels')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nplt.show()\n\n\n\n\nThe Prediction vs Actual Plot for the Random Forest model reveals a tighter clustering of data points along the line of perfect prediction compared to the Linear Regression model. This clustering indicates a higher accuracy in predictions made by the Random Forest model."
  },
  {
    "objectID": "posts/linear/index.html#comparative-analysis-and-conclusion",
    "href": "posts/linear/index.html#comparative-analysis-and-conclusion",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "We compare the performance metrics of Random Forest and Linear Regression:\n\nRandom Forest\n\nMSE: 204.29218141691157\nR²: 0.5579337989410323\n\nLinear Regression\n\nMSE: 411.5799313674985\nR²: 0.10938551133078755\n\n\n\n\nThe Random Forest model shows a significantly lower Mean Squared Error (MSE) and higher R² value compared to Linear Regression. This indicates that the Random Forest model fits the data better and has a greater predictive accuracy. The reduced MSE suggests that the Random Forest model’s predictions are closer to the actual data. The higher R² value indicates that a larger proportion of the variance in the SO2 concentration is being explained by the model.\n\n\n\nThis plot will compare the predictions of both models against the actual SO2 levels. Here, ‘lr_y_pred’ represents the predicted values from the Linear Regression model.\n\nplt.scatter(y_test, lr_y_pred, label='Linear Regression', alpha=0.5, color='b', marker='o')\nplt.scatter(y_test, rf_y_pred, label='Random Forest', alpha=0.5, color='r', marker='+')\nplt.xlabel('Actual SO2 Levels')\nplt.ylabel('Predicted SO2 Levels')\nplt.title('Comparison of Predictions: Linear Regression vs Random Forest')\nplt.legend()\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.show()\n\n\n\n\nThe combined Prediction vs Actual Plot demonstrates a stark contrast between the two models. The Random Forest predictions are more concentrated around the line of perfect fit, while the Linear Regression predictions are more dispersed, indicating more errors in prediction. This visual reaffirms the quantitative metrics, illustrating that Random Forest provides a more accurate model for predicting SO2 levels based on ‘TEMP’, ‘PRES’, and ‘DEWP’.\n\n\n\nAs depicted in the visualizations, there appear to be a few outliers in the graph. Conducting an outlier analysis before proceeding with modeling could potentially enhance the accuracy of our predictions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "1. Probability Theory and Random Variables\n\n\n\n\n\n\n\ncode\n\n\nprobability\n\n\nemail traffic\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nJoanna Fang\n\n\n\n\n\n\n  \n\n\n\n\n3. Linear and Nonlinear Regression\n\n\n\n\n\n\n\nml\n\n\ncode\n\n\nlinear regression\n\n\nnonlinear regression\n\n\npollution\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nJoanna Fang\n\n\n\n\n\n\n  \n\n\n\n\n2. Clustering\n\n\n\n\n\n\n\nml\n\n\ncode\n\n\nclustering\n\n\ndriving\n\n\nkaggle\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nJoanna Fang\n\n\n\n\n\n\n  \n\n\n\n\n4. Classification\n\n\n\n\n\n\n\nml\n\n\ncode\n\n\nclassification\n\n\ndriving\n\n\nkaggle\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nJoanna Fang\n\n\n\n\n\n\n  \n\n\n\n\n5. Anomaly/Outlier Detection\n\n\n\n\n\n\n\nml\n\n\ncode\n\n\nanomaly detection\n\n\noutlier detection\n\n\npollution\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nJoanna Fang\n\n\n\n\n\n\nNo matching items"
  }
]