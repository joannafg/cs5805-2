<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Joanna Fang">
<meta name="dcterms.date" content="2023-11-30">

<title>Ziming’s Journey in Machine Learning - Classification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Ziming’s Journey in Machine Learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/joannafg/cs5805" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/joanna-fang-6122ba182/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Classification</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i></button></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">ml</div>
                <div class="quarto-category">code</div>
                <div class="quarto-category">classification</div>
                <div class="quarto-category">driving</div>
                <div class="quarto-category">kaggle</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Joanna Fang </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 30, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#classification-predictive-analytics-for-driving-behavior-classification" id="toc-classification-predictive-analytics-for-driving-behavior-classification" class="nav-link active" data-scroll-target="#classification-predictive-analytics-for-driving-behavior-classification">Classification: Predictive Analytics for Driving Behavior Classification</a>
  <ul class="collapse">
  <li><a href="#project-introduction" id="toc-project-introduction" class="nav-link" data-scroll-target="#project-introduction">Project Introduction</a>
  <ul class="collapse">
  <li><a href="#objective" id="toc-objective" class="nav-link" data-scroll-target="#objective">Objective</a></li>
  <li><a href="#importance-and-applications" id="toc-importance-and-applications" class="nav-link" data-scroll-target="#importance-and-applications">Importance and Applications</a></li>
  </ul></li>
  <li><a href="#data-collection-and-description" id="toc-data-collection-and-description" class="nav-link" data-scroll-target="#data-collection-and-description">Data Collection and Description</a>
  <ul class="collapse">
  <li><a href="#source-of-the-data" id="toc-source-of-the-data" class="nav-link" data-scroll-target="#source-of-the-data">Source of the Data</a></li>
  <li><a href="#dataset-description" id="toc-dataset-description" class="nav-link" data-scroll-target="#dataset-description">Dataset Description</a></li>
  </ul></li>
  <li><a href="#data-preprocessing" id="toc-data-preprocessing" class="nav-link" data-scroll-target="#data-preprocessing">Data Preprocessing</a>
  <ul class="collapse">
  <li><a href="#handling-missing-values" id="toc-handling-missing-values" class="nav-link" data-scroll-target="#handling-missing-values">Handling Missing Values</a></li>
  <li><a href="#normalizing-or-scaling-the-data" id="toc-normalizing-or-scaling-the-data" class="nav-link" data-scroll-target="#normalizing-or-scaling-the-data">Normalizing or Scaling the Data</a></li>
  <li><a href="#feature-engineering" id="toc-feature-engineering" class="nav-link" data-scroll-target="#feature-engineering">Feature Engineering</a></li>
  </ul></li>
  <li><a href="#exploratory-data-analysis-eda" id="toc-exploratory-data-analysis-eda" class="nav-link" data-scroll-target="#exploratory-data-analysis-eda">Exploratory Data Analysis (EDA)</a>
  <ul class="collapse">
  <li><a href="#statistical-summary-of-the-dataset" id="toc-statistical-summary-of-the-dataset" class="nav-link" data-scroll-target="#statistical-summary-of-the-dataset">Statistical Summary of the Dataset</a></li>
  <li><a href="#visualization-of-data-distribution" id="toc-visualization-of-data-distribution" class="nav-link" data-scroll-target="#visualization-of-data-distribution">Visualization of Data Distribution</a></li>
  </ul></li>
  <li><a href="#data-preparation-for-modeling" id="toc-data-preparation-for-modeling" class="nav-link" data-scroll-target="#data-preparation-for-modeling">Data Preparation for Modeling</a>
  <ul class="collapse">
  <li><a href="#splitting-data-into-training-and-testing-sets" id="toc-splitting-data-into-training-and-testing-sets" class="nav-link" data-scroll-target="#splitting-data-into-training-and-testing-sets">Splitting Data into Training and Testing Sets</a></li>
  <li><a href="#handling-class-imbalance" id="toc-handling-class-imbalance" class="nav-link" data-scroll-target="#handling-class-imbalance">Handling Class Imbalance</a></li>
  </ul></li>
  <li><a href="#model-selection" id="toc-model-selection" class="nav-link" data-scroll-target="#model-selection">Model Selection</a>
  <ul class="collapse">
  <li><a href="#overview-of-potential-machine-learning-models-for-classification" id="toc-overview-of-potential-machine-learning-models-for-classification" class="nav-link" data-scroll-target="#overview-of-potential-machine-learning-models-for-classification">Overview of Potential Machine Learning Models for Classification</a></li>
  <li><a href="#criteria-for-model-selection" id="toc-criteria-for-model-selection" class="nav-link" data-scroll-target="#criteria-for-model-selection">Criteria for Model Selection</a></li>
  </ul></li>
  <li><a href="#model-training" id="toc-model-training" class="nav-link" data-scroll-target="#model-training">Model Training</a>
  <ul class="collapse">
  <li><a href="#training-different-models" id="toc-training-different-models" class="nav-link" data-scroll-target="#training-different-models">Training Different Models</a></li>
  <li><a href="#hyperparameter-tuning" id="toc-hyperparameter-tuning" class="nav-link" data-scroll-target="#hyperparameter-tuning">Hyperparameter Tuning</a></li>
  </ul></li>
  <li><a href="#model-evaluation" id="toc-model-evaluation" class="nav-link" data-scroll-target="#model-evaluation">Model Evaluation</a>
  <ul class="collapse">
  <li><a href="#evaluating-model-performance" id="toc-evaluating-model-performance" class="nav-link" data-scroll-target="#evaluating-model-performance">Evaluating Model Performance</a></li>
  <li><a href="#confusion-matrix-for-each-model" id="toc-confusion-matrix-for-each-model" class="nav-link" data-scroll-target="#confusion-matrix-for-each-model">Confusion Matrix for Each Model</a></li>
  </ul></li>
  <li><a href="#model-interpretation" id="toc-model-interpretation" class="nav-link" data-scroll-target="#model-interpretation">Model Interpretation</a>
  <ul class="collapse">
  <li><a href="#interpretation-of-model-results" id="toc-interpretation-of-model-results" class="nav-link" data-scroll-target="#interpretation-of-model-results">Interpretation of Model Results</a></li>
  <li><a href="#feature-importance-analysis" id="toc-feature-importance-analysis" class="nav-link" data-scroll-target="#feature-importance-analysis">Feature Importance Analysis</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a>
  <ul class="collapse">
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations">Limitations</a></li>
  <li><a href="#potential-improvements" id="toc-potential-improvements" class="nav-link" data-scroll-target="#potential-improvements">Potential Improvements</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/joannafg/cs5805/edit/main/posts/classification2/index.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/joannafg/cs5805/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="classification-predictive-analytics-for-driving-behavior-classification" class="level1">
<h1>Classification: Predictive Analytics for Driving Behavior Classification</h1>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="thumbnail.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
<section id="project-introduction" class="level2">
<h2 class="anchored" data-anchor-id="project-introduction">Project Introduction</h2>
<p>Welcome to our exploration into the world of machine learning and its application in predicting driving behaviors. In this project, we dive into the realm of vehicular safety, aiming to leverage sensor data to classify driving patterns into three categories: SLOW, NORMAL, and AGGRESSIVE. This endeavor is not just a technical challenge but a crucial step towards enhancing road safety and reducing traffic accidents.</p>
<section id="objective" class="level3">
<h3 class="anchored" data-anchor-id="objective">Objective</h3>
<p>The primary goal of this project is to accurately predict driving behaviors using data from commonly available sensors in smartphones. By analyzing accelerometer and gyroscope data, we aim to classify driving styles into SLOW, NORMAL, or AGGRESSIVE, contributing significantly to the prevention of road mishaps.</p>
</section>
<section id="importance-and-applications" class="level3">
<h3 class="anchored" data-anchor-id="importance-and-applications">Importance and Applications</h3>
<p>The importance of this project is underscored by the alarming statistics from the AAA Foundation for Traffic Safety, highlighting that over half of fatal crashes involve aggressive driving actions. Through this project, we offer a scalable and readily deployable solution to monitor and predict dangerous driving behaviors, potentially saving lives and making our roads safer. Applications of this model extend to insurance companies for risk assessment, ride-sharing services for driver monitoring, and personal safety apps to alert drivers of their driving patterns.</p>
</section>
</section>
<section id="data-collection-and-description" class="level2">
<h2 class="anchored" data-anchor-id="data-collection-and-description">Data Collection and Description</h2>
<section id="source-of-the-data" class="level3">
<h3 class="anchored" data-anchor-id="source-of-the-data">Source of the Data</h3>
<p>The dataset for this project is derived from a real-world scenario, specifically designed to capture driving behaviors. Utilizing a data collector application on Android devices, we have gathered sensor readings directly relevant to driving dynamics.</p>
</section>
<section id="dataset-description" class="level3">
<h3 class="anchored" data-anchor-id="dataset-description">Dataset Description</h3>
<p>The dataset is a rich collection of sensor data recorded from a Samsung Galaxy S21, chosen for its advanced sensor capabilities. Here’s a breakdown of the dataset features:</p>
<section id="acceleration-data" class="level4">
<h4 class="anchored" data-anchor-id="acceleration-data">Acceleration Data</h4>
<ul>
<li>Axes: X, Y, Z</li>
<li>Unit: Meters per second squared (m/s²)</li>
<li>Note: Gravitational acceleration has been filtered out to focus on the acceleration caused by driving actions.</li>
</ul>
</section>
<section id="rotation-data" class="level4">
<h4 class="anchored" data-anchor-id="rotation-data">Rotation Data</h4>
<ul>
<li>Axes: X, Y, Z</li>
<li>Unit: Degrees per second (°/s)</li>
<li>Purpose: Captures the angular changes during driving, indicative of turns and maneuvers.</li>
</ul>
</section>
<section id="classification-label" class="level4">
<h4 class="anchored" data-anchor-id="classification-label">Classification Label</h4>
<ul>
<li>Categories: SLOW, NORMAL, AGGRESSIVE</li>
<li>Basis: The driving behavior classification based on the sensor data patterns.</li>
</ul>
</section>
<section id="additional-information" class="level4">
<h4 class="anchored" data-anchor-id="additional-information">Additional Information</h4>
<ul>
<li>Sampling Rate: 2 samples per second, ensuring a fine-grained capture of driving dynamics.</li>
<li>Timestamp: Included for each sample, allowing for temporal analysis of driving patterns.</li>
</ul>
<p>In the following sections, we will delve into the preprocessing, exploratory analysis, and modeling of this dataset to build a robust classifier for driving behaviors. Stay tuned as we unravel the insights hidden within this data and develop a machine learning model with the potential to make a real-world impact.</p>
</section>
</section>
</section>
<section id="data-preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="data-preprocessing">Data Preprocessing</h2>
<p>In the realm of machine learning, data preprocessing is a critical step in preparing raw data for modeling. Our datasets, <code>motion_data_1.csv</code> (test data) and <code>motion_data_2.csv</code> (train data), contain acceleration and rotation measurements alongside driving behavior classifications. Let’s walk through the preprocessing steps:</p>
<section id="handling-missing-values" class="level3">
<h3 class="anchored" data-anchor-id="handling-missing-values">Handling Missing Values</h3>
<p>First, we’ll check for missing values in both datasets. Missing data can significantly impact the performance of a machine learning model. If missing values are found, strategies such as imputation or removal of the affected rows can be considered.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the datasets</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> pd.read_csv(<span class="st">'motion_data_1.csv'</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> pd.read_csv(<span class="st">'motion_data_2.csv'</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Checking for missing values in both datasets</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>missing_values_test <span class="op">=</span> test_data.isnull().<span class="bu">sum</span>()</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>missing_values_train <span class="op">=</span> train_data.isnull().<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="normalizing-or-scaling-the-data" class="level3">
<h3 class="anchored" data-anchor-id="normalizing-or-scaling-the-data">Normalizing or Scaling the Data</h3>
<p>Normalization or scaling is crucial when dealing with sensor data. It ensures that each feature contributes proportionately to the final prediction. Given the different scales of acceleration (in m/s²) and rotation (in °/s), applying a scaling method like Min-Max scaling or Standardization is important.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardizing the data</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>train_data_scaled <span class="op">=</span> scaler.fit_transform(train_data.iloc[:, :<span class="op">-</span><span class="dv">2</span>]) <span class="co"># Excluding 'Class' and 'Timestamp' columns</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>test_data_scaled <span class="op">=</span> scaler.transform(test_data.iloc[:, :<span class="op">-</span><span class="dv">2</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="feature-engineering" class="level3">
<h3 class="anchored" data-anchor-id="feature-engineering">Feature Engineering</h3>
<p>Feature engineering might involve creating new features or modifying existing ones to improve model performance. In our case, we might consider deriving features like the total magnitude of acceleration or rotation.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Adding a feature: Total magnitude of acceleration</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>train_data[<span class="st">'TotalAcc'</span>] <span class="op">=</span> np.sqrt(train_data[<span class="st">'AccX'</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> train_data[<span class="st">'AccY'</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> train_data[<span class="st">'AccZ'</span>]<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>test_data[<span class="st">'TotalAcc'</span>] <span class="op">=</span> np.sqrt(test_data[<span class="st">'AccX'</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> test_data[<span class="st">'AccY'</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> test_data[<span class="st">'AccZ'</span>]<span class="op">**</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="exploratory-data-analysis-eda" class="level2">
<h2 class="anchored" data-anchor-id="exploratory-data-analysis-eda">Exploratory Data Analysis (EDA)</h2>
<section id="statistical-summary-of-the-dataset" class="level3">
<h3 class="anchored" data-anchor-id="statistical-summary-of-the-dataset">Statistical Summary of the Dataset</h3>
<p>Understanding the basic statistics of the dataset is essential. This includes measures like mean, median, standard deviation, etc., providing insights into the data distribution.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Descriptive statistics of the training data</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>train_data_description <span class="op">=</span> train_data.describe()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="visualization-of-data-distribution" class="level3">
<h3 class="anchored" data-anchor-id="visualization-of-data-distribution">Visualization of Data Distribution</h3>
<section id="histograms-or-box-plots-for-acceleration-and-rotation-data" class="level4">
<h4 class="anchored" data-anchor-id="histograms-or-box-plots-for-acceleration-and-rotation-data">Histograms or Box Plots for Acceleration and Rotation Data</h4>
<p>Histograms and box plots are effective for visualizing the distribution of sensor data and identifying outliers or skewness in the data.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Setting up the figure for multiple subplots</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">3</span>, ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">15</span>))</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'Histograms of Acceleration and Rotation Data'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting histograms for each sensor data column</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>sns.histplot(train_data[<span class="st">'AccX'</span>], kde<span class="op">=</span><span class="va">True</span>, ax<span class="op">=</span>axes[<span class="dv">0</span>, <span class="dv">0</span>], color<span class="op">=</span><span class="st">'skyblue'</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_title(<span class="st">'Acceleration in X-axis (AccX)'</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>sns.histplot(train_data[<span class="st">'AccY'</span>], kde<span class="op">=</span><span class="va">True</span>, ax<span class="op">=</span>axes[<span class="dv">0</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'olive'</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].set_title(<span class="st">'Acceleration in Y-axis (AccY)'</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>sns.histplot(train_data[<span class="st">'AccZ'</span>], kde<span class="op">=</span><span class="va">True</span>, ax<span class="op">=</span>axes[<span class="dv">1</span>, <span class="dv">0</span>], color<span class="op">=</span><span class="st">'gold'</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_title(<span class="st">'Acceleration in Z-axis (AccZ)'</span>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>sns.histplot(train_data[<span class="st">'GyroX'</span>], kde<span class="op">=</span><span class="va">True</span>, ax<span class="op">=</span>axes[<span class="dv">1</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'teal'</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_title(<span class="st">'Rotation in X-axis (GyroX)'</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>sns.histplot(train_data[<span class="st">'GyroY'</span>], kde<span class="op">=</span><span class="va">True</span>, ax<span class="op">=</span>axes[<span class="dv">2</span>, <span class="dv">0</span>], color<span class="op">=</span><span class="st">'salmon'</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>, <span class="dv">0</span>].set_title(<span class="st">'Rotation in Y-axis (GyroY)'</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>sns.histplot(train_data[<span class="st">'GyroZ'</span>], kde<span class="op">=</span><span class="va">True</span>, ax<span class="op">=</span>axes[<span class="dv">2</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'violet'</span>)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>, <span class="dv">1</span>].set_title(<span class="st">'Rotation in Z-axis (GyroZ)'</span>)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>plt.tight_layout(rect<span class="op">=</span>[<span class="dv">0</span>, <span class="fl">0.03</span>, <span class="dv">1</span>, <span class="fl">0.95</span>])</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-6-output-1.png" width="1430" height="1373"></p>
</div>
</div>
<ol type="1">
<li><p><strong>Acceleration (AccX, AccY, AccZ)</strong>: The distributions for acceleration on all three axes appear to be roughly bell-shaped, indicating that most of the readings are clustered around the mean, with fewer readings at the extreme ends. This suggests normal driving conditions with occasional variances that could indicate moments of acceleration or deceleration.</p></li>
<li><p><strong>Rotation (GyroX, GyroY, GyroZ)</strong>: The rotation data histograms show a similar bell-shaped distribution, especially for the X and Y axes, indicating consistent turning behavior with some outliers potentially representing more aggressive turns or corrections. The GyroZ histogram is notably narrower, which might suggest that rotation around the Z-axis (often corresponding to yaw movements) is less variable during normal driving conditions.</p></li>
</ol>
</section>
<section id="correlation-heatmaps" class="level4">
<h4 class="anchored" data-anchor-id="correlation-heatmaps">Correlation Heatmaps</h4>
<p>A correlation heatmap helps in understanding the relationships between different sensor readings. It’s crucial for identifying features that are highly correlated and might need to be addressed during feature selection.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For the correlation heatmap, we need to exclude non-numeric columns (Class and Timestamp)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>train_data_numeric <span class="op">=</span> train_data.select_dtypes(include<span class="op">=</span>[<span class="st">'float64'</span>, <span class="st">'int64'</span>])</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Generating the correlation heatmap</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>sns.heatmap(train_data_numeric.corr(), annot<span class="op">=</span><span class="va">True</span>, cmap<span class="op">=</span><span class="st">'coolwarm'</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Correlation Heatmap of Numeric Features'</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-7-output-1.png" width="799" height="505"></p>
</div>
</div>
<p>In this heatmap:</p>
<ul>
<li>Values close to 1 or -1 indicate a strong positive or negative correlation, respectively.</li>
<li>Values close to 0 suggest no linear correlation between the variables.</li>
</ul>
<p>It appears that:</p>
<ul>
<li>There are no exceptionally strong correlations between the different axes of acceleration and rotation, which is desirable in a dataset used for behavior prediction as it indicates that the sensor readings provide unique information.</li>
<li>The strongest negative correlation is observed between GyroZ and AccX, which might suggest that certain types of aggressive driving behaviors cause inverse changes in these two measurements.</li>
</ul>
<p>The absence of very high correlations means that there may not be redundant features in the dataset, which is good for a machine learning model that relies on diverse data points to make predictions. However, the subtle correlations that do exist can still provide valuable insights when developing features and training models.</p>
<p>These visualizations and their interpretations are key in understanding the underlying patterns in the driving behavior dataset, which will inform the feature selection and modeling phases of the machine learning project.</p>
<p>In the next sections, we will delve into model selection, training, and evaluation, using the insights and data preparations we’ve just discussed. Stay tuned!</p>
</section>
</section>
</section>
<section id="data-preparation-for-modeling" class="level2">
<h2 class="anchored" data-anchor-id="data-preparation-for-modeling">Data Preparation for Modeling</h2>
<p>Preparing the data for modeling is a crucial step in the machine learning pipeline. It ensures that the data fed into the model is clean, representative, and well-formatted.</p>
<section id="splitting-data-into-training-and-testing-sets" class="level3">
<h3 class="anchored" data-anchor-id="splitting-data-into-training-and-testing-sets">Splitting Data into Training and Testing Sets</h3>
<p>We have two datasets: <code>train_data</code> and <code>test_data</code>, pre-split for our convenience. Typically, we would use a function like <code>train_test_split</code> from <code>scikit-learn</code> to divide our dataset into a training set and a test set, ensuring that both sets are representative of the overall distribution. However, in this scenario, that step is already accounted for.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Since the data is pre-split, this step is not required for our current workflow.</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Normally, we would do something like this:</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># from sklearn.model_selection import train_test_split</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="handling-class-imbalance" class="level3">
<h3 class="anchored" data-anchor-id="handling-class-imbalance">Handling Class Imbalance</h3>
<p>Class imbalance can significantly skew the performance of a classifier towards the majority class. It’s important to check the balance of classes and apply techniques such as resampling if necessary.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Checking the balance of the classes</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>class_counts <span class="op">=</span> train_data[<span class="st">'Class'</span>].value_counts()</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># If imbalance is found, we might consider resampling strategies like:</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># - Upsampling minority classes</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># - Downsampling majority classes</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># - Using SMOTE (Synthetic Minority Over-sampling Technique)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="model-selection" class="level2">
<h2 class="anchored" data-anchor-id="model-selection">Model Selection</h2>
<p>Selecting the right model is about finding the balance between prediction accuracy, computational efficiency, and the ease of interpretation.</p>
<section id="overview-of-potential-machine-learning-models-for-classification" class="level3">
<h3 class="anchored" data-anchor-id="overview-of-potential-machine-learning-models-for-classification">Overview of Potential Machine Learning Models for Classification</h3>
<p>For our classification task, we have several models at our disposal:</p>
<ul>
<li><strong>Decision Tree</strong>: A good baseline that is easy to interpret.</li>
<li><strong>Random Forest</strong>: An ensemble method that can improve on the performance of a single decision tree.</li>
<li><strong>Support Vector Machine (SVM)</strong>: Effective in high-dimensional spaces.</li>
<li><strong>Neural Networks</strong>: Potentially high performance but may require more data and compute resources.</li>
</ul>
</section>
<section id="criteria-for-model-selection" class="level3">
<h3 class="anchored" data-anchor-id="criteria-for-model-selection">Criteria for Model Selection</h3>
<p>When selecting the model, we consider several criteria:</p>
<ul>
<li><strong>Accuracy</strong>: How often the model makes the correct prediction.</li>
<li><strong>Speed</strong>: How quickly the model can be trained and used for prediction.</li>
<li><strong>Interpretability</strong>: The ease with which we can understand the model’s predictions.</li>
</ul>
</section>
</section>
<section id="model-training" class="level2">
<h2 class="anchored" data-anchor-id="model-training">Model Training</h2>
<p>Training our models is like teaching them to understand patterns within the data that distinguish between SLOW, NORMAL, and AGGRESSIVE driving behaviors. We will employ four different machine learning models to find the best predictor.</p>
<section id="training-different-models" class="level3">
<h3 class="anchored" data-anchor-id="training-different-models">Training Different Models</h3>
<p>We start by training different types of models. Each has its own strengths and might capture different aspects of the driving behavior.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neural_network <span class="im">import</span> MLPClassifier</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Initializing models with default parameters</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>decision_tree <span class="op">=</span> DecisionTreeClassifier()</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>random_forest <span class="op">=</span> RandomForestClassifier()</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>svm <span class="op">=</span> SVC()</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>neural_network <span class="op">=</span> MLPClassifier()</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>decision_tree.fit(train_data.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>), train_data[<span class="st">'Class'</span>])</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>random_forest.fit(train_data.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>), train_data[<span class="st">'Class'</span>])</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>svm.fit(train_data.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>), train_data[<span class="st">'Class'</span>]) </span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>neural_network.fit(train_data.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>), train_data[<span class="st">'Class'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>MLPClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">MLPClassifier</label><div class="sk-toggleable__content"><pre>MLPClassifier()</pre></div></div></div></div></div>
</div>
</div>
<p>The <code>fit</code> method allows the models to learn from the training data. It’s during this process that they identify which features are most important for predicting driving behavior.</p>
</section>
<section id="hyperparameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter-tuning">Hyperparameter Tuning</h3>
<p>To optimize our models, we tweak their settings, known as hyperparameters. This process is akin to fine-tuning an instrument to ensure it plays the perfect note.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> RandomizedSearchCV</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Reduce the number of iterations and cross-validation folds</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>n_iter_search <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>cv_folds <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Simplify the models - example for Random Forest</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>rf_param_grid <span class="op">=</span> {</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_estimators'</span>: [<span class="dv">50</span>, <span class="dv">100</span>],  <span class="co"># reduced number of trees</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="dv">10</span>, <span class="va">None</span>],  <span class="co"># fewer options</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_split'</span>: [<span class="dv">2</span>, <span class="dv">5</span>]</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the RandomizedSearchCV object for Random Forest with fewer iterations and folds</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>random_search_rf <span class="op">=</span> RandomizedSearchCV(</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    RandomForestClassifier(), </span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    param_distributions<span class="op">=</span>rf_param_grid, </span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span>n_iter_search, </span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>cv_folds, </span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>, </span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>random_search_rf.fit(train_data.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>), train_data[<span class="st">'Class'</span>])</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>best_params_rf <span class="op">=</span> random_search_rf.best_params_</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="co"># RandomizedSearchCV for Decision Tree</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>dt_param_grid <span class="op">=</span> {</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>],</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_leaf'</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>]</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>random_search_dt <span class="op">=</span> RandomizedSearchCV(</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>    DecisionTreeClassifier(), </span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>    param_distributions<span class="op">=</span>dt_param_grid, </span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span>n_iter_search, </span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>cv_folds, </span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>random_search_dt.fit(train_data.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>), train_data[<span class="st">'Class'</span>])</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>best_params_dt <span class="op">=</span> random_search_dt.best_params_</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a><span class="co"># RandomizedSearchCV for SVM</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Using an extremely small subset of the data</span></span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>subset_size <span class="op">=</span> <span class="dv">50</span>  <span class="co"># Very small subset for extremely quick execution</span></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>train_subset <span class="op">=</span> train_data.sample(n<span class="op">=</span>subset_size, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Simplified RandomizedSearchCV for SVM</span></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>random_search_svm <span class="op">=</span> RandomizedSearchCV(</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>    SVC(), </span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>    param_distributions<span class="op">=</span>{<span class="st">'C'</span>: [<span class="dv">1</span>], <span class="st">'kernel'</span>: [<span class="st">'linear'</span>]},  <span class="co"># Minimal hyperparameter space</span></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span><span class="dv">1</span>,  <span class="co"># Only one iteration</span></span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">2</span>,  <span class="co"># Reduced to 2-fold cross-validation</span></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=</span><span class="dv">1</span>,  <span class="co"># Limit the number of jobs to 1 for a quick run</span></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span>  <span class="co"># No verbosity</span></span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>random_search_svm.fit(train_subset.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>), train_subset[<span class="st">'Class'</span>])</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>best_params_svm <span class="op">=</span> random_search_svm.best_params_</span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a><span class="co"># RandomizedSearchCV for Neural Network</span></span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Using a very small subset of the data</span></span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>subset_size <span class="op">=</span> <span class="dv">50</span>  <span class="co"># Extremely small subset for quick execution</span></span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>train_subset <span class="op">=</span> train_data.sample(n<span class="op">=</span>subset_size, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Simplified parameter grid for Neural Network</span></span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>nn_param_grid <span class="op">=</span> {</span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a>    <span class="st">'hidden_layer_sizes'</span>: [(<span class="dv">50</span>,)],  <span class="co"># Smaller network</span></span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>    <span class="st">'activation'</span>: [<span class="st">'relu'</span>],  <span class="co"># One activation function</span></span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>    <span class="st">'solver'</span>: [<span class="st">'adam'</span>],  <span class="co"># One solver</span></span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_iter'</span>: [<span class="dv">100</span>]  <span class="co"># Fewer iterations</span></span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a><span class="co"># RandomizedSearchCV for Neural Network with simplified settings</span></span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a>random_search_nn <span class="op">=</span> RandomizedSearchCV(</span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>    MLPClassifier(), </span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a>    nn_param_grid, </span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span><span class="dv">2</span>,  <span class="co"># Fewer iterations</span></span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">2</span>,  <span class="co"># Reduced cross-validation</span></span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=</span><span class="dv">1</span>,  <span class="co"># Using single job for faster execution in limited environments</span></span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a>random_search_nn.fit(train_subset.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>), train_subset[<span class="st">'Class'</span>])</span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a>best_params_nn <span class="op">=</span> random_search_nn.best_params_</span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the best parameters for all models</span></span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best parameters for Random Forest:"</span>, best_params_rf)</span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best parameters for Decision Tree:"</span>, best_params_dt)</span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best parameters for SVM:"</span>, best_params_svm)</span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best parameters for Neural Network:"</span>, best_params_nn)</span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a><span class="co"># Now we can print or use the best_params_rf, best_params_svm, and best_params_nn for the best-found parameters</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best parameters for Random Forest: {'n_estimators': 50, 'min_samples_split': 2, 'max_depth': 10}
Best parameters for Decision Tree: {'min_samples_leaf': 2, 'max_depth': 30}
Best parameters for SVM: {'kernel': 'linear', 'C': 1}
Best parameters for Neural Network: {'solver': 'adam', 'max_iter': 100, 'hidden_layer_sizes': (50,), 'activation': 'relu'}</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 1 is smaller than n_iter=2. Running 1 iterations. For exhaustive searches, use GridSearchCV.
  warnings.warn(</code></pre>
</div>
</div>
<p>Using <code>GridSearchCV</code>, we systematically work through multiple combinations of parameter tunes, cross-validating as we go to determine which tune gives the best performance.</p>
<p>By the end of these steps, we will have trained four different models and optimized their hyperparameters for best performance. This careful tuning is what sets the stage for the final step: evaluating these models to select the ultimate candidate for our driving behavior prediction task.</p>
</section>
</section>
<section id="model-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="model-evaluation">Model Evaluation</h2>
<p>After training our models and tuning their hyperparameters, the next critical step is model evaluation. This step helps us understand how well our models perform and guides us in selecting the best one for our application.</p>
<section id="evaluating-model-performance" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-model-performance">Evaluating Model Performance</h3>
<p>We’ll evaluate each model using several key metrics: accuracy, precision, recall, and the F1-score. These metrics give us a well-rounded view of our models’ performance.</p>
<ul>
<li><strong>Accuracy</strong>: The proportion of true results (both true positives and true negatives) among the total number of cases examined.</li>
<li><strong>Precision</strong>: The ratio of correctly predicted positive observations to the total predicted positives.</li>
<li><strong>Recall (Sensitivity)</strong>: The ratio of correctly predicted positive observations to all observations in the actual class.</li>
<li><strong>F1-Score</strong>: The weighted average of Precision and Recall, best if there’s an uneven class distribution.</li>
</ul>
<p>Let’s calculate and print these metrics for each model:</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, precision_score, recall_score, f1_score, confusion_matrix</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Models we've trained</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> {</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Decision Tree"</span>: decision_tree,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Random Forest"</span>: random_forest,</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"SVM"</span>: svm,</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Neural Network"</span>: neural_network</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate each model</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, model <span class="kw">in</span> models.items():</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model.predict(test_data.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> accuracy_score(test_data[<span class="st">'Class'</span>], predictions)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    precision <span class="op">=</span> precision_score(test_data[<span class="st">'Class'</span>], predictions, average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    recall <span class="op">=</span> recall_score(test_data[<span class="st">'Class'</span>], predictions, average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    f1 <span class="op">=</span> f1_score(test_data[<span class="st">'Class'</span>], predictions, average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Precision: </span><span class="sc">{</span>precision<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Recall: </span><span class="sc">{</span>recall<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  F1-Score: </span><span class="sc">{</span>f1<span class="sc">:.4f}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/Users/zimingfang/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Decision Tree:
  Accuracy: 0.3233
  Precision: 0.1045
  Recall: 0.3233
  F1-Score: 0.1580

Random Forest:
  Accuracy: 0.3233
  Precision: 0.1045
  Recall: 0.3233
  F1-Score: 0.1580

SVM:
  Accuracy: 0.4128
  Precision: 0.1704
  Recall: 0.4128
  F1-Score: 0.2412

Neural Network:
  Accuracy: 0.3233
  Precision: 0.1045
  Recall: 0.3233
  F1-Score: 0.1580
</code></pre>
</div>
</div>
</section>
<section id="confusion-matrix-for-each-model" class="level3">
<h3 class="anchored" data-anchor-id="confusion-matrix-for-each-model">Confusion Matrix for Each Model</h3>
<p>The confusion matrix is a useful tool for understanding the performance of a classification model. It shows the actual vs.&nbsp;predicted classifications.</p>
<p>Let’s plot the confusion matrix for each model and discuss what the scores mean:</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, model <span class="kw">in</span> models.items():</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model.predict(test_data.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    conf_mat <span class="op">=</span> confusion_matrix(test_data[<span class="st">'Class'</span>], predictions)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    sns.heatmap(conf_mat, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'g'</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f'Confusion Matrix for </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Predicted'</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Actual'</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-13-output-1.png" width="631" height="523"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-13-output-2.png" width="631" height="523"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-13-output-3.png" width="631" height="523"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-13-output-4.png" width="631" height="523"></p>
</div>
</div>
<section id="interpretation-of-confusion-matrix" class="level4">
<h4 class="anchored" data-anchor-id="interpretation-of-confusion-matrix">Interpretation of Confusion Matrix:</h4>
<ul>
<li><strong>True Positives (TP)</strong>: Correctly predicted positive observations.</li>
<li><strong>True Negatives (TN)</strong>: Correctly predicted negative observations.</li>
<li><strong>False Positives (FP)</strong>: Incorrectly predicted positive observations (Type I error).</li>
<li><strong>False Negatives (FN)</strong>: Incorrectly predicted negative observations (Type II error).</li>
</ul>
<p>A model with high TP and TN values and low FP and FN values is generally considered good. However, the importance of each type of error can vary based on the application. For instance, in medical diagnosis, reducing FN (missed diagnoses) may be more critical than reducing FP.</p>
</section>
</section>
</section>
<section id="model-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="model-interpretation">Model Interpretation</h2>
<p>Interpreting machine learning models involves understanding the decisions made by the model and assessing their importance in the context of the task. For our driving behavior classification project, the model interpretation will shed light on how well the models are distinguishing between SLOW, NORMAL, and AGGRESSIVE driving behaviors.</p>
<section id="interpretation-of-model-results" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-of-model-results">Interpretation of Model Results</h3>
<p>The confusion matrices provide a visual and quantitative way to measure the performance of the models. Ideally, a well-performing model would have high numbers along the diagonal, indicating correct predictions, and low numbers off the diagonal.</p>
<p>In all four confusion matrices, we observe that the models predict only one class, which indicates a severe imbalance in their learning, potentially caused by class imbalance or other data issues. They fail to predict any instances of the ‘SLOW’ and ‘AGGRESSIVE’ classes, classifying everything as ‘NORMAL’. This is also reflected in the low precision and F1-scores for all models, as these metrics account for both false positives and false negatives.</p>
</section>
<section id="feature-importance-analysis" class="level3">
<h3 class="anchored" data-anchor-id="feature-importance-analysis">Feature Importance Analysis</h3>
<p>Understanding which features are most important for making predictions can provide insights into the dataset and the model’s decision-making process. For tree-based models, we can directly retrieve feature importance. For other models, such as SVM and Neural Networks, the interpretation can be more complex and may require additional techniques like permutation importance.</p>
<p>For Decision Trees and Random Forests, feature importance is calculated based on how well they split the data and reduce impurity. Here’s how you might compute and plot the feature importances for the Random Forest model:</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>importances <span class="op">=</span> random_forest.feature_importances_</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> np.argsort(importances)[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> train_data.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>).columns</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the feature importances of the forest</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Feature importances"</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>plt.bar(<span class="bu">range</span>(train_data.shape[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">1</span>), importances[indices], align<span class="op">=</span><span class="st">"center"</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(train_data.shape[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">1</span>), features[indices], rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>plt.xlim([<span class="op">-</span><span class="dv">1</span>, train_data.shape[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">1</span>])</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-14-output-1.png" width="943" height="566"></p>
</div>
</div>
<p>For the SVM and Neural Network models, feature importance is not as straightforward because these models do not inherently provide a method for evaluating the importance of features. However, for the SVM model with a linear kernel, we can look at the weights assigned to each feature to determine their importance. For neural networks, especially deep ones, feature importance is a more complex area of research and often involves using additional tools or methods.</p>
<section id="results" class="level4">
<h4 class="anchored" data-anchor-id="results">Results</h4>
<p>The models’ inability to correctly classify all three driving behaviors indicates that further investigation into the data distribution, preprocessing, and model training is necessary. This could include resampling techniques to address class imbalance, feature engineering to better capture the characteristics of aggressive driving, or exploring more complex models and tuning strategies. It’s also important to consider the real-world applicability of these models — in a safety-critical domain such as driving behavior classification, we must strive for high precision and recall to ensure all aggressive behaviors are accurately detected.</p>
</section>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In summarizing our findings, the machine learning models developed to classify driving behavior as SLOW, NORMAL, or AGGRESSIVE revealed critical insights. The evaluation metrics and confusion matrices indicated that our models predominantly predicted the NORMAL class while failing to recognize SLOW and AGGRESSIVE behaviors. The accuracy across the models ranged from approximately 26% to 41%, with the SVM model achieving the highest accuracy. However, the precision and recall scores were low for all models, indicating a significant area for improvement.</p>
<section id="limitations" class="level3">
<h3 class="anchored" data-anchor-id="limitations">Limitations</h3>
<p>The limitations of our current approach are evident:</p>
<ul>
<li><strong>Class Imbalance</strong>: The models’ tendency to predict mostly the NORMAL class suggests a possible class imbalance.</li>
<li><strong>Model Overfitting</strong>: The poor generalization to other classes may also indicate overfitting to the majority class.</li>
<li><strong>Feature Representation</strong>: The current features may not sufficiently represent the characteristics unique to each class.</li>
</ul>
</section>
<section id="potential-improvements" class="level3">
<h3 class="anchored" data-anchor-id="potential-improvements">Potential Improvements</h3>
<p>Several improvements can be proposed:</p>
<ul>
<li><strong>Data Resampling</strong>: Implementing techniques like SMOTE or random oversampling to balance the class distribution could be beneficial.</li>
<li><strong>Feature Engineering</strong>: Developing more sophisticated features or utilizing feature selection methods to capture more nuanced patterns of aggressive driving.</li>
<li><strong>Model Complexity</strong>: Exploring more complex models or deep learning approaches that might capture the intricacies in the data better.</li>
<li><strong>Extended Hyperparameter Tuning</strong>: Conducting a more thorough hyperparameter optimization, given more computational time and resources.</li>
</ul>
<p>In conclusion, while the current models have provided a foundation, further work is needed to develop a robust classifier that can accurately identify various driving behaviors, which is crucial for enhancing road safety and reducing accidents.</p>


<!-- -->

</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb18" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Classification"</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Joanna Fang"</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2023-11-30"</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [ml, code, classification, driving, kaggle]</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co">  html: </span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co">    code-block-bg: "#FFFFFF"</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="co">    code-block-border-left: "#E83283"</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools:</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co">      source: true</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co">      toggle: false</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="co">      caption: none</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="fu"># Classification: Predictive Analytics for Driving Behavior Classification</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a><span class="al">![](thumbnail.jpg)</span>{width="50%" fig-align="center"}</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a><span class="fu">## Project Introduction</span></span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>Welcome to our exploration into the world of machine learning and its application in predicting driving behaviors. In this project, we dive into the realm of vehicular safety, aiming to leverage sensor data to classify driving patterns into three categories: SLOW, NORMAL, and AGGRESSIVE. This endeavor is not just a technical challenge but a crucial step towards enhancing road safety and reducing traffic accidents.</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a><span class="fu">### Objective</span></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>The primary goal of this project is to accurately predict driving behaviors using data from commonly available sensors in smartphones. By analyzing accelerometer and gyroscope data, we aim to classify driving styles into SLOW, NORMAL, or AGGRESSIVE, contributing significantly to the prevention of road mishaps.</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a><span class="fu">### Importance and Applications</span></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>The importance of this project is underscored by the alarming statistics from the AAA Foundation for Traffic Safety, highlighting that over half of fatal crashes involve aggressive driving actions. Through this project, we offer a scalable and readily deployable solution to monitor and predict dangerous driving behaviors, potentially saving lives and making our roads safer. Applications of this model extend to insurance companies for risk assessment, ride-sharing services for driver monitoring, and personal safety apps to alert drivers of their driving patterns.</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data Collection and Description</span></span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a><span class="fu">### Source of the Data</span></span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>The dataset for this project is derived from a real-world scenario, specifically designed to capture driving behaviors. Utilizing a data collector application on Android devices, we have gathered sensor readings directly relevant to driving dynamics.</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a><span class="fu">### Dataset Description</span></span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>The dataset is a rich collection of sensor data recorded from a Samsung Galaxy S21, chosen for its advanced sensor capabilities. Here's a breakdown of the dataset features:</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Acceleration Data</span></span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Axes: X, Y, Z</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Unit: Meters per second squared (m/s²)</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Note: Gravitational acceleration has been filtered out to focus on the acceleration caused by driving actions.</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Rotation Data</span></span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Axes: X, Y, Z</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Unit: Degrees per second (°/s)</span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Purpose: Captures the angular changes during driving, indicative of turns and maneuvers.</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Classification Label</span></span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Categories: SLOW, NORMAL, AGGRESSIVE</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Basis: The driving behavior classification based on the sensor data patterns.</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Additional Information</span></span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sampling Rate: 2 samples per second, ensuring a fine-grained capture of driving dynamics.</span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Timestamp: Included for each sample, allowing for temporal analysis of driving patterns.</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>In the following sections, we will delve into the preprocessing, exploratory analysis, and modeling of this dataset to build a robust classifier for driving behaviors. Stay tuned as we unravel the insights hidden within this data and develop a machine learning model with the potential to make a real-world impact.</span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data Preprocessing</span></span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a>In the realm of machine learning, data preprocessing is a critical step in preparing raw data for modeling. Our datasets, <span class="in">`motion_data_1.csv`</span> (test data) and <span class="in">`motion_data_2.csv`</span> (train data), contain acceleration and rotation measurements alongside driving behavior classifications. Let's walk through the preprocessing steps:</span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a><span class="fu">### Handling Missing Values</span></span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a>First, we'll check for missing values in both datasets. Missing data can significantly impact the performance of a machine learning model. If missing values are found, strategies such as imputation or removal of the affected rows can be considered.</span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the datasets</span></span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> pd.read_csv(<span class="st">'motion_data_1.csv'</span>)</span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> pd.read_csv(<span class="st">'motion_data_2.csv'</span>)</span>
<span id="cb18-78"><a href="#cb18-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-79"><a href="#cb18-79" aria-hidden="true" tabindex="-1"></a><span class="co"># Checking for missing values in both datasets</span></span>
<span id="cb18-80"><a href="#cb18-80" aria-hidden="true" tabindex="-1"></a>missing_values_test <span class="op">=</span> test_data.isnull().<span class="bu">sum</span>()</span>
<span id="cb18-81"><a href="#cb18-81" aria-hidden="true" tabindex="-1"></a>missing_values_train <span class="op">=</span> train_data.isnull().<span class="bu">sum</span>()</span>
<span id="cb18-82"><a href="#cb18-82" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-83"><a href="#cb18-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-84"><a href="#cb18-84" aria-hidden="true" tabindex="-1"></a><span class="fu">### Normalizing or Scaling the Data</span></span>
<span id="cb18-85"><a href="#cb18-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-86"><a href="#cb18-86" aria-hidden="true" tabindex="-1"></a>Normalization or scaling is crucial when dealing with sensor data. It ensures that each feature contributes proportionately to the final prediction. Given the different scales of acceleration (in m/s²) and rotation (in °/s), applying a scaling method like Min-Max scaling or Standardization is important.</span>
<span id="cb18-87"><a href="#cb18-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-90"><a href="#cb18-90" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-91"><a href="#cb18-91" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb18-92"><a href="#cb18-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-93"><a href="#cb18-93" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardizing the data</span></span>
<span id="cb18-94"><a href="#cb18-94" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb18-95"><a href="#cb18-95" aria-hidden="true" tabindex="-1"></a>train_data_scaled <span class="op">=</span> scaler.fit_transform(train_data.iloc[:, :<span class="op">-</span><span class="dv">2</span>]) <span class="co"># Excluding 'Class' and 'Timestamp' columns</span></span>
<span id="cb18-96"><a href="#cb18-96" aria-hidden="true" tabindex="-1"></a>test_data_scaled <span class="op">=</span> scaler.transform(test_data.iloc[:, :<span class="op">-</span><span class="dv">2</span>])</span>
<span id="cb18-97"><a href="#cb18-97" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-98"><a href="#cb18-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-99"><a href="#cb18-99" aria-hidden="true" tabindex="-1"></a><span class="fu">### Feature Engineering </span></span>
<span id="cb18-100"><a href="#cb18-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-101"><a href="#cb18-101" aria-hidden="true" tabindex="-1"></a>Feature engineering might involve creating new features or modifying existing ones to improve model performance. In our case, we might consider deriving features like the total magnitude of acceleration or rotation.</span>
<span id="cb18-102"><a href="#cb18-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-105"><a href="#cb18-105" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-106"><a href="#cb18-106" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-107"><a href="#cb18-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-108"><a href="#cb18-108" aria-hidden="true" tabindex="-1"></a><span class="co"># Adding a feature: Total magnitude of acceleration</span></span>
<span id="cb18-109"><a href="#cb18-109" aria-hidden="true" tabindex="-1"></a>train_data[<span class="st">'TotalAcc'</span>] <span class="op">=</span> np.sqrt(train_data[<span class="st">'AccX'</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> train_data[<span class="st">'AccY'</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> train_data[<span class="st">'AccZ'</span>]<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb18-110"><a href="#cb18-110" aria-hidden="true" tabindex="-1"></a>test_data[<span class="st">'TotalAcc'</span>] <span class="op">=</span> np.sqrt(test_data[<span class="st">'AccX'</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> test_data[<span class="st">'AccY'</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> test_data[<span class="st">'AccZ'</span>]<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb18-111"><a href="#cb18-111" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-112"><a href="#cb18-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-113"><a href="#cb18-113" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exploratory Data Analysis (EDA)</span></span>
<span id="cb18-114"><a href="#cb18-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-115"><a href="#cb18-115" aria-hidden="true" tabindex="-1"></a><span class="fu">### Statistical Summary of the Dataset</span></span>
<span id="cb18-116"><a href="#cb18-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-117"><a href="#cb18-117" aria-hidden="true" tabindex="-1"></a>Understanding the basic statistics of the dataset is essential. This includes measures like mean, median, standard deviation, etc., providing insights into the data distribution.</span>
<span id="cb18-118"><a href="#cb18-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-121"><a href="#cb18-121" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-122"><a href="#cb18-122" aria-hidden="true" tabindex="-1"></a><span class="co"># Descriptive statistics of the training data</span></span>
<span id="cb18-123"><a href="#cb18-123" aria-hidden="true" tabindex="-1"></a>train_data_description <span class="op">=</span> train_data.describe()</span>
<span id="cb18-124"><a href="#cb18-124" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-125"><a href="#cb18-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-126"><a href="#cb18-126" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visualization of Data Distribution</span></span>
<span id="cb18-127"><a href="#cb18-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-128"><a href="#cb18-128" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Histograms or Box Plots for Acceleration and Rotation Data</span></span>
<span id="cb18-129"><a href="#cb18-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-130"><a href="#cb18-130" aria-hidden="true" tabindex="-1"></a>Histograms and box plots are effective for visualizing the distribution of sensor data and identifying outliers or skewness in the data.</span>
<span id="cb18-131"><a href="#cb18-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-134"><a href="#cb18-134" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-135"><a href="#cb18-135" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-136"><a href="#cb18-136" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb18-137"><a href="#cb18-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-138"><a href="#cb18-138" aria-hidden="true" tabindex="-1"></a><span class="co"># Setting up the figure for multiple subplots</span></span>
<span id="cb18-139"><a href="#cb18-139" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">3</span>, ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">15</span>))</span>
<span id="cb18-140"><a href="#cb18-140" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'Histograms of Acceleration and Rotation Data'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb18-141"><a href="#cb18-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-142"><a href="#cb18-142" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting histograms for each sensor data column</span></span>
<span id="cb18-143"><a href="#cb18-143" aria-hidden="true" tabindex="-1"></a>sns.histplot(train_data[<span class="st">'AccX'</span>], kde<span class="op">=</span><span class="va">True</span>, ax<span class="op">=</span>axes[<span class="dv">0</span>, <span class="dv">0</span>], color<span class="op">=</span><span class="st">'skyblue'</span>)</span>
<span id="cb18-144"><a href="#cb18-144" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_title(<span class="st">'Acceleration in X-axis (AccX)'</span>)</span>
<span id="cb18-145"><a href="#cb18-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-146"><a href="#cb18-146" aria-hidden="true" tabindex="-1"></a>sns.histplot(train_data[<span class="st">'AccY'</span>], kde<span class="op">=</span><span class="va">True</span>, ax<span class="op">=</span>axes[<span class="dv">0</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'olive'</span>)</span>
<span id="cb18-147"><a href="#cb18-147" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].set_title(<span class="st">'Acceleration in Y-axis (AccY)'</span>)</span>
<span id="cb18-148"><a href="#cb18-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-149"><a href="#cb18-149" aria-hidden="true" tabindex="-1"></a>sns.histplot(train_data[<span class="st">'AccZ'</span>], kde<span class="op">=</span><span class="va">True</span>, ax<span class="op">=</span>axes[<span class="dv">1</span>, <span class="dv">0</span>], color<span class="op">=</span><span class="st">'gold'</span>)</span>
<span id="cb18-150"><a href="#cb18-150" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_title(<span class="st">'Acceleration in Z-axis (AccZ)'</span>)</span>
<span id="cb18-151"><a href="#cb18-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-152"><a href="#cb18-152" aria-hidden="true" tabindex="-1"></a>sns.histplot(train_data[<span class="st">'GyroX'</span>], kde<span class="op">=</span><span class="va">True</span>, ax<span class="op">=</span>axes[<span class="dv">1</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'teal'</span>)</span>
<span id="cb18-153"><a href="#cb18-153" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_title(<span class="st">'Rotation in X-axis (GyroX)'</span>)</span>
<span id="cb18-154"><a href="#cb18-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-155"><a href="#cb18-155" aria-hidden="true" tabindex="-1"></a>sns.histplot(train_data[<span class="st">'GyroY'</span>], kde<span class="op">=</span><span class="va">True</span>, ax<span class="op">=</span>axes[<span class="dv">2</span>, <span class="dv">0</span>], color<span class="op">=</span><span class="st">'salmon'</span>)</span>
<span id="cb18-156"><a href="#cb18-156" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>, <span class="dv">0</span>].set_title(<span class="st">'Rotation in Y-axis (GyroY)'</span>)</span>
<span id="cb18-157"><a href="#cb18-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-158"><a href="#cb18-158" aria-hidden="true" tabindex="-1"></a>sns.histplot(train_data[<span class="st">'GyroZ'</span>], kde<span class="op">=</span><span class="va">True</span>, ax<span class="op">=</span>axes[<span class="dv">2</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'violet'</span>)</span>
<span id="cb18-159"><a href="#cb18-159" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>, <span class="dv">1</span>].set_title(<span class="st">'Rotation in Z-axis (GyroZ)'</span>)</span>
<span id="cb18-160"><a href="#cb18-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-161"><a href="#cb18-161" aria-hidden="true" tabindex="-1"></a>plt.tight_layout(rect<span class="op">=</span>[<span class="dv">0</span>, <span class="fl">0.03</span>, <span class="dv">1</span>, <span class="fl">0.95</span>])</span>
<span id="cb18-162"><a href="#cb18-162" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-163"><a href="#cb18-163" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-164"><a href="#cb18-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-165"><a href="#cb18-165" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Acceleration (AccX, AccY, AccZ)**: The distributions for acceleration on all three axes appear to be roughly bell-shaped, indicating that most of the readings are clustered around the mean, with fewer readings at the extreme ends. This suggests normal driving conditions with occasional variances that could indicate moments of acceleration or deceleration.</span>
<span id="cb18-166"><a href="#cb18-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-167"><a href="#cb18-167" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Rotation (GyroX, GyroY, GyroZ)**: The rotation data histograms show a similar bell-shaped distribution, especially for the X and Y axes, indicating consistent turning behavior with some outliers potentially representing more aggressive turns or corrections. The GyroZ histogram is notably narrower, which might suggest that rotation around the Z-axis (often corresponding to yaw movements) is less variable during normal driving conditions.</span>
<span id="cb18-168"><a href="#cb18-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-169"><a href="#cb18-169" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Correlation Heatmaps</span></span>
<span id="cb18-170"><a href="#cb18-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-171"><a href="#cb18-171" aria-hidden="true" tabindex="-1"></a>A correlation heatmap helps in understanding the relationships between different sensor readings. It's crucial for identifying features that are highly correlated and might need to be addressed during feature selection.</span>
<span id="cb18-172"><a href="#cb18-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-175"><a href="#cb18-175" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-176"><a href="#cb18-176" aria-hidden="true" tabindex="-1"></a><span class="co"># For the correlation heatmap, we need to exclude non-numeric columns (Class and Timestamp)</span></span>
<span id="cb18-177"><a href="#cb18-177" aria-hidden="true" tabindex="-1"></a>train_data_numeric <span class="op">=</span> train_data.select_dtypes(include<span class="op">=</span>[<span class="st">'float64'</span>, <span class="st">'int64'</span>])</span>
<span id="cb18-178"><a href="#cb18-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-179"><a href="#cb18-179" aria-hidden="true" tabindex="-1"></a><span class="co"># Generating the correlation heatmap</span></span>
<span id="cb18-180"><a href="#cb18-180" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb18-181"><a href="#cb18-181" aria-hidden="true" tabindex="-1"></a>sns.heatmap(train_data_numeric.corr(), annot<span class="op">=</span><span class="va">True</span>, cmap<span class="op">=</span><span class="st">'coolwarm'</span>)</span>
<span id="cb18-182"><a href="#cb18-182" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Correlation Heatmap of Numeric Features'</span>)</span>
<span id="cb18-183"><a href="#cb18-183" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-184"><a href="#cb18-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-185"><a href="#cb18-185" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-186"><a href="#cb18-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-187"><a href="#cb18-187" aria-hidden="true" tabindex="-1"></a>In this heatmap:</span>
<span id="cb18-188"><a href="#cb18-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-189"><a href="#cb18-189" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Values close to 1 or -1 indicate a strong positive or negative correlation, respectively.</span>
<span id="cb18-190"><a href="#cb18-190" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Values close to 0 suggest no linear correlation between the variables.</span>
<span id="cb18-191"><a href="#cb18-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-192"><a href="#cb18-192" aria-hidden="true" tabindex="-1"></a>It appears that:</span>
<span id="cb18-193"><a href="#cb18-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-194"><a href="#cb18-194" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>There are no exceptionally strong correlations between the different axes of acceleration and rotation, which is desirable in a dataset used for behavior prediction as it indicates that the sensor readings provide unique information.</span>
<span id="cb18-195"><a href="#cb18-195" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The strongest negative correlation is observed between GyroZ and AccX, which might suggest that certain types of aggressive driving behaviors cause inverse changes in these two measurements.</span>
<span id="cb18-196"><a href="#cb18-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-197"><a href="#cb18-197" aria-hidden="true" tabindex="-1"></a>The absence of very high correlations means that there may not be redundant features in the dataset, which is good for a machine learning model that relies on diverse data points to make predictions. However, the subtle correlations that do exist can still provide valuable insights when developing features and training models.</span>
<span id="cb18-198"><a href="#cb18-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-199"><a href="#cb18-199" aria-hidden="true" tabindex="-1"></a>These visualizations and their interpretations are key in understanding the underlying patterns in the driving behavior dataset, which will inform the feature selection and modeling phases of the machine learning project.</span>
<span id="cb18-200"><a href="#cb18-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-201"><a href="#cb18-201" aria-hidden="true" tabindex="-1"></a>In the next sections, we will delve into model selection, training, and evaluation, using the insights and data preparations we've just discussed. Stay tuned!</span>
<span id="cb18-202"><a href="#cb18-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-203"><a href="#cb18-203" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data Preparation for Modeling</span></span>
<span id="cb18-204"><a href="#cb18-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-205"><a href="#cb18-205" aria-hidden="true" tabindex="-1"></a>Preparing the data for modeling is a crucial step in the machine learning pipeline. It ensures that the data fed into the model is clean, representative, and well-formatted.</span>
<span id="cb18-206"><a href="#cb18-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-207"><a href="#cb18-207" aria-hidden="true" tabindex="-1"></a><span class="fu">### Splitting Data into Training and Testing Sets</span></span>
<span id="cb18-208"><a href="#cb18-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-209"><a href="#cb18-209" aria-hidden="true" tabindex="-1"></a>We have two datasets: <span class="in">`train_data`</span> and <span class="in">`test_data`</span>, pre-split for our convenience. Typically, we would use a function like <span class="in">`train_test_split`</span> from <span class="in">`scikit-learn`</span> to divide our dataset into a training set and a test set, ensuring that both sets are representative of the overall distribution. However, in this scenario, that step is already accounted for.</span>
<span id="cb18-210"><a href="#cb18-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-213"><a href="#cb18-213" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-214"><a href="#cb18-214" aria-hidden="true" tabindex="-1"></a><span class="co"># Since the data is pre-split, this step is not required for our current workflow.</span></span>
<span id="cb18-215"><a href="#cb18-215" aria-hidden="true" tabindex="-1"></a><span class="co"># Normally, we would do something like this:</span></span>
<span id="cb18-216"><a href="#cb18-216" aria-hidden="true" tabindex="-1"></a><span class="co"># from sklearn.model_selection import train_test_split</span></span>
<span id="cb18-217"><a href="#cb18-217" aria-hidden="true" tabindex="-1"></a><span class="co"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span></span>
<span id="cb18-218"><a href="#cb18-218" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-219"><a href="#cb18-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-220"><a href="#cb18-220" aria-hidden="true" tabindex="-1"></a><span class="fu">### Handling Class Imbalance</span></span>
<span id="cb18-221"><a href="#cb18-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-222"><a href="#cb18-222" aria-hidden="true" tabindex="-1"></a>Class imbalance can significantly skew the performance of a classifier towards the majority class. It's important to check the balance of classes and apply techniques such as resampling if necessary.</span>
<span id="cb18-223"><a href="#cb18-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-226"><a href="#cb18-226" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-227"><a href="#cb18-227" aria-hidden="true" tabindex="-1"></a><span class="co"># Checking the balance of the classes</span></span>
<span id="cb18-228"><a href="#cb18-228" aria-hidden="true" tabindex="-1"></a>class_counts <span class="op">=</span> train_data[<span class="st">'Class'</span>].value_counts()</span>
<span id="cb18-229"><a href="#cb18-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-230"><a href="#cb18-230" aria-hidden="true" tabindex="-1"></a><span class="co"># If imbalance is found, we might consider resampling strategies like:</span></span>
<span id="cb18-231"><a href="#cb18-231" aria-hidden="true" tabindex="-1"></a><span class="co"># - Upsampling minority classes</span></span>
<span id="cb18-232"><a href="#cb18-232" aria-hidden="true" tabindex="-1"></a><span class="co"># - Downsampling majority classes</span></span>
<span id="cb18-233"><a href="#cb18-233" aria-hidden="true" tabindex="-1"></a><span class="co"># - Using SMOTE (Synthetic Minority Over-sampling Technique)</span></span>
<span id="cb18-234"><a href="#cb18-234" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-235"><a href="#cb18-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-236"><a href="#cb18-236" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model Selection</span></span>
<span id="cb18-237"><a href="#cb18-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-238"><a href="#cb18-238" aria-hidden="true" tabindex="-1"></a>Selecting the right model is about finding the balance between prediction accuracy, computational efficiency, and the ease of interpretation.</span>
<span id="cb18-239"><a href="#cb18-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-240"><a href="#cb18-240" aria-hidden="true" tabindex="-1"></a><span class="fu">### Overview of Potential Machine Learning Models for Classification</span></span>
<span id="cb18-241"><a href="#cb18-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-242"><a href="#cb18-242" aria-hidden="true" tabindex="-1"></a>For our classification task, we have several models at our disposal:</span>
<span id="cb18-243"><a href="#cb18-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-244"><a href="#cb18-244" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Decision Tree**: A good baseline that is easy to interpret.</span>
<span id="cb18-245"><a href="#cb18-245" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Random Forest**: An ensemble method that can improve on the performance of a single decision tree.</span>
<span id="cb18-246"><a href="#cb18-246" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Support Vector Machine (SVM)**: Effective in high-dimensional spaces.</span>
<span id="cb18-247"><a href="#cb18-247" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Neural Networks**: Potentially high performance but may require more data and compute resources.</span>
<span id="cb18-248"><a href="#cb18-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-249"><a href="#cb18-249" aria-hidden="true" tabindex="-1"></a><span class="fu">### Criteria for Model Selection</span></span>
<span id="cb18-250"><a href="#cb18-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-251"><a href="#cb18-251" aria-hidden="true" tabindex="-1"></a>When selecting the model, we consider several criteria:</span>
<span id="cb18-252"><a href="#cb18-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-253"><a href="#cb18-253" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Accuracy**: How often the model makes the correct prediction.</span>
<span id="cb18-254"><a href="#cb18-254" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Speed**: How quickly the model can be trained and used for prediction.</span>
<span id="cb18-255"><a href="#cb18-255" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Interpretability**: The ease with which we can understand the model's predictions.</span>
<span id="cb18-256"><a href="#cb18-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-257"><a href="#cb18-257" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model Training</span></span>
<span id="cb18-258"><a href="#cb18-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-259"><a href="#cb18-259" aria-hidden="true" tabindex="-1"></a>Training our models is like teaching them to understand patterns within the data that distinguish between SLOW, NORMAL, and AGGRESSIVE driving behaviors. We will employ four different machine learning models to find the best predictor.</span>
<span id="cb18-260"><a href="#cb18-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-261"><a href="#cb18-261" aria-hidden="true" tabindex="-1"></a><span class="fu">### Training Different Models</span></span>
<span id="cb18-262"><a href="#cb18-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-263"><a href="#cb18-263" aria-hidden="true" tabindex="-1"></a>We start by training different types of models. Each has its own strengths and might capture different aspects of the driving behavior.</span>
<span id="cb18-264"><a href="#cb18-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-267"><a href="#cb18-267" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-268"><a href="#cb18-268" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb18-269"><a href="#cb18-269" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb18-270"><a href="#cb18-270" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb18-271"><a href="#cb18-271" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neural_network <span class="im">import</span> MLPClassifier</span>
<span id="cb18-272"><a href="#cb18-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-273"><a href="#cb18-273" aria-hidden="true" tabindex="-1"></a><span class="co"># Initializing models with default parameters</span></span>
<span id="cb18-274"><a href="#cb18-274" aria-hidden="true" tabindex="-1"></a>decision_tree <span class="op">=</span> DecisionTreeClassifier()</span>
<span id="cb18-275"><a href="#cb18-275" aria-hidden="true" tabindex="-1"></a>random_forest <span class="op">=</span> RandomForestClassifier()</span>
<span id="cb18-276"><a href="#cb18-276" aria-hidden="true" tabindex="-1"></a>svm <span class="op">=</span> SVC()</span>
<span id="cb18-277"><a href="#cb18-277" aria-hidden="true" tabindex="-1"></a>neural_network <span class="op">=</span> MLPClassifier()</span>
<span id="cb18-278"><a href="#cb18-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-279"><a href="#cb18-279" aria-hidden="true" tabindex="-1"></a>decision_tree.fit(train_data.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>), train_data[<span class="st">'Class'</span>])</span>
<span id="cb18-280"><a href="#cb18-280" aria-hidden="true" tabindex="-1"></a>random_forest.fit(train_data.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>), train_data[<span class="st">'Class'</span>])</span>
<span id="cb18-281"><a href="#cb18-281" aria-hidden="true" tabindex="-1"></a>svm.fit(train_data.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>), train_data[<span class="st">'Class'</span>]) </span>
<span id="cb18-282"><a href="#cb18-282" aria-hidden="true" tabindex="-1"></a>neural_network.fit(train_data.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>), train_data[<span class="st">'Class'</span>])</span>
<span id="cb18-283"><a href="#cb18-283" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-284"><a href="#cb18-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-285"><a href="#cb18-285" aria-hidden="true" tabindex="-1"></a>The <span class="in">`fit`</span> method allows the models to learn from the training data. It's during this process that they identify which features are most important for predicting driving behavior.</span>
<span id="cb18-286"><a href="#cb18-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-287"><a href="#cb18-287" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hyperparameter Tuning</span></span>
<span id="cb18-288"><a href="#cb18-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-289"><a href="#cb18-289" aria-hidden="true" tabindex="-1"></a>To optimize our models, we tweak their settings, known as hyperparameters. This process is akin to fine-tuning an instrument to ensure it plays the perfect note.</span>
<span id="cb18-290"><a href="#cb18-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-293"><a href="#cb18-293" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-294"><a href="#cb18-294" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> RandomizedSearchCV</span>
<span id="cb18-295"><a href="#cb18-295" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-296"><a href="#cb18-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-297"><a href="#cb18-297" aria-hidden="true" tabindex="-1"></a><span class="co"># Reduce the number of iterations and cross-validation folds</span></span>
<span id="cb18-298"><a href="#cb18-298" aria-hidden="true" tabindex="-1"></a>n_iter_search <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb18-299"><a href="#cb18-299" aria-hidden="true" tabindex="-1"></a>cv_folds <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb18-300"><a href="#cb18-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-301"><a href="#cb18-301" aria-hidden="true" tabindex="-1"></a><span class="co"># Simplify the models - example for Random Forest</span></span>
<span id="cb18-302"><a href="#cb18-302" aria-hidden="true" tabindex="-1"></a>rf_param_grid <span class="op">=</span> {</span>
<span id="cb18-303"><a href="#cb18-303" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_estimators'</span>: [<span class="dv">50</span>, <span class="dv">100</span>],  <span class="co"># reduced number of trees</span></span>
<span id="cb18-304"><a href="#cb18-304" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="dv">10</span>, <span class="va">None</span>],  <span class="co"># fewer options</span></span>
<span id="cb18-305"><a href="#cb18-305" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_split'</span>: [<span class="dv">2</span>, <span class="dv">5</span>]</span>
<span id="cb18-306"><a href="#cb18-306" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-307"><a href="#cb18-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-308"><a href="#cb18-308" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the RandomizedSearchCV object for Random Forest with fewer iterations and folds</span></span>
<span id="cb18-309"><a href="#cb18-309" aria-hidden="true" tabindex="-1"></a>random_search_rf <span class="op">=</span> RandomizedSearchCV(</span>
<span id="cb18-310"><a href="#cb18-310" aria-hidden="true" tabindex="-1"></a>    RandomForestClassifier(), </span>
<span id="cb18-311"><a href="#cb18-311" aria-hidden="true" tabindex="-1"></a>    param_distributions<span class="op">=</span>rf_param_grid, </span>
<span id="cb18-312"><a href="#cb18-312" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span>n_iter_search, </span>
<span id="cb18-313"><a href="#cb18-313" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>cv_folds, </span>
<span id="cb18-314"><a href="#cb18-314" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>, </span>
<span id="cb18-315"><a href="#cb18-315" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb18-316"><a href="#cb18-316" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-317"><a href="#cb18-317" aria-hidden="true" tabindex="-1"></a>random_search_rf.fit(train_data.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>), train_data[<span class="st">'Class'</span>])</span>
<span id="cb18-318"><a href="#cb18-318" aria-hidden="true" tabindex="-1"></a>best_params_rf <span class="op">=</span> random_search_rf.best_params_</span>
<span id="cb18-319"><a href="#cb18-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-320"><a href="#cb18-320" aria-hidden="true" tabindex="-1"></a><span class="co"># RandomizedSearchCV for Decision Tree</span></span>
<span id="cb18-321"><a href="#cb18-321" aria-hidden="true" tabindex="-1"></a>dt_param_grid <span class="op">=</span> {</span>
<span id="cb18-322"><a href="#cb18-322" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>],</span>
<span id="cb18-323"><a href="#cb18-323" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_leaf'</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>]</span>
<span id="cb18-324"><a href="#cb18-324" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-325"><a href="#cb18-325" aria-hidden="true" tabindex="-1"></a>random_search_dt <span class="op">=</span> RandomizedSearchCV(</span>
<span id="cb18-326"><a href="#cb18-326" aria-hidden="true" tabindex="-1"></a>    DecisionTreeClassifier(), </span>
<span id="cb18-327"><a href="#cb18-327" aria-hidden="true" tabindex="-1"></a>    param_distributions<span class="op">=</span>dt_param_grid, </span>
<span id="cb18-328"><a href="#cb18-328" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span>n_iter_search, </span>
<span id="cb18-329"><a href="#cb18-329" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>cv_folds, </span>
<span id="cb18-330"><a href="#cb18-330" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb18-331"><a href="#cb18-331" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-332"><a href="#cb18-332" aria-hidden="true" tabindex="-1"></a>random_search_dt.fit(train_data.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>), train_data[<span class="st">'Class'</span>])</span>
<span id="cb18-333"><a href="#cb18-333" aria-hidden="true" tabindex="-1"></a>best_params_dt <span class="op">=</span> random_search_dt.best_params_</span>
<span id="cb18-334"><a href="#cb18-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-335"><a href="#cb18-335" aria-hidden="true" tabindex="-1"></a><span class="co"># RandomizedSearchCV for SVM</span></span>
<span id="cb18-336"><a href="#cb18-336" aria-hidden="true" tabindex="-1"></a><span class="co"># Using an extremely small subset of the data</span></span>
<span id="cb18-337"><a href="#cb18-337" aria-hidden="true" tabindex="-1"></a>subset_size <span class="op">=</span> <span class="dv">50</span>  <span class="co"># Very small subset for extremely quick execution</span></span>
<span id="cb18-338"><a href="#cb18-338" aria-hidden="true" tabindex="-1"></a>train_subset <span class="op">=</span> train_data.sample(n<span class="op">=</span>subset_size, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb18-339"><a href="#cb18-339" aria-hidden="true" tabindex="-1"></a><span class="co"># Simplified RandomizedSearchCV for SVM</span></span>
<span id="cb18-340"><a href="#cb18-340" aria-hidden="true" tabindex="-1"></a>random_search_svm <span class="op">=</span> RandomizedSearchCV(</span>
<span id="cb18-341"><a href="#cb18-341" aria-hidden="true" tabindex="-1"></a>    SVC(), </span>
<span id="cb18-342"><a href="#cb18-342" aria-hidden="true" tabindex="-1"></a>    param_distributions<span class="op">=</span>{<span class="st">'C'</span>: [<span class="dv">1</span>], <span class="st">'kernel'</span>: [<span class="st">'linear'</span>]},  <span class="co"># Minimal hyperparameter space</span></span>
<span id="cb18-343"><a href="#cb18-343" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span><span class="dv">1</span>,  <span class="co"># Only one iteration</span></span>
<span id="cb18-344"><a href="#cb18-344" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">2</span>,  <span class="co"># Reduced to 2-fold cross-validation</span></span>
<span id="cb18-345"><a href="#cb18-345" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=</span><span class="dv">1</span>,  <span class="co"># Limit the number of jobs to 1 for a quick run</span></span>
<span id="cb18-346"><a href="#cb18-346" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb18-347"><a href="#cb18-347" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span>  <span class="co"># No verbosity</span></span>
<span id="cb18-348"><a href="#cb18-348" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-349"><a href="#cb18-349" aria-hidden="true" tabindex="-1"></a>random_search_svm.fit(train_subset.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>), train_subset[<span class="st">'Class'</span>])</span>
<span id="cb18-350"><a href="#cb18-350" aria-hidden="true" tabindex="-1"></a>best_params_svm <span class="op">=</span> random_search_svm.best_params_</span>
<span id="cb18-351"><a href="#cb18-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-352"><a href="#cb18-352" aria-hidden="true" tabindex="-1"></a><span class="co"># RandomizedSearchCV for Neural Network</span></span>
<span id="cb18-353"><a href="#cb18-353" aria-hidden="true" tabindex="-1"></a><span class="co"># Using a very small subset of the data</span></span>
<span id="cb18-354"><a href="#cb18-354" aria-hidden="true" tabindex="-1"></a>subset_size <span class="op">=</span> <span class="dv">50</span>  <span class="co"># Extremely small subset for quick execution</span></span>
<span id="cb18-355"><a href="#cb18-355" aria-hidden="true" tabindex="-1"></a>train_subset <span class="op">=</span> train_data.sample(n<span class="op">=</span>subset_size, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb18-356"><a href="#cb18-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-357"><a href="#cb18-357" aria-hidden="true" tabindex="-1"></a><span class="co"># Simplified parameter grid for Neural Network</span></span>
<span id="cb18-358"><a href="#cb18-358" aria-hidden="true" tabindex="-1"></a>nn_param_grid <span class="op">=</span> {</span>
<span id="cb18-359"><a href="#cb18-359" aria-hidden="true" tabindex="-1"></a>    <span class="st">'hidden_layer_sizes'</span>: [(<span class="dv">50</span>,)],  <span class="co"># Smaller network</span></span>
<span id="cb18-360"><a href="#cb18-360" aria-hidden="true" tabindex="-1"></a>    <span class="st">'activation'</span>: [<span class="st">'relu'</span>],  <span class="co"># One activation function</span></span>
<span id="cb18-361"><a href="#cb18-361" aria-hidden="true" tabindex="-1"></a>    <span class="st">'solver'</span>: [<span class="st">'adam'</span>],  <span class="co"># One solver</span></span>
<span id="cb18-362"><a href="#cb18-362" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_iter'</span>: [<span class="dv">100</span>]  <span class="co"># Fewer iterations</span></span>
<span id="cb18-363"><a href="#cb18-363" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-364"><a href="#cb18-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-365"><a href="#cb18-365" aria-hidden="true" tabindex="-1"></a><span class="co"># RandomizedSearchCV for Neural Network with simplified settings</span></span>
<span id="cb18-366"><a href="#cb18-366" aria-hidden="true" tabindex="-1"></a>random_search_nn <span class="op">=</span> RandomizedSearchCV(</span>
<span id="cb18-367"><a href="#cb18-367" aria-hidden="true" tabindex="-1"></a>    MLPClassifier(), </span>
<span id="cb18-368"><a href="#cb18-368" aria-hidden="true" tabindex="-1"></a>    nn_param_grid, </span>
<span id="cb18-369"><a href="#cb18-369" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span><span class="dv">2</span>,  <span class="co"># Fewer iterations</span></span>
<span id="cb18-370"><a href="#cb18-370" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">2</span>,  <span class="co"># Reduced cross-validation</span></span>
<span id="cb18-371"><a href="#cb18-371" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=</span><span class="dv">1</span>,  <span class="co"># Using single job for faster execution in limited environments</span></span>
<span id="cb18-372"><a href="#cb18-372" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb18-373"><a href="#cb18-373" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb18-374"><a href="#cb18-374" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-375"><a href="#cb18-375" aria-hidden="true" tabindex="-1"></a>random_search_nn.fit(train_subset.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>), train_subset[<span class="st">'Class'</span>])</span>
<span id="cb18-376"><a href="#cb18-376" aria-hidden="true" tabindex="-1"></a>best_params_nn <span class="op">=</span> random_search_nn.best_params_</span>
<span id="cb18-377"><a href="#cb18-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-378"><a href="#cb18-378" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the best parameters for all models</span></span>
<span id="cb18-379"><a href="#cb18-379" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best parameters for Random Forest:"</span>, best_params_rf)</span>
<span id="cb18-380"><a href="#cb18-380" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best parameters for Decision Tree:"</span>, best_params_dt)</span>
<span id="cb18-381"><a href="#cb18-381" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best parameters for SVM:"</span>, best_params_svm)</span>
<span id="cb18-382"><a href="#cb18-382" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best parameters for Neural Network:"</span>, best_params_nn)</span>
<span id="cb18-383"><a href="#cb18-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-384"><a href="#cb18-384" aria-hidden="true" tabindex="-1"></a><span class="co"># Now we can print or use the best_params_rf, best_params_svm, and best_params_nn for the best-found parameters</span></span>
<span id="cb18-385"><a href="#cb18-385" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-386"><a href="#cb18-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-387"><a href="#cb18-387" aria-hidden="true" tabindex="-1"></a>Using <span class="in">`GridSearchCV`</span>, we systematically work through multiple combinations of parameter tunes, cross-validating as we go to determine which tune gives the best performance.</span>
<span id="cb18-388"><a href="#cb18-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-389"><a href="#cb18-389" aria-hidden="true" tabindex="-1"></a>By the end of these steps, we will have trained four different models and optimized their hyperparameters for best performance. This careful tuning is what sets the stage for the final step: evaluating these models to select the ultimate candidate for our driving behavior prediction task.</span>
<span id="cb18-390"><a href="#cb18-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-391"><a href="#cb18-391" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model Evaluation</span></span>
<span id="cb18-392"><a href="#cb18-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-393"><a href="#cb18-393" aria-hidden="true" tabindex="-1"></a>After training our models and tuning their hyperparameters, the next critical step is model evaluation. This step helps us understand how well our models perform and guides us in selecting the best one for our application.</span>
<span id="cb18-394"><a href="#cb18-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-395"><a href="#cb18-395" aria-hidden="true" tabindex="-1"></a><span class="fu">### Evaluating Model Performance</span></span>
<span id="cb18-396"><a href="#cb18-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-397"><a href="#cb18-397" aria-hidden="true" tabindex="-1"></a>We'll evaluate each model using several key metrics: accuracy, precision, recall, and the F1-score. These metrics give us a well-rounded view of our models' performance.</span>
<span id="cb18-398"><a href="#cb18-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-399"><a href="#cb18-399" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Accuracy**: The proportion of true results (both true positives and true negatives) among the total number of cases examined.</span>
<span id="cb18-400"><a href="#cb18-400" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Precision**: The ratio of correctly predicted positive observations to the total predicted positives.</span>
<span id="cb18-401"><a href="#cb18-401" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Recall (Sensitivity)**: The ratio of correctly predicted positive observations to all observations in the actual class.</span>
<span id="cb18-402"><a href="#cb18-402" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**F1-Score**: The weighted average of Precision and Recall, best if there's an uneven class distribution.</span>
<span id="cb18-403"><a href="#cb18-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-404"><a href="#cb18-404" aria-hidden="true" tabindex="-1"></a>Let's calculate and print these metrics for each model:</span>
<span id="cb18-405"><a href="#cb18-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-408"><a href="#cb18-408" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-409"><a href="#cb18-409" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, precision_score, recall_score, f1_score, confusion_matrix</span>
<span id="cb18-410"><a href="#cb18-410" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-411"><a href="#cb18-411" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb18-412"><a href="#cb18-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-413"><a href="#cb18-413" aria-hidden="true" tabindex="-1"></a><span class="co"># Models we've trained</span></span>
<span id="cb18-414"><a href="#cb18-414" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> {</span>
<span id="cb18-415"><a href="#cb18-415" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Decision Tree"</span>: decision_tree,</span>
<span id="cb18-416"><a href="#cb18-416" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Random Forest"</span>: random_forest,</span>
<span id="cb18-417"><a href="#cb18-417" aria-hidden="true" tabindex="-1"></a>    <span class="st">"SVM"</span>: svm,</span>
<span id="cb18-418"><a href="#cb18-418" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Neural Network"</span>: neural_network</span>
<span id="cb18-419"><a href="#cb18-419" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-420"><a href="#cb18-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-421"><a href="#cb18-421" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate each model</span></span>
<span id="cb18-422"><a href="#cb18-422" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, model <span class="kw">in</span> models.items():</span>
<span id="cb18-423"><a href="#cb18-423" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model.predict(test_data.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb18-424"><a href="#cb18-424" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> accuracy_score(test_data[<span class="st">'Class'</span>], predictions)</span>
<span id="cb18-425"><a href="#cb18-425" aria-hidden="true" tabindex="-1"></a>    precision <span class="op">=</span> precision_score(test_data[<span class="st">'Class'</span>], predictions, average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb18-426"><a href="#cb18-426" aria-hidden="true" tabindex="-1"></a>    recall <span class="op">=</span> recall_score(test_data[<span class="st">'Class'</span>], predictions, average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb18-427"><a href="#cb18-427" aria-hidden="true" tabindex="-1"></a>    f1 <span class="op">=</span> f1_score(test_data[<span class="st">'Class'</span>], predictions, average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb18-428"><a href="#cb18-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-429"><a href="#cb18-429" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb18-430"><a href="#cb18-430" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-431"><a href="#cb18-431" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Precision: </span><span class="sc">{</span>precision<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-432"><a href="#cb18-432" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Recall: </span><span class="sc">{</span>recall<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-433"><a href="#cb18-433" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  F1-Score: </span><span class="sc">{</span>f1<span class="sc">:.4f}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb18-434"><a href="#cb18-434" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-435"><a href="#cb18-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-436"><a href="#cb18-436" aria-hidden="true" tabindex="-1"></a><span class="fu">### Confusion Matrix for Each Model</span></span>
<span id="cb18-437"><a href="#cb18-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-438"><a href="#cb18-438" aria-hidden="true" tabindex="-1"></a>The confusion matrix is a useful tool for understanding the performance of a classification model. It shows the actual vs. predicted classifications.</span>
<span id="cb18-439"><a href="#cb18-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-440"><a href="#cb18-440" aria-hidden="true" tabindex="-1"></a>Let's plot the confusion matrix for each model and discuss what the scores mean:</span>
<span id="cb18-441"><a href="#cb18-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-444"><a href="#cb18-444" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-445"><a href="#cb18-445" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, model <span class="kw">in</span> models.items():</span>
<span id="cb18-446"><a href="#cb18-446" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model.predict(test_data.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb18-447"><a href="#cb18-447" aria-hidden="true" tabindex="-1"></a>    conf_mat <span class="op">=</span> confusion_matrix(test_data[<span class="st">'Class'</span>], predictions)</span>
<span id="cb18-448"><a href="#cb18-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-449"><a href="#cb18-449" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb18-450"><a href="#cb18-450" aria-hidden="true" tabindex="-1"></a>    sns.heatmap(conf_mat, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'g'</span>)</span>
<span id="cb18-451"><a href="#cb18-451" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f'Confusion Matrix for </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb18-452"><a href="#cb18-452" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Predicted'</span>)</span>
<span id="cb18-453"><a href="#cb18-453" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Actual'</span>)</span>
<span id="cb18-454"><a href="#cb18-454" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb18-455"><a href="#cb18-455" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-456"><a href="#cb18-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-457"><a href="#cb18-457" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Interpretation of Confusion Matrix:</span></span>
<span id="cb18-458"><a href="#cb18-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-459"><a href="#cb18-459" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**True Positives (TP)**: Correctly predicted positive observations.</span>
<span id="cb18-460"><a href="#cb18-460" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**True Negatives (TN)**: Correctly predicted negative observations.</span>
<span id="cb18-461"><a href="#cb18-461" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**False Positives (FP)**: Incorrectly predicted positive observations (Type I error).</span>
<span id="cb18-462"><a href="#cb18-462" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**False Negatives (FN)**: Incorrectly predicted negative observations (Type II error).</span>
<span id="cb18-463"><a href="#cb18-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-464"><a href="#cb18-464" aria-hidden="true" tabindex="-1"></a>A model with high TP and TN values and low FP and FN values is generally considered good. However, the importance of each type of error can vary based on the application. For instance, in medical diagnosis, reducing FN (missed diagnoses) may be more critical than reducing FP.</span>
<span id="cb18-465"><a href="#cb18-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-466"><a href="#cb18-466" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model Interpretation</span></span>
<span id="cb18-467"><a href="#cb18-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-468"><a href="#cb18-468" aria-hidden="true" tabindex="-1"></a>Interpreting machine learning models involves understanding the decisions made by the model and assessing their importance in the context of the task. For our driving behavior classification project, the model interpretation will shed light on how well the models are distinguishing between SLOW, NORMAL, and AGGRESSIVE driving behaviors.</span>
<span id="cb18-469"><a href="#cb18-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-470"><a href="#cb18-470" aria-hidden="true" tabindex="-1"></a><span class="fu">### Interpretation of Model Results</span></span>
<span id="cb18-471"><a href="#cb18-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-472"><a href="#cb18-472" aria-hidden="true" tabindex="-1"></a>The confusion matrices provide a visual and quantitative way to measure the performance of the models. Ideally, a well-performing model would have high numbers along the diagonal, indicating correct predictions, and low numbers off the diagonal.</span>
<span id="cb18-473"><a href="#cb18-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-474"><a href="#cb18-474" aria-hidden="true" tabindex="-1"></a>In all four confusion matrices, we observe that the models predict only one class, which indicates a severe imbalance in their learning, potentially caused by class imbalance or other data issues. They fail to predict any instances of the 'SLOW' and 'AGGRESSIVE' classes, classifying everything as 'NORMAL'. This is also reflected in the low precision and F1-scores for all models, as these metrics account for both false positives and false negatives.</span>
<span id="cb18-475"><a href="#cb18-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-476"><a href="#cb18-476" aria-hidden="true" tabindex="-1"></a><span class="fu">### Feature Importance Analysis</span></span>
<span id="cb18-477"><a href="#cb18-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-478"><a href="#cb18-478" aria-hidden="true" tabindex="-1"></a>Understanding which features are most important for making predictions can provide insights into the dataset and the model's decision-making process. For tree-based models, we can directly retrieve feature importance. For other models, such as SVM and Neural Networks, the interpretation can be more complex and may require additional techniques like permutation importance.</span>
<span id="cb18-479"><a href="#cb18-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-480"><a href="#cb18-480" aria-hidden="true" tabindex="-1"></a>For Decision Trees and Random Forests, feature importance is calculated based on how well they split the data and reduce impurity. Here's how you might compute and plot the feature importances for the Random Forest model:</span>
<span id="cb18-481"><a href="#cb18-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-484"><a href="#cb18-484" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-485"><a href="#cb18-485" aria-hidden="true" tabindex="-1"></a>importances <span class="op">=</span> random_forest.feature_importances_</span>
<span id="cb18-486"><a href="#cb18-486" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> np.argsort(importances)[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb18-487"><a href="#cb18-487" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> train_data.drop([<span class="st">'Class'</span>], axis<span class="op">=</span><span class="dv">1</span>).columns</span>
<span id="cb18-488"><a href="#cb18-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-489"><a href="#cb18-489" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the feature importances of the forest</span></span>
<span id="cb18-490"><a href="#cb18-490" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb18-491"><a href="#cb18-491" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Feature importances"</span>)</span>
<span id="cb18-492"><a href="#cb18-492" aria-hidden="true" tabindex="-1"></a>plt.bar(<span class="bu">range</span>(train_data.shape[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">1</span>), importances[indices], align<span class="op">=</span><span class="st">"center"</span>)</span>
<span id="cb18-493"><a href="#cb18-493" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(train_data.shape[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">1</span>), features[indices], rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb18-494"><a href="#cb18-494" aria-hidden="true" tabindex="-1"></a>plt.xlim([<span class="op">-</span><span class="dv">1</span>, train_data.shape[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">1</span>])</span>
<span id="cb18-495"><a href="#cb18-495" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-496"><a href="#cb18-496" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-497"><a href="#cb18-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-498"><a href="#cb18-498" aria-hidden="true" tabindex="-1"></a>For the SVM and Neural Network models, feature importance is not as straightforward because these models do not inherently provide a method for evaluating the importance of features. However, for the SVM model with a linear kernel, we can look at the weights assigned to each feature to determine their importance. For neural networks, especially deep ones, feature importance is a more complex area of research and often involves using additional tools or methods.</span>
<span id="cb18-499"><a href="#cb18-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-500"><a href="#cb18-500" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Results</span></span>
<span id="cb18-501"><a href="#cb18-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-502"><a href="#cb18-502" aria-hidden="true" tabindex="-1"></a>The models' inability to correctly classify all three driving behaviors indicates that further investigation into the data distribution, preprocessing, and model training is necessary. This could include resampling techniques to address class imbalance, feature engineering to better capture the characteristics of aggressive driving, or exploring more complex models and tuning strategies. It's also important to consider the real-world applicability of these models — in a safety-critical domain such as driving behavior classification, we must strive for high precision and recall to ensure all aggressive behaviors are accurately detected.</span>
<span id="cb18-503"><a href="#cb18-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-504"><a href="#cb18-504" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion</span></span>
<span id="cb18-505"><a href="#cb18-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-506"><a href="#cb18-506" aria-hidden="true" tabindex="-1"></a>In summarizing our findings, the machine learning models developed to classify driving behavior as SLOW, NORMAL, or AGGRESSIVE revealed critical insights. The evaluation metrics and confusion matrices indicated that our models predominantly predicted the NORMAL class while failing to recognize SLOW and AGGRESSIVE behaviors. The accuracy across the models ranged from approximately 26% to 41%, with the SVM model achieving the highest accuracy. However, the precision and recall scores were low for all models, indicating a significant area for improvement.</span>
<span id="cb18-507"><a href="#cb18-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-508"><a href="#cb18-508" aria-hidden="true" tabindex="-1"></a><span class="fu">### Limitations</span></span>
<span id="cb18-509"><a href="#cb18-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-510"><a href="#cb18-510" aria-hidden="true" tabindex="-1"></a>The limitations of our current approach are evident:</span>
<span id="cb18-511"><a href="#cb18-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-512"><a href="#cb18-512" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Class Imbalance**: The models' tendency to predict mostly the NORMAL class suggests a possible class imbalance.</span>
<span id="cb18-513"><a href="#cb18-513" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Model Overfitting**: The poor generalization to other classes may also indicate overfitting to the majority class.</span>
<span id="cb18-514"><a href="#cb18-514" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Feature Representation**: The current features may not sufficiently represent the characteristics unique to each class.</span>
<span id="cb18-515"><a href="#cb18-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-516"><a href="#cb18-516" aria-hidden="true" tabindex="-1"></a><span class="fu">### Potential Improvements</span></span>
<span id="cb18-517"><a href="#cb18-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-518"><a href="#cb18-518" aria-hidden="true" tabindex="-1"></a>Several improvements can be proposed:</span>
<span id="cb18-519"><a href="#cb18-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-520"><a href="#cb18-520" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Data Resampling**: Implementing techniques like SMOTE or random oversampling to balance the class distribution could be beneficial.</span>
<span id="cb18-521"><a href="#cb18-521" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Feature Engineering**: Developing more sophisticated features or utilizing feature selection methods to capture more nuanced patterns of aggressive driving.</span>
<span id="cb18-522"><a href="#cb18-522" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Model Complexity**: Exploring more complex models or deep learning approaches that might capture the intricacies in the data better.</span>
<span id="cb18-523"><a href="#cb18-523" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Extended Hyperparameter Tuning**: Conducting a more thorough hyperparameter optimization, given more computational time and resources.</span>
<span id="cb18-524"><a href="#cb18-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-525"><a href="#cb18-525" aria-hidden="true" tabindex="-1"></a>In conclusion, while the current models have provided a foundation, further work is needed to develop a robust classifier that can accurately identify various driving behaviors, which is crucial for enhancing road safety and reducing accidents.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>